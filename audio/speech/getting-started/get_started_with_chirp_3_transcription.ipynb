{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Get started with Chirp 3 Transcription\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Faudio%2Fspeech%2Fgetting-started%2Fget_started_with_chirp_3_transcription.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Katie Nguyen](https://github.com/katiemn) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Chirp 3\n",
        "\n",
        "This notebook introduces [Chirp 3](https://cloud.google.com/speech-to-text/v2/docs/chirp_3-model), Google's model for converting speech to text in multiple languages.\n",
        "\n",
        "In this tutorial, you'll learn how to use the Speech-to-Text API V2 to:\n",
        "\n",
        "- Transcribe an audio file with batch speech recognition\n",
        "- Perform a language-agnostic transcription\n",
        "- Use Chirp 3 for speaker diarization\n",
        "- Perform streaming speech recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install the Speech SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-cloud-speech ipywebrtc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqm0OQpAYCph"
      },
      "outputs": [],
      "source": [
        "from google.cloud.speech_v2 import SpeechClient\n",
        "from google.cloud.speech_v2.types import cloud_speech\n",
        "\n",
        "from IPython.display import Audio, HTML, display\n",
        "from google.api_core.client_options import ClientOptions\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using the Speech-to-Text API, you must have an existing Google Cloud project and [enable the API](https://console.cloud.google.com/flows/enableapi?apiid=speech.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "Please note the **available regions** for Chirp 3, see [documentation](https://cloud.google.com/speech-to-text/v2/docs/chirp_3-model#regional_availability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIQyBhAn_9tK"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "STT_LOCATION = \"us\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR5PafOSrV2W"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project {PROJECT_ID}\n",
        "! gcloud auth application-default login -q\n",
        "! gcloud auth application-default set-quota-project {PROJECT_ID}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP8GBj3tBAC1"
      },
      "source": [
        "### Create client\n",
        "\n",
        "Initiate the API endpoint and the speech to text client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXTVeU1uBBqY"
      },
      "outputs": [],
      "source": [
        "client = SpeechClient(\n",
        "    client_options=ClientOptions(api_endpoint=f\"{STT_LOCATION}-speech.googleapis.com\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7WQQFp_RvGH"
      },
      "source": [
        "### Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWrNWdV_RxGn"
      },
      "outputs": [],
      "source": [
        "def generate_audio_chunks(audio_content: bytes, chunk_size: int):\n",
        "    for start in range(0, len(audio_content), chunk_size):\n",
        "        yield audio_content[start : start + chunk_size]\n",
        "\n",
        "\n",
        "def group_utterances_by_speaker_from_file(json_file_path: str):\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        json_data_string = f.read()\n",
        "    words_regex = r'\"words\":\\s*(\\[.*?\\])'\n",
        "    match = re.search(words_regex, json_data_string, re.DOTALL)\n",
        "\n",
        "    words_list = json.loads(match.group(1))\n",
        "    dialogue = []\n",
        "    current_speaker = None\n",
        "    current_utterance_words = []\n",
        "    current_speaker = words_list[0][\"speakerLabel\"]\n",
        "\n",
        "    for item in words_list:\n",
        "        word = item[\"word\"]\n",
        "        speaker = item[\"speakerLabel\"]\n",
        "        # Check if the speaker has changed\n",
        "        if speaker != current_speaker:\n",
        "            dialogue.append({\n",
        "                \"speaker\": current_speaker,\n",
        "                \"text\": \" \".join(current_utterance_words)\n",
        "            })\n",
        "            # Start a new utterance\n",
        "            current_speaker = speaker\n",
        "            current_utterance_words = [word]\n",
        "        else:\n",
        "            # Continue the current utterance\n",
        "            current_utterance_words.append(word)\n",
        "    # Add the final pending utterance\n",
        "    if current_speaker is not None:\n",
        "        dialogue.append({\n",
        "            \"speaker\": current_speaker,\n",
        "            \"text\": \" \".join(current_utterance_words)\n",
        "        })\n",
        "\n",
        "    return {\"dialogue\": dialogue}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPVDNRyVxquo"
      },
      "source": [
        "## Transcribe using Chirp 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfBiMs0bNwhV"
      },
      "source": [
        "### Batch speech recognition\n",
        "\n",
        "For this first request, you'll use the `batch_recognize` method to transcribe an audio file in Cloud Storage. Run the following cell to play the audio you'll be transcribing. If you'd like to use a different audio clip, modify the `audio_url` and `audio_gcs_uri` variables below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE_-iirV7UDA"
      },
      "outputs": [],
      "source": [
        "audio_url = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/audio/audio_summary_clean_energy.mp3\"\n",
        "\n",
        "audio_gcs_uri = \"gs://cloud-samples-data/generative-ai/audio/audio_summary_clean_energy.mp3\"\n",
        "\n",
        "display(Audio(url=audio_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAfgmnZyPo1f"
      },
      "source": [
        "Now, you'll send the `batch_recognize` request. The transcription will be returned as part of the response and displayed in HTML for better visualization in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egBk4a3t0lUD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set a timeout for the batch recognition operation\n",
        "MAX_AUDIO_LENGTH_SECS = 8 * 60 * 60\n",
        "\n",
        "config = cloud_speech.RecognitionConfig(\n",
        "    auto_decoding_config={},\n",
        "    model=\"chirp_3\",\n",
        "    language_codes=[\"en-US\"],\n",
        ")\n",
        "\n",
        "files = [cloud_speech.BatchRecognizeFileMetadata(uri=audio_gcs_uri)]\n",
        "\n",
        "request = cloud_speech.BatchRecognizeRequest(\n",
        "    recognizer=f\"projects/{PROJECT_ID}/locations/{STT_LOCATION}/recognizers/_\",\n",
        "    config=config,\n",
        "    files=files,\n",
        "    recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
        "          inline_response_config=cloud_speech.InlineOutputConfig(),\n",
        "    ),\n",
        ")\n",
        "\n",
        "operation = client.batch_recognize(request=request)\n",
        "response = operation.result(timeout=3 * MAX_AUDIO_LENGTH_SECS)\n",
        "\n",
        "for result in response.results[audio_gcs_uri].transcript.results:\n",
        "   styled_html = f\"\"\"<div style=\"word-break: break-all;\">{result.alternatives[0].transcript}</div>\n",
        "   \"\"\"\n",
        "HTML(styled_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJSi0bFQQYel"
      },
      "source": [
        "### Perform a language-agnostic transcription\n",
        "\n",
        "In this next request, you'll perform a language-agnostic transcription. This means that Chirp 3 will automatically identify and transcribe the dominant language spoken in the audio, which is essential for multilingual applications.\n",
        "\n",
        "In this next example, you'll use a Spanish audio clip saved in Cloud Storage. To see a full list of the languages available for transcription, check the [documentation](https://cloud.google.com/speech-to-text/v2/docs/chirp_3-model#language_availability_for_transcription)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcTrgN9xHDXR"
      },
      "outputs": [],
      "source": [
        "audio_url = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/audio/spanish.wav\"\n",
        "\n",
        "audio_gcs_uri = \"gs://cloud-samples-data/generative-ai/audio/spanish.wav\"\n",
        "\n",
        "display(Audio(url=audio_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4cDExAMCnb"
      },
      "source": [
        "This request is the same as the previous one, except this time, you'll set `language_codes=[\"auto\"]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CphxakXgBEsz"
      },
      "outputs": [],
      "source": [
        "MAX_AUDIO_LENGTH_SECS = 8 * 60 * 60\n",
        "\n",
        "config = cloud_speech.RecognitionConfig(\n",
        "    auto_decoding_config={},\n",
        "    model=\"chirp_3\",\n",
        "    language_codes=[\"auto\"],\n",
        ")\n",
        "\n",
        "files = [cloud_speech.BatchRecognizeFileMetadata(uri=audio_gcs_uri)]\n",
        "\n",
        "request = cloud_speech.BatchRecognizeRequest(\n",
        "    recognizer=f\"projects/{PROJECT_ID}/locations/{STT_LOCATION}/recognizers/_\",\n",
        "    config=config,\n",
        "    files=files,\n",
        "    recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
        "          inline_response_config=cloud_speech.InlineOutputConfig(),\n",
        "    ),\n",
        ")\n",
        "\n",
        "operation = client.batch_recognize(request=request)\n",
        "response = operation.result(timeout=3 * MAX_AUDIO_LENGTH_SECS)\n",
        "\n",
        "for result in response.results[audio_gcs_uri].transcript.results:\n",
        "   styled_html = f\"\"\"<div style=\"word-break: break-all;\">{result.alternatives[0].transcript}</div>\n",
        "   \"\"\"\n",
        "HTML(styled_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOGjzlRMFMw"
      },
      "source": [
        "### Speaker Diarization\n",
        "\n",
        "Chirp 3 also supports speaker diarization, which means it can automatically identify the different speakers in a single-channel audio sample. See the [documentation](https://cloud.google.com/speech-to-text/v2/docs/chirp_3-model#language_availability_for_diarization) for a list of supported available languages for diarization.\n",
        "\n",
        "In this example, you'll also save the transcription to Cloud Storage. Make sure to add the bucket where you'd like it to be saved in `gcs_output_folder` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FosRgcFqEMXX"
      },
      "outputs": [],
      "source": [
        "audio_url = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/audio/Chirp-3-Docs-Dive.mp3\"\n",
        "\n",
        "audio_gcs_uri = \"gs://cloud-samples-data/generative-ai/audio/Chirp-3-Docs-Dive.mp3\"\n",
        "\n",
        "display(Audio(url=audio_url))\n",
        "\n",
        "# The output path of the transcription result\n",
        "gcs_output_folder = \"gs://[your-bucket-path]\" # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb_gkdN9BvnP"
      },
      "source": [
        "In order to enable speaker diarization, set the `diarization_config` in the `features` parameter of the `RecognitionConfig`.\n",
        "\n",
        "You'll also set your `gcs_output_folder` in a `RecognitionOutputConfig` so the transcription will be saved in Cloud Storage. To display the transcription, you'll copy the output JSON file and use a helper function to format it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odgee64-xA2O"
      },
      "outputs": [],
      "source": [
        "MAX_AUDIO_LENGTH_SECS = 8 * 60 * 60\n",
        "\n",
        "config = cloud_speech.RecognitionConfig(\n",
        "    auto_decoding_config={},\n",
        "    features=cloud_speech.RecognitionFeatures(\n",
        "        diarization_config=cloud_speech.SpeakerDiarizationConfig(\n",
        "        ),\n",
        "      ),\n",
        "    model=\"chirp_3\",\n",
        "    language_codes=[\"en-US\"],\n",
        ")\n",
        "\n",
        "output_config = cloud_speech.RecognitionOutputConfig(\n",
        "    gcs_output_config=cloud_speech.GcsOutputConfig(uri=gcs_output_folder),\n",
        ")\n",
        "\n",
        "files = [cloud_speech.BatchRecognizeFileMetadata(uri=audio_gcs_uri)]\n",
        "\n",
        "request = cloud_speech.BatchRecognizeRequest(\n",
        "    recognizer=f\"projects/{PROJECT_ID}/locations/{STT_LOCATION}/recognizers/_\",\n",
        "    config=config,\n",
        "    files=files,\n",
        "    recognition_output_config=output_config,\n",
        ")\n",
        "operation = client.batch_recognize(request=request)\n",
        "\n",
        "response = operation.result(timeout=3 * MAX_AUDIO_LENGTH_SECS)\n",
        "transcript = response.results[audio_gcs_uri].uri\n",
        "\n",
        "!gsutil cp {transcript} output.json\n",
        "\n",
        "print(json.dumps(group_utterances_by_speaker_from_file(\"output.json\"), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRFD1ByUDuqW"
      },
      "source": [
        "### Streaming speech recognition\n",
        "\n",
        "In the following cells you'll simulate transcribing text from an audio stream. To start, you'll record an audio clip with your microphone by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2OrVxHeMxV2"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "camera = CameraStream(constraints={'audio': True, 'video': False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "recorder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXhPcYx2ECJ-"
      },
      "source": [
        "Once the audio is captured and you've stopped recording, you'll use FFmpeg to convert and save the clip to a MP3 file for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtS36HTdP1hM"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "\n",
        "!ffmpeg -i recording.webm -vn -ar 44100 -ac 2 -f mp3 recording.mp3\n",
        "audio_file = \"recording.mp3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swn2txVDEkSx"
      },
      "source": [
        "Now, you'll read the audio file and generate audio chunks to simulate streaming from a helper function. You'll then use the `streaming_recognize` method to get the transcription from each audio chunk with help from a generator function to correctly structure the data stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwMRXTlf7q-d"
      },
      "outputs": [],
      "source": [
        "# Create a generator to send the recognition config and then the audio stream\n",
        "def requests() -> iter:\n",
        "    yield request\n",
        "    yield from audio_requests\n",
        "\n",
        "with open(audio_file, \"rb\") as f:\n",
        "    audio_content = f.read()\n",
        "\n",
        "CHUNK_SIZE = 3200\n",
        "stream = list(generate_audio_chunks(audio_content, CHUNK_SIZE))\n",
        "\n",
        "recognition_config = cloud_speech.RecognitionConfig(\n",
        "    auto_decoding_config={},\n",
        "    language_codes=[\"auto\"],\n",
        "    model=\"chirp_3\",\n",
        ")\n",
        "\n",
        "streaming_config = cloud_speech.StreamingRecognitionConfig(\n",
        "    config=recognition_config,\n",
        ")\n",
        "\n",
        "request = cloud_speech.StreamingRecognizeRequest(\n",
        "    recognizer=f\"projects/{PROJECT_ID}/locations/{STT_LOCATION}/recognizers/_\",\n",
        "    streaming_config=streaming_config,\n",
        ")\n",
        "\n",
        "audio_requests = (\n",
        "    cloud_speech.StreamingRecognizeRequest(audio=chunk) for chunk in stream\n",
        ")\n",
        "\n",
        "responses_iterator = client.streaming_recognize(\n",
        "    requests=requests()\n",
        ")\n",
        "responses = []\n",
        "for response in responses_iterator:\n",
        "    responses.append(response)\n",
        "    for result in response.results:\n",
        "        print(result.alternatives[0].transcript)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
