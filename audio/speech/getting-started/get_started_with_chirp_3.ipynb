{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Get started with Chirp 3 using Text-to-Speech\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Faudio%2Fspeech%2Fgetting-started%2Fget_started_with_chirp_3.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/audio/speech/getting-started/get_started_with_chirp_3.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Holt Skinner](https://github.com/holtskinner), [Ivan Nardini](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook introduces [Chirp 3 HD Voices](https://cloud.google.com/text-to-speech/docs/chirp3-hd), which are Google Cloud's latest advancement in Text-to-Speech (TTS) technology.\n",
    "\n",
    "These voices, powered by state-of-the-art large language models (LLMs), offer a significantly improved level of realism and emotional expressiveness.\n",
    "\n",
    "Chirp 3 HD voices provide high-fidelity audio and natural-sounding speech, complete with human-like intonation and pauses. They are available on the Vertex AI platform and are designed for various uses like, voice assistants, audiobooks, and customer service applications.\n",
    "\n",
    "There are currently eight distinct voice options(4 male, 4 female) available in 31 languages.\n",
    "\n",
    "In this tutorial, you learn how to:\n",
    "\n",
    "- How to synthesize speech using real-time (online) processing\n",
    "- How to synthesize speech using streaming processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Text-to-Speech SDK and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e73_ZgKWYedz"
   },
   "outputs": [],
   "source": [
    "! apt update -y -qq\n",
    "! apt install ffmpeg -y -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-cloud-texttospeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize SDK\n",
    "\n",
    "To get started using the Text-to-Speech API, you must have an existing Google Cloud project and [enable the API](https://console.cloud.google.com/flows/enableapi?apiid=texttospeech.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
    "\n",
    "Please note the **available regions** for Chirp 3, see [documentation](https://cloud.google.com/text-to-speech/docs/endpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIQyBhAn_9tK"
   },
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "TTS_LOCATION = \"global\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76915236b1c9"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project {PROJECT_ID}\n",
    "! gcloud auth application-default set-quota-project {PROJECT_ID}\n",
    "! gcloud auth application-default login -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqm0OQpAYCph"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "import os\n",
    "import re\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import texttospeech_v1beta1 as texttospeech\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP8GBj3tBAC1"
   },
   "source": [
    "### Set constants\n",
    "\n",
    "Initiate the API endpoint and the text to speech client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXTVeU1uBBqY"
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = (\n",
    "    f\"{TTS_LOCATION}-texttospeech.googleapis.com\"\n",
    "    if TTS_LOCATION != \"global\"\n",
    "    else \"texttospeech.googleapis.com\"\n",
    ")\n",
    "\n",
    "client = texttospeech.TextToSpeechClient(\n",
    "    client_options=ClientOptions(api_endpoint=API_ENDPOINT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7WQQFp_RvGH"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWrNWdV_RxGn"
   },
   "outputs": [],
   "source": [
    "def text_generator(text: str) -> Iterator[str]:\n",
    "    \"\"\"Split text into sentences to simulate streaming\"\"\"\n",
    "\n",
    "    # Use regex with positive lookahead to find sentence boundaries\n",
    "    # without consuming the space after the punctuation\n",
    "    sentences = re.findall(r\"[^.!?]+[.!?](?:\\s|$)\", text + \" \")\n",
    "\n",
    "    # Yield each complete sentence\n",
    "    for sentence in sentences:\n",
    "        yield sentence.strip()\n",
    "\n",
    "    # Check if there's remaining text not caught by the regex\n",
    "    # (text without ending punctuation)\n",
    "    last_char_pos = 0\n",
    "    for sentence in sentences:\n",
    "        last_char_pos += len(sentence)\n",
    "\n",
    "    if last_char_pos < len(text.strip()):\n",
    "        remaining = text.strip()[last_char_pos:]\n",
    "        if remaining:\n",
    "            yield remaining.strip()\n",
    "\n",
    "\n",
    "def process_streaming_audio(text: str, display_individual_chunks: bool = False):\n",
    "    \"\"\"Process text into speech using streaming TTS\"\"\"\n",
    "\n",
    "    # Generate sentences from text\n",
    "    sentences = list(text_generator(text))\n",
    "\n",
    "    # Get streaming audio\n",
    "    print(\"Streaming audio processing...\")\n",
    "    audio_iterator = synthesize_streaming(iter(sentences))\n",
    "\n",
    "    # Process audio chunks\n",
    "    final_audio_data = np.array([], dtype=np.int16)\n",
    "\n",
    "    for idx, audio_content in enumerate(audio_iterator):\n",
    "        audio_chunk = np.frombuffer(audio_content, dtype=np.int16)\n",
    "\n",
    "        # Concatenate to final audio\n",
    "        final_audio_data = np.concatenate((final_audio_data, audio_chunk))\n",
    "\n",
    "        # Optionally display individual chunks\n",
    "        if display_individual_chunks and len(audio_chunk) > 0:\n",
    "            print(f\"Processed chunk # {idx}\")\n",
    "            display(Audio(audio_chunk, rate=24000))\n",
    "\n",
    "    print(\"Streaming audio processing complete!\")\n",
    "    return final_audio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPVDNRyVxquo"
   },
   "source": [
    "## Synthesize using Chirp 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9aa2ab365ac"
   },
   "source": [
    "### Synthesize speech using real-time (online) processing\n",
    "\n",
    "You define the text you want to convert, select a specific voice and language, and then instruct the API to generate an audio of the spoken text.\n",
    "\n",
    "This example uses the `en-US-Chirp3-HD-Aoede` voice, which is a high-definition voice, offering improved clarity. The code will call the `synthesize_speech` method, which handles the core conversion process, and the output will be an MP3 audio as `bytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7XwbSxUD4eW"
   },
   "outputs": [],
   "source": [
    "prompt = \"Hello world! I am Chirp 3\"  # @param [\"Hallo Welt! Ich bin Chirp 3\", \"Hello world! I am Chirp 3\", \"¡Hola mundo! Soy Chirp 3\", \"Bonjour le monde ! Je suis Chirp 3\", \"नमस्ते दुनिया! मैं चिर्प 3 हूं\", \"Olá Mundo! Eu sou o Chirp 3\", \"مرحبا بالعالم! أنا تشيرب 3\", \"¡Hola mundo! Soy Chirp 3\", \"Bonjour le monde ! Je suis Chirp 3\", \"Halo dunia! Saya Chirp 3\", \"Ciao mondo! Sono Chirp 3\", \"こんにちは世界！私はチャープ3です\", \"Merhaba dünya! Ben Chirp 3\", \"Chào thế giới! Tôi là Chirp 3\", \"হ্যালো ওয়ার্ল্ড! আমি চির্প 3\", \"હેલો વર્લ્ડ! હું ચિર્પ 3 છું\", \"ನಮಸ್ಕಾರ ಪ್ರಪಂಚ! ನಾನು ಚಿರ್ಪ್ 3\", \"ഹലോ വേൾഡ്! ഞാൻ ചിർപ് 3 ആണ്\", \"नमस्कार जग! मी चिरप 3 आहे\", \"வணக்கம் உலகம்! நான் சிர்ப் 3\", \"హలో వరల్డ్! నేను చిర్ప్ 3\", \"Hallo wereld! Ik ben Chirp 3\", \"안녕하세요! 저는 Chirp 3입니다\", \"你好世界！我是 Chirp 3\", \"Witaj świecie! Jestem Chirp 3\", \"Привет, мир! Я Чирп 3\", \"สวัสดีชาวโลก! ฉันคือเชิร์ป 3\"]\n",
    "\n",
    "voice = \"Aoede\"  # @param [\"Aoede\", \"Puck\", \"Charon\", \"Kore\", \"Fenrir\", \"Leda\", \"Orus\", \"Zephyr\"]\n",
    "\n",
    "language_code = \"en-US\"  # @param [ \"de-DE\", \"en-AU\", \"en-GB\", \"en-IN\", \"en-US\", \"fr-FR\", \"hi-IN\", \"pt-BR\", \"ar-XA\", \"es-ES\", \"fr-CA\", \"id-ID\", \"it-IT\", \"ja-JP\", \"tr-TR\", \"vi-VN\", \"bn-IN\", \"gu-IN\", \"kn-IN\", \"ml-IN\", \"mr-IN\", \"ta-IN\", \"te-IN\", \"nl-NL\", \"ko-KR\", \"cmn-CN\", \"pl-PL\", \"ru-RU\", \"th-TH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98d104dc4d27"
   },
   "outputs": [],
   "source": [
    "voice_name = f\"{language_code}-Chirp3-HD-{voice}\"\n",
    "\n",
    "# Perform the text-to-speech request on the text input with the selected\n",
    "# voice parameters and audio file type\n",
    "response = client.synthesize_speech(\n",
    "    input=texttospeech.SynthesisInput(text=prompt),\n",
    "    voice=texttospeech.VoiceSelectionParams(\n",
    "        name=voice_name,\n",
    "        language_code=language_code,\n",
    "    ),\n",
    "    # Select the type of audio file you want returned\n",
    "    audio_config=texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fad3a3de36d9"
   },
   "source": [
    "Play the generated audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e0881955f62"
   },
   "outputs": [],
   "source": [
    "display(Audio(response.audio_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYKj5gmDG7nT"
   },
   "source": [
    "### Synthesize speech using streaming processing\n",
    "\n",
    "Chirp 3 HD voices also support streaming text-to-speech conversion using the `streaming_synthesize` method.  Unlike the standard `synthesize_speech`, which handles single requests, `streaming_synthesize` processes continuous streams of text, generating corresponding audio streams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll9rWo-dQ8Cx"
   },
   "outputs": [],
   "source": [
    "def synthesize_streaming(\n",
    "    text_iterator,\n",
    "):\n",
    "    \"\"\"Synthesizes speech from an iterator of text inputs and yields audio content as an iterator.\n",
    "\n",
    "    This function demonstrates how to use the Google Cloud Text-to-Speech API\n",
    "    to synthesize speech from a stream of text inputs provided by an iterator.\n",
    "    It yields the audio content from each response as an iterator of bytes.\n",
    "\n",
    "    \"\"\"\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    streaming_config = texttospeech.StreamingSynthesizeConfig(\n",
    "        voice=texttospeech.VoiceSelectionParams(\n",
    "            name=\"en-US-Chirp3-HD-Charon\", language_code=\"en-US\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    config_request = texttospeech.StreamingSynthesizeRequest(\n",
    "        streaming_config=streaming_config\n",
    "    )\n",
    "\n",
    "    def request_generator():\n",
    "        yield config_request\n",
    "        for text in text_iterator:\n",
    "            yield texttospeech.StreamingSynthesizeRequest(\n",
    "                input=texttospeech.StreamingSynthesisInput(text=text)\n",
    "            )\n",
    "\n",
    "    streaming_responses = client.streaming_synthesize(request_generator())\n",
    "\n",
    "    for response in streaming_responses:\n",
    "        yield response.audio_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HniB988UTUlq"
   },
   "outputs": [],
   "source": [
    "final_audio_data = process_streaming_audio(prompt, display_individual_chunks=False)\n",
    "\n",
    "display(Audio(final_audio_data, rate=24000))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_chirp_3.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
