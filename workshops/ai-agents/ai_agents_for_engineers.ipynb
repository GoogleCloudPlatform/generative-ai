{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# AI Agents for Engineers (Evolution of AI Agents)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/holtskinner/gemini-agent-ai-camp/blob/main/ai_agents_for_engineers.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Kristopher Overholt](https://github.com/koverholt) [Holt Skinner](https://github.com/holtskinner)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates 3 different approaches to generating essays using the [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs). Each method illustrates a distinct paradigm for running AI Agents in differing levels of complexity.\n",
        "\n",
        "1. Zero-Shot Approach with the Gemini API\n",
        "2. Step-by-Step Approach With LangChain\n",
        "3. Iterative, AI-Agent Approach with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Gemini SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet \\\n",
        "    google-generativeai \\\n",
        "    langgraph \\\n",
        "    langchain \\ \n",
        "    langchain-google-genai \\\n",
        "    langchain-google-vertexai \\\n",
        "    langchain-community \\\n",
        "    tavily-python \\\n",
        "    pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0781fd4c9001"
      },
      "source": [
        "### Configure API keys\n",
        "\n",
        "Get API keys from [Google AI Studio](https://ai.google.dev/gemini-api/docs/api-key) and [Tavily](https://tavily.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2d2ecd0e96d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"YOUR_API_KEY_HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Generating Essays Using a Zero-Shot Approach with the Gemini API\n",
        "\n",
        "With just a single call to the `generate_content` method, users can create detailed, structured essays on any topic by leveraging state-of-the-art language models such as Gemini 1.5 Pro or Gemini 1.5 Flash.\n",
        "\n",
        "<img src=\"1-prompt-essay.png\" width=\"350px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbcd4f57190f"
      },
      "source": [
        "### Make an API call to generate the essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3734f520c1b3"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a 3-paragraph essay about the application of heat transfer in modern data centers\"\n",
        "response = model.generate_content([prompt])\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520e23ea4332"
      },
      "source": [
        "---\n",
        "\n",
        "However, what if we ask the model to write an essay about an event that happened more recently and the LLM doesn't inherently know about that event?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcb775b83997"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a 3-paragraph essay about the impacts of Hurricane Helene and Hurricane Milton in 2024.\"\n",
        "response = model.generate_content([prompt])\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "764ce71aecd5"
      },
      "source": [
        "In this case, the model had no information about these recent events and was unable to write an effective essay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16aafc60d80b"
      },
      "source": [
        "## Generating Essays Using a Step-by-Step Approach With LangChain\n",
        "\n",
        "This step demonstrates how to build an essay-writing pipeline using [LangChain](https://www.langchain.com/), the [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs), and [Tavily](https://tavily.com/) for search.\n",
        "\n",
        "By combining these tools, we create a seamless workflow that plans an essay outline, performs web searches for relevant information, and generates a complete essay draft based on the collected data.\n",
        "\n",
        "This solution showcases the power of chaining LLM models and external tools to tackle complex tasks with minimal human intervention, providing a robust approach to automated content generation.\n",
        "\n",
        "<img src=\"2-langchain-essay.png\" width=\"550px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85666976a359"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29d6e42d27ae"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43392590b1d8"
      },
      "source": [
        "### Initialize Gemini model & search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f8b0c205551"
      },
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7)\n",
        "tavily_tool = TavilySearchResults(max_results=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ee8707e1867"
      },
      "source": [
        "### Define prompt templates and Runnables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a09a6a6d1f36"
      },
      "outputs": [],
      "source": [
        "# Planning: Create an outline for the essay\n",
        "outline_template = ChatPromptTemplate.from_template(\n",
        "    \"Create a detailed outline for an essay on {topic}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Research: Web search\n",
        "def research_fn(topic):\n",
        "    response = tavily_tool.invoke({\"query\": topic})\n",
        "    return \"\\n\".join([f\"- {result['content']}\" for result in response])\n",
        "\n",
        "\n",
        "# Writing: Write the essay based on outline and research\n",
        "writing_template = ChatPromptTemplate.from_template(\n",
        "    \"Based on the following outline and research, write a 3-paragraph essay on '{topic}':\\n\\nOutline:\\n{outline}\\n\\nResearch:\\n{research}\\n\\nEssay:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a18006523f7"
      },
      "source": [
        "### Define the Runnable Chain using [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf48601613fd"
      },
      "outputs": [],
      "source": [
        "# Define individual chains\n",
        "outline_chain = LLMChain(llm=model, prompt=outline_template)\n",
        "writing_chain = LLMChain(llm=model, prompt=writing_template)\n",
        "\n",
        "# Use the pipe operator to combine chains\n",
        "chain = (\n",
        "    outline_chain\n",
        "    | (\n",
        "        lambda result: {\n",
        "            \"topic\": result[\"topic\"],\n",
        "            \"outline\": result[\"text\"],\n",
        "            \"research\": research_fn(result[\"topic\"]),\n",
        "        }\n",
        "    )\n",
        "    | writing_chain\n",
        "    | (lambda result: result[\"text\"])  # Extract the essay text from the final result\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839fc48dd408"
      },
      "source": [
        "### Generate the essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a76f80ceec98"
      },
      "outputs": [],
      "source": [
        "topic = (\n",
        "    \"Write an essay about the impacts of Hurricane Helene and Hurricane Milton in 2024.\"\n",
        ")\n",
        "essay = chain.invoke({\"topic\": topic})\n",
        "display(Markdown(essay))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "294d3b7c43b2"
      },
      "source": [
        "## Generating Essays Using an Iterative, AI-Agent Approach with LangGraph\n",
        "\n",
        "This section demonstrates how to build a [LangGraph](https://langchain-ai.github.io/langgraph/)-powered AI agent to generate, revise, and critique essays using large language models such as Google's [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs) or the [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview). The LangGraph code was adapted from the awesome DeepLearning.AI course on [AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/).\n",
        "\n",
        "By defining a structured state flow with nodes such as \"Planner,\" \"Research Plan,\" \"Generate,\" \"Reflect,\" and \"Research Critique,\" the system iteratively creates an essay on a given topic, incorporates feedback, and provides research-backed insights.\n",
        "\n",
        "<img src=\"3-langgraph-essay.png\" width=\"900px\">\n",
        "\n",
        "The workflow enables automated essay generation with revision controls, making it ideal for structured writing tasks or educational use cases. Additionally, the notebook uses external search tools to gather and integrate real-time information into the essay content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8e41763086f"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52fbe2cb7be7"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from typing import TypedDict\n",
        "\n",
        "# Common libraries\n",
        "from IPython.display import Image, Markdown, display\n",
        "\n",
        "# LangChain and LangGraph components\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# LangChain integrations for Gemini API in Google AI Studio and Vertex AI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Typing utilities for data validation and schema definitions\n",
        "from pydantic.v1 import BaseModel\n",
        "\n",
        "# Tavily client for performing web searches\n",
        "from tavily import TavilyClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6ae1fac44f"
      },
      "source": [
        "### Initialize agent memory, agent state, and schema for search queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b92f7bab46d"
      },
      "outputs": [],
      "source": [
        "# Initialize agent memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "# Define the agent's state\n",
        "class AgentState(TypedDict):\n",
        "    task: str\n",
        "    plan: str\n",
        "    draft: str\n",
        "    critique: str\n",
        "    content: list[str]\n",
        "    revision_number: int\n",
        "    max_revisions: int\n",
        "\n",
        "\n",
        "# Define a schema for search queries\n",
        "class Queries(BaseModel):\n",
        "    \"\"\"Variants of query to search for\"\"\"\n",
        "\n",
        "    queries: list[str]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9660e58afab"
      },
      "source": [
        "### Initialize Gemini model and search tool\n",
        "\n",
        "Remember to set the environment variables `GOOGLE_API_KEY` and `TAVILY_API_KEY`. And configure credentials for Vertex AI if you switch to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec96b00bb67f"
      },
      "outputs": [],
      "source": [
        "# Initialize Gemini API in Google AI Studio via LangChain\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
        "\n",
        "# Initialize Gemini API in Vertex AI via LangChain\n",
        "# model = ChatVertexAI(model=\"gemini-1.5-pro-002\", temperature=0)\n",
        "\n",
        "# Initialize Tavily client for performing web searches\n",
        "tavily = TavilyClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d94dc64d3846"
      },
      "source": [
        "### Define prompt templates for each stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cc6f9b05d29"
      },
      "outputs": [],
      "source": [
        "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay.\n",
        "Write such an outline for the user provided topic. Give an outline of the essay along with any\n",
        "relevant notes or instructions for the sections.\"\"\"\n",
        "\n",
        "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 3-paragraph essays.\n",
        "Generate the best essay possible for the user's request and the initial outline.\n",
        "If the user provides critique, respond with a revised version of your previous attempts.\n",
        "Use Markdown formatting to specify a title and section headers for each paragraph.\n",
        "Utilize all of the information below as needed:\n",
        "---\n",
        "{content}\"\"\"\n",
        "\n",
        "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission.\n",
        "Generate critique and recommendations for the user's submission.\n",
        "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
        "\n",
        "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can\n",
        "be used when writing the following essay. Generate a list of search queries that will gather\n",
        "any relevant information. Only generate 3 queries max.\"\"\"\n",
        "\n",
        "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can\n",
        "be used when making any requested revisions (as outlined below).\n",
        "Generate a list of search queries that will gather any relevant information.\n",
        "Only generate 3 queries max.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4f51c668222"
      },
      "source": [
        "### Define node functions for each stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75c8d7021369"
      },
      "outputs": [],
      "source": [
        "# Generate an outline for the essay\n",
        "\n",
        "\n",
        "def plan_node(state: AgentState):\n",
        "    messages = [SystemMessage(content=PLAN_PROMPT), HumanMessage(content=state[\"task\"])]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"plan\": response.content}\n",
        "\n",
        "\n",
        "# Conducts research based on the generated plan and web search results\n",
        "def research_plan_node(state: AgentState):\n",
        "    queries = model.with_structured_output(Queries).invoke(\n",
        "        [\n",
        "            SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
        "            HumanMessage(content=state[\"task\"]),\n",
        "        ]\n",
        "    )\n",
        "    content = state[\"content\"] or []\n",
        "    for q in queries.queries:\n",
        "        response = tavily.search(query=q, max_results=2)\n",
        "        for r in response[\"results\"]:\n",
        "            content.append(r[\"content\"])\n",
        "    return {\"content\": content}\n",
        "\n",
        "\n",
        "# Generates a draft based on the content and plan\n",
        "def generation_node(state: AgentState):\n",
        "    content = \"\\n\\n\".join(state[\"content\"] or [])\n",
        "    user_message = HumanMessage(\n",
        "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\"\n",
        "    )\n",
        "    messages = [\n",
        "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
        "        user_message,\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\n",
        "        \"draft\": response.content,\n",
        "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "# Provides feedback or critique on the draft\n",
        "def reflection_node(state: AgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=REFLECTION_PROMPT),\n",
        "        HumanMessage(content=state[\"draft\"]),\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"critique\": response.content}\n",
        "\n",
        "\n",
        "# Conducts research based on the critique\n",
        "def research_critique_node(state: AgentState):\n",
        "    queries = model.with_structured_output(Queries).invoke(\n",
        "        [\n",
        "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
        "            HumanMessage(content=state[\"critique\"]),\n",
        "        ]\n",
        "    )\n",
        "    content = state[\"content\"] or []\n",
        "    for q in queries.queries:\n",
        "        response = tavily.search(query=q, max_results=2)\n",
        "        for r in response[\"results\"]:\n",
        "            content.append(r[\"content\"])\n",
        "    return {\"content\": content}\n",
        "\n",
        "\n",
        "# Determines whether the critique and research cycle should\n",
        "# continue based on the number of revisions\n",
        "def should_continue(state):\n",
        "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
        "        return END\n",
        "    return \"reflect\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48569416595a"
      },
      "source": [
        "### Define and compile the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86567ad87aa2"
      },
      "outputs": [],
      "source": [
        "# Initialize the state graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes for each step in the workflow\n",
        "builder.add_node(\"planner\", plan_node)\n",
        "builder.add_node(\"generate\", generation_node)\n",
        "builder.add_node(\"reflect\", reflection_node)\n",
        "builder.add_node(\"research_plan\", research_plan_node)\n",
        "builder.add_node(\"research_critique\", research_critique_node)\n",
        "\n",
        "# Set the entry point of the workflow\n",
        "builder.set_entry_point(\"planner\")\n",
        "\n",
        "# Add conditional edges for task continuation or end\n",
        "builder.add_conditional_edges(\n",
        "    \"generate\", should_continue, {END: END, \"reflect\": \"reflect\"}\n",
        ")\n",
        "\n",
        "# Define task sequence edges\n",
        "builder.add_edge(\"planner\", \"research_plan\")\n",
        "builder.add_edge(\"research_plan\", \"generate\")\n",
        "\n",
        "builder.add_edge(\"reflect\", \"research_critique\")\n",
        "builder.add_edge(\"research_critique\", \"generate\")\n",
        "\n",
        "# Compile the graph with memory state management\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44d87b0a2052"
      },
      "source": [
        "### Show the compiled graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c3170874384"
      },
      "outputs": [],
      "source": [
        "Image(graph.get_graph().draw_mermaid_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95bebc7e74c"
      },
      "source": [
        "### Run the agent - write on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a849843b454"
      },
      "outputs": [],
      "source": [
        "# Define the topic of the essay\n",
        "ESSAY_TOPIC = \"What were the impacts of Hurricane Helene and Hurricane Milton in 2024?\"\n",
        "\n",
        "# Define a thread configuration with a unique thread ID\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Stream through the graph execution with an initial task and state\n",
        "for s in graph.stream(\n",
        "    {\n",
        "        \"task\": ESSAY_TOPIC,  # Initial task\n",
        "        \"max_revisions\": 2,  # Maximum number of revisions allowed\n",
        "        \"revision_number\": 1,  # Current revision number\n",
        "        \"content\": [],  # Initial empty content list\n",
        "    },\n",
        "    thread,\n",
        "):\n",
        "    pprint(s.keys())  # Output each step in the stream\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c95d0cd7f6a"
      },
      "source": [
        "### Output the final draft of the essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e50e674081f2"
      },
      "outputs": [],
      "source": [
        "display(Markdown(s[\"generate\"][\"draft\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01d26c8df5b"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [Google Cloud Generative AI repository on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/)\n",
        "- [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs)\n",
        "- [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview)\n",
        "- [LangGraph tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n",
        "- [DeepLearning.AI course on AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ai_agents_for_engineers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
