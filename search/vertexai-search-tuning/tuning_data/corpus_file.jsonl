{"_id": "ans0001", "text": "" }
{"_id": "ans0002", "text": "Cluster Autoscaler is a standalone program that adjusts the size of a Kubernetes cluster to meet the current needs." }
{"_id": "ans0003", "text": "Cluster Autoscaler increases the size of the cluster when:  * there are pods that failed to schedule on any of the current nodes due to insufficient resources. * adding a node similar to the nodes currently present in the cluster would help.  Cluster Autoscaler decreases the size of the cluster when some nodes are consistently unneeded for a significant amount of time. A node is unneeded when it has low utilization and all of its important pods can be moved elsewhere." }
{"_id": "ans0004", "text": "* Pods with restrictive PodDisruptionBudget. * Kube-system pods that:   * are not run on the node by default, *   * don't have a [pod disruption budget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work) set or their PDB is too restrictive (since CA 0.6). * Pods that are not backed by a controller object (so not created by deployment, replica set, job, stateful set etc). * * Pods with local storage **. *   * unless the pod has the following annotation set:        ```       \"cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes\": \"volume-1,volume-2,..\"       ```        and all of the pod's local volumes are listed in the annotation value. * Pods that cannot be moved elsewhere due to scheduling constraints. CA simulates kube-scheduler behavior, and if there's no other node where a given pod can schedule, the pod's node won't be scaled down.   * This can be particularly visible if a given workloads' pods are configured to only fit one pod per node on some subset of nodes. Such pods will always block CA from scaling down their nodes, because all     other valid nodes are either taken by another pod, or empty (and CA prefers scaling down empty nodes).   * Examples of scenarios where scheduling constraints prevent CA from deleting a node:     * No other node has enough resources to satisfy a pod's request     * No other node has available ports to satisfy a pod's `hostPort` configuration.     * No other node with enough resources has the labels required by a pod's node selector * Pods that have the following annotation set:  ``` \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\" ```  <sup>*</sup>Unless the pod has the following annotation (supported in CA 1.0.3 or later):  ``` \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\" ```  __Or__ you have overridden this behaviour with one of the relevant flags. [See below for more information on these flags.](#what-are-the-parameters-to-ca)  <sup>**</sup>Local storage in this case considers a Volume configured with properties making it a local Volume, such as the following examples:  * [`hostPath`](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) * [`emptyDir`](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir) which does __not__ use \"Memory\" for its `emptyDir.medium` field  ConfigMaps, Secrets, Projected volumes and emptyDir with `medium=Memory` are not considered local storage." }
{"_id": "ans0005", "text": "See [Cluster Autoscaler Releases](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#releases)." }
{"_id": "ans0006", "text": "Since version 1.0.0 we consider CA as GA. It means that:  * We have enough confidence that it does what it is expected to do. Each commit goes through a big suite of unit tests    with more than 75% coverage (on average). We have a series of e2e tests that validate that CA works well on    [GCE](https://testgrid.k8s.io/sig-autoscaling#gce-autoscaling)    and [GKE](https://testgrid.k8s.io/sig-autoscaling#gke-autoscaling).    Due to the missing testing infrastructure, AWS (or any other cloud provider) compatibility    tests are not the part of the standard development or release procedure.    However there is a number of AWS users who run CA in their production environment and submit new code, patches and bug reports. * It was tested that CA scales well. CA should handle up to 1000 nodes running 30 pods each. Our testing procedure is described    [here](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/scalability_tests.md). * Most of the pain-points reported by the users (like too short graceful termination support) were fixed, however    some of the less critical feature requests are yet to be implemented. * CA has decent monitoring, logging and eventing. * CA tries to handle most of the error situations in the cluster (like cloud provider stockouts, broken nodes, etc). The cases handled can however vary from cloudprovider to cloudprovider. * CA developers are committed to maintaining and supporting CA in the foreseeable future.  All of the previous versions (earlier than 1.0.0) are considered beta." }
{"_id": "ans0007", "text": "The main purpose of Cluster Autoscaler is to get pending pods a place to run. Cluster Autoscaler periodically checks whether there are any pending pods and increases the size of the cluster if it makes sense and if the scaled up cluster is still within the user-provided constraints. The time of new node provisioning doesn't depend on CA, but rather on the cloud provider and other Kubernetes components.  So, the main SLO for CA would be expressed in the latency time measured from the time a pod is marked as unschedulable (by K8S scheduler) to the time CA issues scale-up request to the cloud provider (assuming that happens). During our scalability tests (described [here](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/scalability_tests.md)) we aimed at max 20sec latency, even in the big clusters. We reach these goals on GCE on our test cases, however in practice, the performance may differ. Hence, users should expect:  * No more than 30 sec latency on small clusters (less than 100 nodes with up to 30 pods each), with the average latency of about 5 sec. * No more than 60 sec latency on big clusters (100 to 1000 nodes), with average latency of about 15 sec.  Please note that the above performance can be achieved only if NO pod affinity and anti-affinity is used on any of the pods. Unfortunately, the current implementation of the affinity predicate in scheduler is about 3 orders of magnitude slower than for all other predicates combined, and it makes CA hardly usable on big clusters.  It is also important to request full 1 core (or make it available) for CA pod in a bigger clusters. Putting CA on an overloaded node would not allow to reach the declared performance.  We didn't run any performance tests on clusters bigger than 1000 nodes, and supporting them was not a goal for 1.0.  More SLOs may be defined in the future." }
{"_id": "ans0008", "text": "Horizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough resources, CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will stop some of the replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate such unneeded nodes." }
{"_id": "ans0009", "text": "* Do not modify the nodes belonging to autoscaled node groups directly. All nodes within the same node group should have the same capacity, labels and system pods running on them. * Specify requests for your pods. * Use PodDisruptionBudgets to prevent pods from being deleted too abruptly (if needed). * Check if your cloud provider's quota is big enough before specifying min/max settings for your node pools. * Do not run any additional node group autoscalers (especially those from your cloud provider)." }
{"_id": "ans0010", "text": "No." }
{"_id": "ans0011", "text": "Cluster Autoscaler makes sure that all pods in the cluster have a place to run, no matter if there is any CPU load or not. Moreover, it tries to ensure that there are no unneeded nodes in the cluster.  CPU-usage-based (or any metric-based) cluster/node group autoscalers don't care about pods when scaling up and down. As a result, they may add a node that will not have any pods, or remove a node that has some system-critical pods on it, like kube-dns. Usage of these autoscalers with Kubernetes is discouraged." }
{"_id": "ans0012", "text": "No. CPU-based (or any metric-based) cluster/node group autoscalers, like [GCE Instance Group Autoscaler](https://cloud.google.com/compute/docs/autoscaler/), are NOT compatible with CA. They are also not particularly suited to use with Kubernetes in general." }
{"_id": "ans0013", "text": "Since version 1.1 (to be shipped with Kubernetes 1.9), CA takes pod priorities into account.  Pod Priority and Preemption feature enables scheduling pods based on priorities if there is not enough resources. On the other hand, Cluster Autoscaler makes sure that there is enough resources to run all pods. In order to allow users to schedule \"best-effort\" pods, which shouldn't trigger Cluster Autoscaler actions, but only run when there are spare resources available, we introduced priority cutoff to Cluster Autoscaler.  Pods with priority lower than this cutoff:  * don't trigger scale-ups - no new node is added in order to run them, * don't prevent scale-downs - nodes running such pods can be terminated.  Nothing changes for pods with priority greater or equal to cutoff, and pods without priority.  Default priority cutoff is -10 (since version 1.12, was 0 before that). It can be changed using `--expendable-pods-priority-cutoff` flag, but we discourage it. Cluster Autoscaler also doesn't trigger scale-up if an unschedulable pod is already waiting for a lower priority pod preemption.  Older versions of CA won't take priorities into account.  More about Pod Priority and Preemption:  * [Priority in Kubernetes API](https://github.com/kubernetes/design-proposals-archive/blob/main/scheduling/pod-priority-api.md), * [Pod Preemption in Kubernetes](https://github.com/kubernetes/design-proposals-archive/blob/main/scheduling/pod-preemption.md), * [Pod Priority and Preemption tutorial](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/)." }
{"_id": "ans0014", "text": "Cluster Autoscaler terminates the underlying instance in a cloud-provider-dependent manner.  It does _not_ delete the [Node object](https://kubernetes.io/docs/concepts/architecture/nodes/#api-object) from Kubernetes. Cleaning up Node objects corresponding to terminated instances is the responsibility of the [cloud node controller](https://kubernetes.io/docs/concepts/architecture/cloud-controller/#node-controller), which can run as part of [kube-controller-manager](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/) or [cloud-controller-manager](https://kubernetes.io/docs/concepts/architecture/cloud-controller/)." }
{"_id": "ans0015", "text": "" }
{"_id": "ans0016", "text": "Startup taints are meant to be used when there is an operation that has to complete before any pods can run on the node, e.g. drivers installation.  Cluster Autoscaler treats nodes tainted with `startup taints` as unready, but taken into account during scale up logic, assuming they will become ready shortly.  __However, if the substantial number of nodes are tainted with `startup taints` (and therefore unready) for an extended period of time the Cluster Autoscaler might stop working as it might assume the cluster is broken and should not be scaled (creating new nodes doesn't help as they don't become ready).__  Startup taints are defined as:  * all taints with the prefix `startup-taint.cluster-autoscaler.kubernetes.io/`, * all taints defined using `--startup-taint` flag." }
{"_id": "ans0017", "text": "Status taints are meant to be used when a given node should not be used to run pods for the time being.  Cluster Autoscaler internally treats nodes tainted with `status taints` as ready, but filtered out during scale up logic.  This means that even though the node is ready, no pods should run there as long as the node is tainted and if necessary a scale-up should occur.  Status taints are defined as:  * all taints with the prefix `status-taint.cluster-autoscaler.kubernetes.io/`, * all taints defined using `--status-taint` flag." }
{"_id": "ans0018", "text": "Ignore taints are now deprecated and treated as startup taints.  Ignore taints are defined as:  * all taints with the prefix `ignore-taint.cluster-autoscaler.kubernetes.io/`, * all taints defined using `--ignore-taint` flag.  ****************  # How to?" }
{"_id": "ans0019", "text": "CA 0.6 introduced `--balance-similar-node-groups` flag to support this use case. If you set the flag to true, CA will automatically identify node groups with the same instance type and the same set of labels (except for automatically added zone label) and try to keep the sizes of those node groups balanced.  This does not guarantee similar node groups will have exactly the same sizes:  * Currently the balancing is only done at scale-up. Cluster Autoscaler will   still scale down underutilized nodes regardless of the relative sizes of underlying   node groups. We plan to take balancing into account in scale-down in the future. * Cluster Autoscaler will only add as many nodes as required to run all existing   pods. If the number of nodes is not divisible by the number of balanced node   groups, some groups will get 1 more node than others. * Cluster Autoscaler will only balance between node groups that can support the   same set of pending pods. If you run pods that can only go to a single node group   (for example due to nodeSelector on zone label) CA will only add nodes to   this particular node group.  You can opt-out a node group from being automatically balanced with other node groups using the same instance type by giving it any custom label." }
{"_id": "ans0020", "text": "Cluster Autoscaler provides metrics and livenessProbe endpoints. By default they're available on port 8085 (configurable with `--address` flag), respectively under `/metrics` and `/health-check`.  Metrics are provided in Prometheus format and their detailed description is available [here](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/metrics.md)." }
{"_id": "ans0021", "text": "By default, the Cluster Autoscaler will deduplicate similar events that occur within a 5 minute window. This is done to improve scalability performance where many similar events might be triggered in a short timespan, such as when there are too many unscheduled pods.  In some cases, such as for debugging or when scalability of events is not an issue, you might want to see all the events coming from the Cluster Autoscaler. In these scenarios you should use the `--record-duplicated-events` command line flag." }
{"_id": "ans0022", "text": "Prior to version 0.6, Cluster Autoscaler was not touching nodes that were running important kube-system pods like DNS, Metrics Server, Dashboard, etc. If these pods landed on different nodes, CA could not scale the cluster down and the user could end up with a completely empty 3 node cluster. In 0.6, we added an option to tell CA that some system pods can be moved around. If the user configures a [PodDisruptionBudget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/) for the kube-system pod, then the default strategy of not touching the node running this pod is overridden with PDB settings. So, to enable kube-system pods migration, one should set [minAvailable](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#poddisruptionbudget-v1-policy) to 0 (or <= N if there are N+1 pod replicas.) See also [I have a couple of nodes with low utilization, but they are not scaled down. Why?](#i-have-a-couple-of-nodes-with-low-utilization-but-they-are-not-scaled-down-why)" }
{"_id": "ans0023", "text": "From CA 0.6 for GCE/GKE and CA 0.6.1 for AWS, it is possible to scale a node group to 0 (and obviously from 0), assuming that all scale-down conditions are met.  For AWS, if you are using `nodeSelector`, you need to tag the ASG with a node-template key `\"k8s.io/cluster-autoscaler/node-template/label/\"`.  For example, for a node label of `foo=bar`, you would tag the ASG with:  ``` {     \"ResourceType\": \"auto-scaling-group\",     \"ResourceId\": \"foo.example.com\",     \"PropagateAtLaunch\": true,     \"Value\": \"bar\",     \"Key\": \"k8s.io/cluster-autoscaler/node-template/label/foo\" } ```" }
{"_id": "ans0024", "text": "From CA 1.0, node will be excluded from scale-down if it has the annotation preventing scale-down:  ``` \"cluster-autoscaler.kubernetes.io/scale-down-disabled\": \"true\" ```  It can be added to (or removed from) a node using kubectl:  ``` kubectl annotate node <nodename> cluster-autoscaler.kubernetes.io/scale-down-disabled=true ```" }
{"_id": "ans0025", "text": "CA might scale down non-empty nodes with utilization below a threshold (configurable with `--scale-down-utilization-threshold` flag).  To prevent this behavior, set the utilization threshold to `0`." }
{"_id": "ans0026", "text": "There are multiple flags which can be used to configure scale up and scale down delays.  In some environments, you may wish to give the k8s scheduler a bit more time to schedule a pod than the CA's scan-interval. One way to do this is by setting `--new-pod-scale-up-delay`, which causes the CA to ignore unschedulable pods until they are a certain \"age\", regardless of the scan-interval. This setting can be overridden per pod through `cluster-autoscaler.kubernetes.io/pod-scale-up-delay` annotation. If k8s has not scheduled them by the end of that delay, then they may be considered by the CA for a possible scale-up.  ``` \"cluster-autoscaler.kubernetes.io/pod-scale-up-delay\": \"600s\" ```  Scaling down of unneeded nodes can be configured by setting `--scale-down-unneeded-time`. Increasing value will make nodes stay up longer, waiting for pods to be scheduled while decreasing value will make nodes be deleted sooner." }
{"_id": "ans0027", "text": "Below solution works since version 1.1 (to be shipped with Kubernetes 1.9).  Overprovisioning can be configured using deployment running pause pods with very low assigned priority (see [Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/)) which keeps resources that can be used by other pods. If there is not enough resources then pause pods are preempted and new pods take their place. Next pause pods become unschedulable and force CA to scale up the cluster.  The size of overprovisioned resources can be controlled by changing the size of pause pods and the number of replicas. This way you can configure static size of overprovisioning resources (i.e. 2 additional cores). If we want to configure dynamic size (i.e. 20% of resources in the cluster) then we need to use [Horizontal Cluster Proportional Autoscaler](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler) which will change number of pause pods depending on the size of the cluster. It will increase the number of replicas when cluster grows and decrease the number of replicas if cluster shrinks.  Configuration of dynamic overprovisioning:  1. (For 1.10, and below) Enable priority preemption in your cluster.  For GCE, it can be done by exporting following env variables before executing kube-up (more details [here](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/)):  ```sh export KUBE_RUNTIME_CONFIG=scheduling.k8s.io/v1alpha1=true export ENABLE_POD_PRIORITY=true ```  For AWS using kops, see [this issue](https://github.com/kubernetes/autoscaler/issues/1410#issuecomment-439840945).  2. Define priority class for overprovisioning pods. Priority -10 will be reserved for overprovisioning pods as it is the lowest priority that triggers scaling clusters. Other pods need to use priority 0 or higher in order to be able to preempt overprovisioning pods. You can use following definitions.  ```yaml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata:   name: overprovisioning value: -10 globalDefault: false description: \"Priority class used by overprovisioning.\" ```  3. Create service account that will be used by Horizontal Cluster Proportional Autoscaler which needs specific roles. More details [here](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler/tree/master/examples#rbac-configurations)  4. Create deployments that will reserve resources. \"overprovisioning\" deployment will reserve resources and \"overprovisioning-autoscaler\" deployment will change the size of reserved resources. You can use following definitions (you need to change service account for \"overprovisioning-autoscaler\" deployment to the one created in the previous step):  ```yaml apiVersion: apps/v1 kind: Deployment metadata:   name: overprovisioning   namespace: default spec:   replicas: 1   selector:     matchLabels:       run: overprovisioning   template:     metadata:       labels:         run: overprovisioning     spec:       priorityClassName: overprovisioning       terminationGracePeriodSeconds: 0       containers:       - name: reserve-resources         image: registry.k8s.io/pause:3.9         resources:           requests:             cpu: \"200m\"  --- apiVersion: apps/v1 kind: Deployment metadata:   name: overprovisioning-autoscaler   namespace: default   labels:     app: overprovisioning-autoscaler spec:   selector:     matchLabels:       app: overprovisioning-autoscaler   replicas: 1   template:     metadata:       labels:         app: overprovisioning-autoscaler     spec:       containers:         - image: registry.k8s.io/cluster-proportional-autoscaler-amd64:1.8.1           name: autoscaler           command:             - /cluster-proportional-autoscaler             - --namespace=default             - --configmap=overprovisioning-autoscaler             - --default-params={\"linear\":{\"coresPerReplica\":1}}             - --target=deployment/overprovisioning             - --logtostderr=true             - --v=2       serviceAccountName: cluster-proportional-autoscaler-service-account ```" }
{"_id": "ans0028", "text": "Cluster Autoscaler will evict DaemonSets based on its configuration, which is common for the entire cluster. It is possible, however, to specify the desired behavior on a per pod basis. All DaemonSet pods will be evicted when they have the following annotation.  ``` \"cluster-autoscaler.kubernetes.io/enable-ds-eviction\": \"true\" ```  It is also possible to disable DaemonSet pods eviction expicitly:  ``` \"cluster-autoscaler.kubernetes.io/enable-ds-eviction\": \"false\" ```  Note that this annotation needs to be specified on DaemonSet pods, not the DaemonSet object itself. In order to do that for all DaemonSet pods, it is sufficient to modify the pod spec in the DaemonSet object.  This annotation has no effect on pods that are not a part of any DaemonSet." }
{"_id": "ans0029", "text": "Kubernetes scheduler will fail to schedule a Pod to a Node if the Node's max volume count is exceeded. In such case to enable Cluster Autoscaler to scale up in a Kubernetes cluster with [CSI migration](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/625-csi-migration/README.md) enabled, the appropriate CSI related feature gates have to be specified for the Cluster Autoscaler (if the corresponding feature gates are not enabled by default).  For example:  ``` --feature-gates=CSIMigration=true,CSIMigration{Provdider}=true,InTreePlugin{Provider}Unregister=true ```  For a complete list of the feature gates and their default values per Kubernetes versions, refer to the [Feature Gates documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)." }
{"_id": "ans0030", "text": "Provisioning Request (abbr. ProvReq) is a new namespaced Custom Resource that aims to allow users to ask CA for capacity for groups of pods. For a detailed explanation of the ProvisioningRequest API, please refer to the [original proposal](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/provisioning-request.md).  #### Enabling ProvisioningRequest Support  1. __Cluster Autoscaler Version__: Ensure you are using Cluster Autoscaler version 1.30.1 or later.  2. __Feature Flag__: Enable ProvisioningRequest support by setting the following flag in your Cluster Autoscaler configuration: `--enable-provisioning-requests=true`.  3. __Content Type__: Set [API content type flag](https://github.com/kubernetes/autoscaler/blob/522c6fcc06c8cf663175ba03549773cc66a02837/cluster-autoscaler/main.go#L114) to application/json in your Cluster Autoscaler configuration: `--kube-api-content-type application/json`.  4. __RBAC permissions__: Ensure your cluster-autoscaler pod has the necessary permissions to interact with ProvisioningRequests and PodTemplates:  ``` apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   name: cluster-autoscaler-provisioning rules:   - apiGroups:     - \"autoscaling.x-k8s.io\"     resources:     - provisioningrequests     - provisioningrequests/status     verbs: [\"watch\", \"list\", \"get\", \"create\", \"update\", \"patch\", \"delete\"]   - apiGroups: [\"\"]     resources: [\"podtemplates\"]     verbs: [\"watch\", \"list\", \"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: cluster-autoscaler-provisioning-binding roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: cluster-autoscaler-provisioning subjects: - kind: ServiceAccount   name: cluster-autoscaler   namespace: kube-system ```  5. Deploy the [ProvisioningRequest CRD](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/apis/config/crd/autoscaling.x-k8s.io_provisioningrequests.yaml)  #### Supported ProvisioningClasses  Currently, ClusterAutoscaler supports following ProvisioningClasses:  * `check-capacity.autoscaling.x-k8s.io`. When using this class, Cluster Autoscaler performs following actions:    * __Capacity Check__: Determines if sufficient capacity exists in the cluster to fulfill the ProvisioningRequest.    * __Reservation from other ProvReqs__ (if capacity is available): Reserves this capacity for the ProvisioningRequest for 10 minutes,   preventing other ProvReqs from using it.    * __Condition Updates__:   Adds a Accepted=True condition when ProvReq is accepted by ClusterAutoscaler and ClusterAutoscaler will check capacity for this ProvReq.   Adds a Provisioned=True condition to the ProvReq if capacity is available.   Adds a BookingExpired=True condition when the 10-minute reservation period expires.    Since Cluster Autoscaler version 1.33, it is possible to configure the autoscaler    to process only subset of check capacity ProvisioningRequests and ignore the rest.   It should be done with caution by specifying `--check-capacity-processor-instance=<name>` flag.   Then, ProvReq Parameters map should contain a key \"processorInstance\" with a value equal to the configured instance name.    This allows to run two Cluster Autoscalers in the cluster, but the second instance (likely this with configured instance name)   **should only** handle check capacity ProvisioningRequests and not overlap node groups with the main instance.   It is responsibility of the user to ensure the capacity checks are not overlapping.   Best-effort atomic ProvisioningRequests processing is disabled in the instance that has this flag set.    For backwards compatibility, it is possible to differentiate the ProvReqs by prefixing provisioningClassName with the instance name,   but it is **not recommended** and will be removed in CA 1.35.  * `best-effort-atomic-scale-up.autoscaling.x-k8s.io` (supported from Cluster Autoscaler version 1.30.2 or later). When using this class, Cluster Autoscaler performs following actions:    * __Capacity Check__: Check which pods could be scheduled on existing capacity.    * __ScaleUp Request__: Evaluates if scaling up a node group could fulfill all remaining   requirements of the ProvisioningRequest. The scale-up request will use the  AtomicIncreaseSize method   if a given cloud provider supports it. Note that the ScaleUp result depends on the cloud provider's   implementation of the AtomicIncreaseSize method. If the method is not implemented, the scale-up   request will try to increase the node group atomically but doesn't guarantee atomicity.    * __Reservation from other ProvReqs (if scale up request succeeded)__: Reserves this capacity for the ProvisioningRequest for 10 minutes,   preventing other ProvReqs from using it.    * __Condition Updates__:     * Adds a Accepted=True condition when ProvReq is accepted by ClusterAutoscaler.     * Adds a Provisioned=True condition to the ProvReq if the node group scale up request is successful.     * Adds a BookingExpired=True condition when the 10-minute reservation period expires.    Note: make sure you setup --max-nodes-per-scaleup flag correctly. By default --max-nodes-per-scaleup=1000, so any scale up that   require more than 1000 nodes will be rejected.  #### Example Usage  Deploy the first 2 resources, observe the request being Approved and Provisioned, then deploy the Deployment and observe the Deployment using up the Request.  ```yaml apiVersion: v1 kind: PodTemplate metadata:   name: template template:   spec:     containers:     - image: ubuntu       name: default       resources:         requests:           cpu: \"1\"           memory: 600Mi  --- apiVersion: autoscaling.x-k8s.io/v1 kind: ProvisioningRequest metadata:   name: provider spec:   provisioningClassName: \"best-effort-atomic-scale-up.autoscaling.x-k8s.io\"   podSets:   - podTemplateRef:       name: cluster-autoscaler     count: 10  --- apiVersion: apps/v1 kind: Deployment metadata:   name: consumer   annotations:     autoscaling.x-k8s.io/consume-provisioning-request: provider spec:   replicas: 10   selector:     matchLabels:       app: consumer   template:     metadata:       labels:         app: consumer     spec:       containers:       - name: default         image: ubuntu         resources:           requests:             cpu: \"1\"             memory: 600Mi         args: [\"sleep\"] ```" }
{"_id": "ans0031", "text": "Cluster Autoscaler can be run in batch processing mode for CheckCapacity ProvisioningRequests. In this mode, Cluster Autoscaler processes multiple CheckCapacity ProvisioningRequests in a single iteration. This mode is useful for scenarios where a large number of CheckCapacity ProvisioningRequests need to be processed.  However, enabling batch processing for CheckCapacity ProvisioningRequests can adversely affect the performance of processing other types of ProvisioningRequests and incoming pods since iterations where CheckCapacity ProvisioningRequests are processed will take longer and scale-ups for other types of ProvisioningRequests and incoming pods will not be processed during that time.  #### Enabling Batch Processing  1. **Cluster Autoscaler Version**: Ensure you are using Cluster Autoscaler version 1.32.0 or later.  2. **Feature Flag**: Batch processing is disabled by default, it can be enabled by setting the following flag in your Cluster Autoscaler configuration: `--check-capacity-batch-processing=true`.  3. **Batch Size**: Set the maximum number of CheckCapacity ProvisioningRequests to process in a single iteration by setting the following flag in your Cluster Autoscaler configuration: `--check-capacity-provisioning-request-max-batch-size=<batch-size>`. The default value is 10.  4. **Batch Timebox**: Set the maximum time in seconds that Cluster Autoscaler will spend processing CheckCapacity ProvisioningRequests in a single iteration by setting the following flag in your Cluster Autoscaler configuration: `--check-capacity-provisioning-request-batch-timebox=<timebox>`. The default value is 10s.  ****************  # Internals" }
{"_id": "ans0032", "text": "No. We reserve the right to update them in the future if needed." }
{"_id": "ans0033", "text": "Scale-up creates a watch on the API server looking for all pods. It checks for any unschedulable pods every 10 seconds (configurable by `--scan-interval` flag). A pod is unschedulable when the Kubernetes scheduler is unable to find a node that can accommodate the pod. For example, a pod can request more CPU that is available on any of the cluster nodes. Unschedulable pods are recognized by their PodCondition. Whenever a Kubernetes scheduler fails to find a place to run a pod, it sets \"schedulable\" PodCondition to false and reason to \"unschedulable\".  If there are any items in the unschedulable pods list, Cluster Autoscaler tries to find a new place to run them.  It is assumed that the underlying cluster is run on top of some kind of node groups. Inside a node group, all machines have identical capacity and have the same set of assigned labels. Thus, increasing a size of a node group will create a new machine that will be similar to these already in the cluster - they will just not have any user-created pods running (but will have all pods run from the node manifest and daemon sets.)  Based on the above assumption, Cluster Autoscaler creates template nodes for each of the node groups and checks if any of the unschedulable pods would fit on a new node. While it may sound similar to what the real scheduler does, it is currently quite simplified and may require multiple iterations before all of the pods are eventually scheduled. If there are multiple node groups that, if increased, would help with getting some pods running, different strategies can be selected for choosing which node group is increased. Check [What are Expanders?](#what-are-expanders) section to learn more about strategies.  It may take some time before the created nodes appear in Kubernetes. It almost entirely depends on the cloud provider and the speed of node provisioning, including the [TLS bootstrapping process](https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/). Cluster Autoscaler expects requested nodes to appear within 15 minutes (configured by `--max-node-provision-time` flag.) After this time, if they are still unregistered, it stops considering them in simulations and may attempt to scale up a different group if the pods are still pending. It will also attempt to remove any nodes left unregistered after this time.  > Note: Cluster Autoscaler is __not__ responsible for behaviour and registration > to the cluster of the new nodes it creates. The responsibility of registering the new nodes > into your cluster lies with the cluster provisioning tooling you use. > Example: If you use kubeadm to provision your cluster, it is up to you to automatically > execute `kubeadm join` at boot time via some script." }
{"_id": "ans0034", "text": "Every 10 seconds (configurable by `--scan-interval` flag), if no scale-up is needed, Cluster Autoscaler checks which nodes are unneeded. A node is considered for removal when __all__ below conditions hold:  * The sum of cpu requests and sum of memory requests of all pods running on this node ([DaemonSet pods](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) and [Mirror pods](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/) are included by default but this is configurable with `--ignore-daemonsets-utilization` and `--ignore-mirror-pods-utilization` flags) are smaller   than 50% of the node's allocatable. (Before 1.1.0, node capacity was used   instead of allocatable.) Utilization threshold can be configured using   `--scale-down-utilization-threshold` flag.  * All pods running on the node (except these that run on all nodes by default, like manifest-run pods or pods created by daemonsets) can be moved to other nodes. See [What types of pods can prevent CA from removing a node?](#what-types-of-pods-can-prevent-ca-from-removing-a-node) section for more details on what pods don't fulfill this condition, even if there is space for them elsewhere. While checking this condition, the new locations of all movable pods are memorized. With that, Cluster Autoscaler knows where each pod can be moved, and which nodes depend on which other nodes in terms of pod migration. Of course, it may happen that eventually the scheduler will place the pods somewhere else.  * It doesn't have scale-down disabled annotation (see [How can I prevent Cluster Autoscaler from scaling down a particular node?](#how-can-i-prevent-cluster-autoscaler-from-scaling-down-a-particular-node))  If a node is unneeded for more than 10 minutes, it will be terminated. (This time can be configured by flags - please see [I have a couple of nodes with low utilization, but they are not scaled down. Why?](#i-have-a-couple-of-nodes-with-low-utilization-but-they-are-not-scaled-down-why) section for a more detailed explanation.) Cluster Autoscaler terminates one non-empty node at a time to reduce the risk of creating new unschedulable pods. The next node may possibly be terminated just after the first one, if it was also unneeded for more than 10 min and didn't rely on the same nodes in simulation (see below example scenario), but not together. Empty nodes, on the other hand, can be terminated in bulk, up to 10 nodes at a time (configurable by `--max-empty-bulk-delete` flag.)  What happens when a non-empty node is terminated? As mentioned above, all pods should be migrated elsewhere. Cluster Autoscaler does this by evicting them and tainting the node, so they aren't scheduled there again.  DaemonSet pods may also be evicted. This can be configured separately for empty (i.e. containing only DaemonSet pods) and non-empty nodes with `--daemonset-eviction-for-empty-nodes` and `--daemonset-eviction-for-occupied-nodes` flags, respectively. Note that the default behavior is different on each flag: by default DaemonSet pods eviction will happen only on occupied nodes.  Individual DaemonSet pods can also explicitly choose to be evicted (or not). See [How can I enable/disable eviction for a specific DaemonSet](#how-can-i-enabledisable-eviction-for-a-specific-daemonset) for more details.  Example scenario:  Nodes A, B, C, X, Y. A, B, C are below utilization threshold. In simulation, pods from A fit on X, pods from B fit on X, and pods from C fit on Y.  Node A was terminated. OK, but what about B and C, which were also eligible for deletion? Well, it depends.  Pods from B may no longer fit on X after pods from A were moved there. Cluster Autoscaler has to find place for them somewhere else, and it is not sure that if A had been terminated much earlier than B, there would always have been a place for them. So the condition of having been unneeded for 10 min may not be true for B anymore.  But for node C, it's still true as long as nothing happened to Y. So C can be terminated immediately after A, but B may not.  Cluster Autoscaler does all of this accounting based on the simulations and memorized new pod location. They may not always be precise (pods can be scheduled elsewhere in the end), but it seems to be a good heuristic so far." }
{"_id": "ans0035", "text": "From 0.5 CA (K8S 1.6) respects PDBs. Before starting to terminate a node, CA makes sure that PodDisruptionBudgets for pods scheduled there allow for removing at least one replica. Then it deletes all pods from a node through the pod eviction API, retrying, if needed, for up to 2 min. During that time other CA activity is stopped. If one of the evictions fails, the node is saved and it is not terminated, but another attempt to terminate it may be conducted in the near future." }
{"_id": "ans0036", "text": "CA, from version 1.0, gives pods at most 10 minutes graceful termination time by default (configurable via `--max-graceful-termination-sec`). If the pod is not stopped within these 10 min then the node is terminated anyway. Earlier versions of CA gave 1 minute or didn't respect graceful termination at all." }
{"_id": "ans0037", "text": "From 0.5 CA (K8S 1.6) continues to work even if some nodes are unavailable. The default number of tolerated unready nodes in CA 1.2.1 or earlier is 33% of total nodes in the cluster or up to 3 nodes, whichever is higher. For CA 1.2.2 and later, it's 45% or 3 nodes. This is configurable by `--max-total-unready-percentage` and `--ok-total-unready-count` flags. Once there are more unready nodes in the cluster, CA stops all operations until the situation improves. If there are fewer unready nodes, but they are concentrated in a particular node group, then this node group may be excluded from future scale-ups." }
{"_id": "ans0038", "text": "By default, scale-up is considered up to 10 seconds after pod is marked as unschedulable, and scale-down 10 minutes after a node becomes unneeded. Read [this section](#how-can-i-modify-cluster-autoscaler-reaction-time) to see how you can modify this behaviour.  Assuming default settings, [SLOs described here apply](#what-are-the-service-level-objectives-for-cluster-autoscaler)." }
{"_id": "ans0039", "text": "When HPA is combined with CA, the total time from increased load to new pods running is determined by three major factors:  * HPA reaction time,  * CA reaction time,  * node provisioning time.  By default, pods' CPU usage is scraped by kubelet every 10 seconds, and it is obtained from kubelet by Metrics Server every 1 minute. HPA checks CPU load metrics in Metrics Server every 30 seconds. However, after changing the number of replicas, HPA backs off for 3 minutes before taking further action. So it can be up to 3 minutes before pods are added or deleted, but usually it's closer to 1 minute.  CA should react [as fast as described here](#what-are-the-service-level-objectives-for-cluster-autoscaler), regardless of whether it was HPA or the user that modified the number of replicas. For scale-up, we expect it to be less than 30 seconds in most cases.  Node provisioning time depends mostly on cloud provider. In our experience, on GCE it usually takes 3 to 4 minutes from CA request to when pods can be scheduled on newly created nodes.  Total time is a sum of those steps, and it's usually about 5 minutes. Please note that CA is the least significant factor here.  On the other hand, for scale-down CA is usually the most significant factor, as it doesn't attempt to remove nodes immediately, but only after they've been unneeded for a certain time." }
{"_id": "ans0040", "text": "CA team follows the generic Kubernetes process and submits design proposals [HERE](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/proposals) before starting any significant effort. Some of the not-yet-fully-approved proposals may be hidden among [PRs](https://github.com/kubernetes/autoscaler/pulls)." }
{"_id": "ans0041", "text": "When Cluster Autoscaler identifies that it needs to scale up a cluster due to unschedulable pods, it increases the number of nodes in some node group. When there is one node group, this strategy is trivial. When there is more than one node group, it has to decide which to expand.  Expanders provide different strategies for selecting the node group to which new nodes will be added.  Expanders can be selected by passing the name to the `--expander` flag, i.e. `./cluster-autoscaler --expander=random`.  Currently Cluster Autoscaler has 5 expanders:  * `random` - should be used when you don't have a particular need for the node groups to scale differently.  * `most-pods` - selects the node group that would be able to schedule the most pods when scaling up. This is useful when you are using nodeSelector to make sure certain pods land on certain nodes. Note that this won't cause the autoscaler to select bigger nodes vs. smaller, as it can add multiple smaller nodes at once.  * `least-waste` - this is the default expander, selects the node group that will have the least idle CPU (if tied, unused memory) after scale-up. This is useful when you have different classes of nodes, for example, high CPU or high memory nodes, and only want to expand those when there are pending pods that need a lot of those resources.  * `least-nodes` - selects the node group that will use the least number of nodes after scale-up. This is useful when you want to minimize the number of nodes in the cluster and instead opt for fewer larger nodes. Useful when chained with the `most-pods` expander before it to ensure that the node group selected can fit the most pods on the fewest nodes.  * `price` - select the node group that will cost the least and, at the same time, whose machines would match the cluster size. This expander is described in more details [HERE](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/pricing.md). Currently it works only for GCE, GKE and Equinix Metal (patches welcome.)  * `priority` - selects the node group that has the highest priority assigned by the user. It's configuration is described in more details [here](expander/priority/readme.md)  From 1.23.0 onwards, multiple expanders may be passed, i.e. `.cluster-autoscaler --expander=priority,least-waste`  This will cause the `least-waste` expander to be used as a fallback in the event that the priority expander selects multiple node groups. In general, a list of expanders can be used, where the output of one is passed to the next and the final decision by randomly selecting one. An expander must not appear in the list more than once." }
{"_id": "ans0042", "text": "CA respects `nodeSelector` and `requiredDuringSchedulingIgnoredDuringExecution` in nodeAffinity given that you have labelled your node groups accordingly. If there is a pod that cannot be scheduled with either `nodeSelector` or `requiredDuringSchedulingIgnoredDuringExecution` specified, CA will only consider node groups that satisfy those requirements for expansion.  However, CA does not consider \"soft\" constraints like `preferredDuringSchedulingIgnoredDuringExecution` when selecting node groups. That means that if CA has two or more node groups available for expansion, it will not use soft constraints to pick one node group over another.  ****************" }
{"_id": "ans0043", "text": "The following startup parameters are supported for cluster autoscaler:  | Parameter | Description | Default | | --- | --- | --- | | `add-dir-header` | If true, adds the file directory to the header of the log messages |  | | `address` | The address to expose prometheus metrics. | \":8085\" | | `alsologtostderr` | log to standard error as well as files (no effect when -logtostderr=true) |  | | `async-node-groups` | Whether clusterautoscaler creates and deletes node groups asynchronously. Experimental: requires cloud provider supporting async node group operations, enable at your own risk. |  | | `aws-use-static-instance-list` | Should CA fetch instance types in runtime or use a static list. AWS only |  | | `balance-similar-node-groups` | Detect similar node groups and balance the number of nodes between them |  | | `balancing-ignore-label` | Specifies a label to ignore in addition to the basic and cloud-provider set of labels when comparing if two node groups are similar | [] | | `balancing-label` | Specifies a label to use for comparing if two node groups are similar, rather than the built in heuristics. Setting this flag disables all other comparison logic, and cannot be combined with --balancing-ignore-label. | [] | | `bulk-mig-instances-listing-enabled` | Fetch GCE mig instances in bulk instead of per mig |  | | `bypassed-scheduler-names` | Names of schedulers to bypass. If set to non-empty value, CA will not wait for pods to reach a certain age before triggering a scale-up. |  | | `check-capacity-batch-processing` | Whether to enable batch processing for check capacity requests. |  | | `check-capacity-processor-instance` | Name of the processor instance. Only ProvisioningRequests that define this name in their parameters with the key \"processorInstance\" will be processed by this CA instance. It only refers to check capacity ProvisioningRequests, but if not empty, best-effort atomic ProvisioningRequests processing is disabled in this instance. Not recommended: Until CA 1.35, ProvisioningRequests with this name as prefix in their class will be also processed. |  | | `check-capacity-provisioning-request-batch-timebox` | Maximum time to process a batch of provisioning requests. | 10s | | `check-capacity-provisioning-request-max-batch-size` | Maximum number of provisioning requests to process in a single batch. | 10 | | `cloud-config` | The path to the cloud provider configuration file. Empty string for no configuration file. |  | | `cloud-provider` | Cloud provider type. Available values: [aws,azure,gce,alicloud,cherryservers,cloudstack,baiducloud,magnum,digitalocean,exoscale,externalgrpc,huaweicloud,hetzner,oci,ovhcloud,clusterapi,ionoscloud,kamatera,kwok,linode,bizflycloud,brightbox,equinixmetal,vultr,tencentcloud,civo,scaleway,rancher,volcengine] | \"gce\" | | `cloud-provider-gce-l7lb-src-cidrs` | CIDRs opened in GCE firewall for L7 LB traffic proxy & health checks | 130.211.0.0/22,35.191.0.0/16 | | `cloud-provider-gce-lb-src-cidrs` | CIDRs opened in GCE firewall for L4 LB traffic proxy & health checks | 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 | | `cluster-name` | Autoscaled cluster name, if available |  | | `cluster-snapshot-parallelism` | Maximum parallelism of cluster snapshot creation. | 16 | | `clusterapi-cloud-config-authoritative` | Treat the cloud-config flag authoritatively (do not fallback to using kubeconfig flag). ClusterAPI only |  | | `cordon-node-before-terminating` | Should CA cordon nodes before terminating during downscale process |  | | `cores-total` | Minimum and maximum number of cores in cluster, in the format <min>:<max>. Cluster autoscaler will not scale the cluster beyond these numbers. | \"0:320000\" | | `daemonset-eviction-for-empty-nodes` | DaemonSet pods will be gracefully terminated from empty nodes |  | | `daemonset-eviction-for-occupied-nodes` | DaemonSet pods will be gracefully terminated from non-empty nodes | true | | `debugging-snapshot-enabled` | Whether the debugging snapshot of cluster autoscaler feature is enabled |  | | `drain-priority-config` | List of ',' separated pairs (priority:terminationGracePeriodSeconds) of integers separated by ':' enables priority evictor. Priority evictor groups pods into priority groups based on pod priority and evict pods in the ascending order of group priorities--max-graceful-termination-sec flag should not be set when this flag is set. Not setting this flag will use unordered evictor by default.Priority evictor reuses the concepts of drain logic in kubelet(https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown#migration-from-the-node-graceful-shutdown-feature).Eg. flag usage: '10000:20,1000:100,0:60' |  | | `dynamic-node-delete-delay-after-taint-enabled` | Enables dynamic adjustment of NodeDeleteDelayAfterTaint based of the latency between CA and api-server |  | | `emit-per-nodegroup-metrics` | If true, emit per node group metrics. |  | | `enable-dynamic-resource-allocation` | Whether logic for handling DRA (Dynamic Resource Allocation) objects is enabled. |  | | `enable-proactive-scaleup` | Whether to enable/disable proactive scale-ups, defaults to false |  | | `enable-provisioning-requests` | Whether the clusterautoscaler will be handling the ProvisioningRequest CRs. |  | | `enforce-node-group-min-size` | Should CA scale up the node group to the configured min size if needed. |  | | `estimator` | Type of resource estimator to be used in scale up. Available values: [binpacking] | \"binpacking\" | | `expander` | Type of node group expander to be used in scale up. Available values: [random,most-pods,least-waste,price,priority,grpc]. Specifying multiple values separated by commas will call the expanders in succession until there is only one option remaining. Ties still existing after this process are broken randomly. | \"least-waste\" | | `expendable-pods-priority-cutoff` | Pods with priority below cutoff will be expendable. They can be killed without any consideration during scale down and they don't cause scale up. Pods with null priority (PodPriority disabled) are non expendable. | -10 | | `feature-gates` | A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: |  | | `force-delete-unregistered-nodes` | Whether to enable force deletion of long unregistered nodes, regardless of the min size of the node group the belong to. |  | | `force-ds` | Blocks scale-up of node groups too small for all suitable Daemon Sets pods. |  | | `frequent-loops-enabled` | Whether clusterautoscaler triggers new iterations more frequently when it's needed |  | | `gce-concurrent-refreshes` | Maximum number of concurrent refreshes per cloud object type. | 1 | | `gce-expander-ephemeral-storage-support` | Whether scale-up takes ephemeral storage resources into account for GCE cloud provider (Deprecated, to be removed in 1.30+) | true | | `gce-mig-instances-min-refresh-wait-time` | The minimum time which needs to pass before GCE MIG instances from a given MIG can be refreshed. | 5s | | `gpu-total` | Minimum and maximum number of different GPUs in cluster, in the format <gpu_type>:<min>:<max>. Cluster autoscaler will not scale the cluster beyond these numbers. Can be passed multiple times. CURRENTLY THIS FLAG ONLY WORKS ON GKE. | [] | | `grpc-expander-cert` | Path to cert used by gRPC server over TLS |  | | `grpc-expander-url` | URL to reach gRPC expander server. |  | | `ignore-daemonsets-utilization` | Should CA ignore DaemonSet pods when calculating resource utilization for scaling down |  | | `ignore-mirror-pods-utilization` | Should CA ignore Mirror pods when calculating resource utilization for scaling down |  | | `ignore-taint` | Specifies a taint to ignore in node templates when considering to scale a node group (Deprecated, use startup-taints instead) | [] | | `initial-node-group-backoff-duration` | initialNodeGroupBackoffDuration is the duration of first backoff after a new node failed to start. | 5m0s | | `kube-api-content-type` | Content type of requests sent to apiserver. | \"application/vnd.kubernetes.protobuf\" | | `kube-client-burst` | Burst value for kubernetes client. | 10 | | `kube-client-qps` | QPS value for kubernetes client. | 5 | | `kubeconfig` | Path to kubeconfig file with authorization and master location information. |  | | `kubernetes` | Kubernetes master location. Leave blank for default |  | | `leader-elect` | Start a leader election client and gain leadership before executing the main loop. Enable this when running replicated components for high availability. | true | | `leader-elect-lease-duration` | The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled. | 15s | | `leader-elect-renew-deadline` | The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than the lease duration. This is only applicable if leader election is enabled. | 10s | | `leader-elect-resource-lock` | The type of resource object that is used for locking during leader election. Supported options are 'leases'. | \"leases\" | | `leader-elect-resource-name` | The name of resource object that is used for locking during leader election. | \"cluster-autoscaler\" | | `leader-elect-resource-namespace` | The namespace of resource object that is used for locking during leader election. |  | | `leader-elect-retry-period` | The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled. | 2s | | `log-backtrace-at` | when logging hits line file:N, emit a stack trace | :0 | | `log-dir` | If non-empty, write log files in this directory (no effect when -logtostderr=true) |  | | `log-file` | If non-empty, use this log file (no effect when -logtostderr=true) |  | | `log-file-max-size` | Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. | 1800 | | `log-flush-frequency` | Maximum number of seconds between log flushes | 5s | | `log-json-info-buffer-size` | [Alpha] In JSON format with split output streams, the info messages can be buffered for a while to increase performance. The default value of zero bytes disables buffering. The size can be specified as number of bytes (512), multiples of 1000 (1K), multiples of 1024 (2Ki), or powers of those (3M, 4G, 5Mi, 6Gi). Enable the LoggingAlphaOptions feature gate to use this. |  | | `log-json-split-stream` | [Alpha] In JSON format, write error messages to stderr and info messages to stdout. The default is to write a single stream to stdout. Enable the LoggingAlphaOptions feature gate to use this. |  | | `log-text-info-buffer-size` | [Alpha] In text format with split output streams, the info messages can be buffered for a while to increase performance. The default value of zero bytes disables buffering. The size can be specified as number of bytes (512), multiples of 1000 (1K), multiples of 1024 (2Ki), or powers of those (3M, 4G, 5Mi, 6Gi). Enable the LoggingAlphaOptions feature gate to use this. |  | | `log-text-split-stream` | [Alpha] In text format, write error messages to stderr and info messages to stdout. The default is to write a single stream to stdout. Enable the LoggingAlphaOptions feature gate to use this. |  | | `logging-format` | Sets the log format. Permitted formats: \"json\" (gated by LoggingBetaOptions), \"text\". | \"text\" | | `logtostderr` | log to standard error instead of files | true | | `max-allocatable-difference-ratio` | Maximum difference in allocatable resources between two similar node groups to be considered for balancing. Value is a ratio of the smaller node group's allocatable resource. | 0.05 | | `max-autoprovisioned-node-group-count` | The maximum number of autoprovisioned groups in the cluster.This flag is deprecated and will be removed in future releases. | 15 | | `max-binpacking-time` | Maximum time spend on binpacking for a single scale-up. If binpacking is limited by this, scale-up will continue with the already calculated scale-up options. | 5m0s | | `max-bulk-soft-taint-count` | Maximum number of nodes that can be tainted/untainted PreferNoSchedule at the same time. Set to 0 to turn off such tainting. | 10 | | `max-bulk-soft-taint-time` | Maximum duration of tainting/untainting nodes as PreferNoSchedule at the same time. | 3s | | `max-drain-parallelism` | Maximum number of nodes needing drain, that can be drained and deleted in parallel. | 1 | | `max-empty-bulk-delete` | Maximum number of empty nodes that can be deleted at the same time. DEPRECATED: Use --max-scale-down-parallelism instead. | 10 | | `max-failing-time` | Maximum time from last recorded successful autoscaler run before automatic restart | 15m0s | | `max-free-difference-ratio` | Maximum difference in free resources between two similar node groups to be considered for balancing. Value is a ratio of the smaller node group's free resource. | 0.05 | | `max-graceful-termination-sec` | Maximum number of seconds CA waits for pod termination when trying to scale down a node. This flag is mutually exclusion with drain-priority-config flag which allows more configuration options. | 600 | | `max-inactivity` | Maximum time from last recorded autoscaler activity before automatic restart | 10m0s | | `max-node-group-backoff-duration` | maxNodeGroupBackoffDuration is the maximum backoff duration for a NodeGroup after new nodes failed to start. | 30m0s | | `max-node-provision-time` | The default maximum time CA waits for node to be provisioned - the value can be overridden per node group | 15m0s | | `max-nodegroup-binpacking-duration` | Maximum time that will be spent in binpacking simulation for each NodeGroup. | 10s | | `max-nodes-per-scaleup` | Max nodes added in a single scale-up. This is intended strictly for optimizing CA algorithm latency and not a tool to rate-limit scale-up throughput. | 1000 | | `max-nodes-total` | Maximum number of nodes in all node groups. Cluster autoscaler will not grow the cluster beyond this number. |  | | `max-pod-eviction-time` | Maximum time CA tries to evict a pod before giving up | 2m0s | | `max-scale-down-parallelism` | Maximum number of nodes (both empty and needing drain) that can be deleted in parallel. | 10 | | `max-total-unready-percentage` | Maximum percentage of unready nodes in the cluster. After this is exceeded, CA halts operations | 45 | | `memory-difference-ratio` | Maximum difference in memory capacity between two similar node groups to be considered for balancing. Value is a ratio of the smaller node group's memory capacity. | 0.015 | | `memory-total` | Minimum and maximum number of gigabytes of memory in cluster, in the format <min>:<max>. Cluster autoscaler will not scale the cluster beyond these numbers. | \"0:6400000\" | | `min-replica-count` | Minimum number or replicas that a replica set or replication controller should have to allow their pods deletion in scale down |  | | `namespace` | Namespace in which cluster-autoscaler run. | \"kube-system\" | | `new-pod-scale-up-delay` | Pods less than this old will not be considered for scale-up. Can be increased for individual pods through annotation 'cluster-autoscaler.kubernetes.io/pod-scale-up-delay'. | 0s | | `node-autoprovisioning-enabled` | Should CA autoprovision node groups when needed.This flag is deprecated and will be removed in future releases. |  | | `node-delete-delay-after-taint` | How long to wait before deleting a node after tainting it | 5s | | `node-deletion-batcher-interval` | How long CA ScaleDown gather nodes to delete them in batch. | 0s | | `node-deletion-delay-timeout` | Maximum time CA waits for removing delay-deletion.cluster-autoscaler.kubernetes.io/ annotations before deleting the node. | 2m0s | | `node-group-auto-discovery` | of discoverer>:[<key>[=<value>]] One or more definition(s) of node group auto-discovery. A definition is expressed <name of discoverer>:[<key>[=<value>]]. The `aws`, `gce`, and `azure` cloud providers are currently supported. AWS matches by ASG tags, e.g. `asg:tag=tagKey,anotherTagKey`. GCE matches by IG name prefix, and requires you to specify min and max nodes per IG, e.g. `mig:namePrefix=pfx,min=0,max=10` Azure matches by VMSS tags, similar to AWS. And you can optionally specify a default min and max size, e.g. `label:tag=tagKey,anotherTagKey=bar,min=0,max=600`. Can be used multiple times. | [] | | `node-group-backoff-reset-timeout` | nodeGroupBackoffResetTimeout is the time after last failed scale-up when the backoff duration is reset. | 3h0m0s | | `node-info-cache-expire-time` | Node Info cache expire time for each item. Default value is 10 years. | 87600h0m0s | | `nodes` | sets min,max size and other configuration data for a node group in a format accepted by cloud provider. Can be used multiple times. Format: <min>:<max>:<other...> | [] | | `ok-total-unready-count` | Number of allowed unready nodes, irrespective of max-total-unready-percentage | 3 | | `one-output` | If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) |  | | `parallel-scale-up` | Whether to allow parallel node groups scale up. Experimental: may not work on some cloud providers, enable at your own risk. |  | | `pod-injection-limit` | Limits total number of pods while injecting fake pods. If unschedulable pods already exceeds the limit, pod injection is disabled but pods are not truncated. | 5000 | | `profiling` | Is debug/pprof endpoint enabled |  | | `provisioning-request-initial-backoff-time` | Initial backoff time for ProvisioningRequest retry after failed ScaleUp. | 1m0s | | `provisioning-request-max-backoff-cache-size` | Max size for ProvisioningRequest cache size used for retry backoff mechanism. | 1000 | | `provisioning-request-max-backoff-time` | Max backoff time for ProvisioningRequest retry after failed ScaleUp. | 10m0s | | `record-duplicated-events` | enable duplication of similar events within a 5 minute window. |  | | `regional` | Cluster is regional. |  | | `scale-down-candidates-pool-min-count` | Minimum number of nodes that are considered as additional non empty candidatesfor scale down when some candidates from previous iteration are no longer valid.When calculating the pool size for additional candidates we takemax(#nodes * scale-down-candidates-pool-ratio, scale-down-candidates-pool-min-count). | 50 | | `scale-down-candidates-pool-ratio` | A ratio of nodes that are considered as additional non empty candidates forscale down when some candidates from previous iteration are no longer valid.Lower value means better CA responsiveness but possible slower scale down latency.Higher value can affect CA performance with big clusters (hundreds of nodes).Set to 1.0 to turn this heuristics off - CA will take all nodes as additional candidates. | 0.1 | | `scale-down-delay-after-add` | How long after scale up that scale down evaluation resumes | 10m0s | | `scale-down-delay-after-delete` | How long after node deletion that scale down evaluation resumes, defaults to scanInterval | 0s | | `scale-down-delay-after-failure` | How long after scale down failure that scale down evaluation resumes | 3m0s | | `scale-down-delay-type-local` | Should --scale-down-delay-after-* flags be applied locally per nodegroup or globally across all nodegroups |  | | `scale-down-enabled` | Should CA scale down the cluster | true | | `scale-down-gpu-utilization-threshold` | Sum of gpu requests of all pods running on the node divided by node's allocatable resource, below which a node can be considered for scale down.Utilization calculation only cares about gpu resource for accelerator node. cpu and memory utilization will be ignored. | 0.5 | | `scale-down-non-empty-candidates-count` | Maximum number of non empty nodes considered in one iteration as candidates for scale down with drain.Lower value means better CA responsiveness but possible slower scale down latency.Higher value can affect CA performance with big clusters (hundreds of nodes).Set to non positive value to turn this heuristic off - CA will not limit the number of nodes it considers. | 30 | | `scale-down-simulation-timeout` | How long should we run scale down simulation. | 30s | | `scale-down-unneeded-time` | How long a node should be unneeded before it is eligible for scale down | 10m0s | | `scale-down-unready-enabled` | Should CA scale down unready nodes of the cluster | true | | `scale-down-unready-time` | How long an unready node should be unneeded before it is eligible for scale down | 20m0s | | `scale-down-utilization-threshold` | The maximum value between the sum of cpu requests and sum of memory requests of all pods running on the node divided by node's corresponding allocatable resource, below which a node can be considered for scale down | 0.5 | | `scale-up-from-zero` | Should CA scale up when there are 0 ready nodes. | true | | `scan-interval` | How often cluster is reevaluated for scale up or down | 10s | | `scheduler-config-file` | scheduler-config allows changing configuration of in-tree scheduler plugins acting on PreFilter and Filter extension points |  | | `skip-headers` | If true, avoid header prefixes in the log messages |  | | `skip-log-headers` | If true, avoid headers when opening log files (no effect when -logtostderr=true) |  | | `skip-nodes-with-custom-controller-pods` | If true cluster autoscaler will never delete nodes with pods owned by custom controllers | true | | `skip-nodes-with-local-storage` | If true cluster autoscaler will never delete nodes with pods with local storage, e.g. EmptyDir or HostPath | true | | `skip-nodes-with-system-pods` | If true cluster autoscaler will never delete nodes with pods from kube-system (except for DaemonSet or mirror pods) | true | | `startup-taint` | Specifies a taint to ignore in node templates when considering to scale a node group (Equivalent to ignore-taint) | [] | | `status-config-map-name` | Status configmap name | \"cluster-autoscaler-status\" | | `status-taint` | Specifies a taint to ignore in node templates when considering to scale a node group but nodes will not be treated as unready | [] | | `stderrthreshold` | logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=true) | 2 | | `unremovable-node-recheck-timeout` | The timeout before we check again a node that couldn't be removed before | 5m0s | | `user-agent` | User agent used for HTTP calls. | \"cluster-autoscaler\" | | `v` | number for the log level verbosity |  | | `vmodule` | comma-separated list of pattern=N settings for file-filtered logging (only works for text log format) |  | | `write-status-configmap` | Should CA write status information to a configmap | true |  # Troubleshooting" }
{"_id": "ans0044", "text": "CA doesn't remove underutilized nodes if they are running pods [that it shouldn't evict](#what-types-of-pods-can-prevent-ca-from-removing-a-node). Other possible reasons for not scaling down:  * the node group already has the minimum size,  * node has the scale-down disabled annotation (see [How can I prevent Cluster Autoscaler from scaling down a particular node?](#how-can-i-prevent-cluster-autoscaler-from-scaling-down-a-particular-node))  * node was unneeded for less than 10 minutes (configurable by   `--scale-down-unneeded-time` flag),  * there was a scale-up in the last 10 min (configurable by `--scale-down-delay-after-add` flag),  * there was a failed scale-down for this group in the last 3 minutes (configurable by `--scale-down-delay-after-failure` flag),  * there was a failed attempt to remove this particular node, in which case Cluster Autoscaler   will wait for extra 5 minutes before considering it for removal again,  * using large custom value for `--scale-down-delay-after-delete` or `--scan-interval`, which delays CA action.  * make sure `--scale-down-enabled` parameter in command is not set to false" }
{"_id": "ans0045", "text": "By default, kube-system pods prevent CA from removing nodes on which they are running. Users can manually add PDBs for the kube-system pods that can be safely rescheduled elsewhere:  ``` kubectl create poddisruptionbudget <pdb name> --namespace=kube-system --selector app=<app name> --max-unavailable 1 ```  Here's how to do it for some common pods:  * kube-dns can safely be rescheduled as long as there are supposed to be at least 2 of these pods. In 1.7, this will always be the case. For 1.6 and earlier, edit kube-dns-autoscaler config map as described [here](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/#tuning-autoscaling-parameters), adding preventSinglePointFailure parameter. For example:  ``` linear:'{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}' ```  * Metrics Server is best left alone, as restarting it causes the loss of metrics for >1 minute, as well as metrics in dashboard from the last 15 minutes. Metrics Server downtime also means effective HPA downtime as it relies on metrics. Add PDB for it only if you're sure you don't mind." }
{"_id": "ans0046", "text": "CA doesn't add nodes to the cluster if it wouldn't make a pod schedulable. It will only consider adding nodes to node groups for which it was configured. So one of the reasons it doesn't scale up the cluster may be that the pod has too large (e.g. 100 CPUs), or too specific requests (like node selector), and wouldn't fit on any of the available node types. Another possible reason is that all suitable node groups are already at their maximum size.  If the pending pods are in a [stateful set](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset) and the cluster spans multiple zones, CA may not be able to scale up the cluster, even if it has not yet reached the upper scaling limit in all zones. Stateful set pods require an associated Persistent Volume (PV), which is created before scheduling the pod and CA has no way of influencing the zone choice. The pending pod has a strict constraint to be scheduled in the same zone that the PV is in, so if it is a zone that has already reached the upper scaling limit, CA will not be able to perform a scale-up, even if there are other zones in which nodes could be added. This will manifest itself by following events on the pod:  ``` Events:   Type     Reason             Age   From                Message   ----     ------             ----  -------             -------   Normal   NotTriggerScaleUp  ..    cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)   Warning  FailedScheduling   ..    default-scheduler   No nodes are available that match all of the following predicates:: Insufficient cpu (4), NoVolumeZoneConflict (2) ```  This limitation was solved with [volume topological scheduling](https://github.com/kubernetes/design-proposals-archive/blob/main/storage/volume-topology-scheduling.md) introduced as beta in Kubernetes 1.11 and planned for GA in 1.13. To allow CA to take advantage of topological scheduling, use separate node groups per zone. This way CA knows exactly which node group will create nodes in the required zone rather than relying on the cloud provider choosing a zone for a new node in a multi-zone node group. When using separate node groups per zone, the `--balance-similar-node-groups` flag will keep nodes balanced across zones for workloads that don't require topological scheduling." }
{"_id": "ans0047", "text": "Most likely it's due to a problem with the cluster. Steps to debug:  * Check if cluster autoscaler is up and running. In version 0.5 and later, it periodically publishes the kube-system/cluster-autoscaler-status config map. Check last update time annotation. It should be no more than 3 min (usually 10 sec old).  * Check in the above config map if cluster and node groups are in the healthy state. If not, check if there are unready nodes. If some nodes appear unready despite being Ready in the Node object, check `resourceUnready` count. If there are any nodes marked as `resourceUnready`, it is most likely a problem with the device driver failing to install a new resource (e.g. GPU). `resourceUnready` count is only available in CA version 1.24 and later.  If both the cluster and CA appear healthy:  * If you expect some nodes to be terminated, but they are not terminated for a long   time, check   [I have a couple of nodes with low utilization, but they are not scaled down. Why?](#i-have-a-couple-of-nodes-with-low-utilization-but-they-are-not-scaled-down-why) section.  * If you expect some nodes to be added to make space for pending pods, but they are not added for a long time, check [I have a couple of pending pods, but there was no scale-up?](#i-have-a-couple-of-pending-pods-but-there-was-no-scale-up) section.  * If you have access to the control plane (previously referred to as master) machine, check Cluster Autoscaler logs in `/var/log/cluster-autoscaler.log`. Cluster Autoscaler logs a lot of useful information, including why it considers a pod unremovable or what was its scale-up plan.  * Check events added by CA to the pod object.  * Check events on the kube-system/cluster-autoscaler-status config map.  * If you see failed attempts to add nodes, check if you have sufficient quota on your cloud provider side. If VMs are created, but nodes fail to register, it may be a symptom of networking issues." }
{"_id": "ans0048", "text": "There are three options:  * Logs on the control plane (previously referred to as master) nodes, in `/var/log/cluster-autoscaler.log`. * Cluster Autoscaler 0.5 and later publishes kube-system/cluster-autoscaler-status config map.   To see it, run `kubectl get configmap cluster-autoscaler-status -n kube-system   -o yaml`. * Events:   * on pods (particularly those that cannot be scheduled, or on underutilized       nodes),   * on nodes,   * on kube-system/cluster-autoscaler-status config map." }
{"_id": "ans0049", "text": "By default, the Cluster Autoscaler will be conservative about the log messages that it emits. This is primarily due to performance degradations in scenarios where clusters have a large number of nodes (> 100). In these cases excess log messages will lead to the log storage filling more quickly, and in some cases (eg clusters with >1000 nodes) the processing performance of the Cluster Autoscaler can be impacted.  The `--v` flag controls how verbose the Cluster Autoscaler will be when running. In most cases using a value of `--v=0` or `--v=1` will be sufficient to monitor its activity. If you would like to have more information, especially about the scaling decisions made by the Cluster Autoscaler, then setting a value of `--v=4` is recommended. If you are debugging connection issues between the Cluster Autoscaler and the Kubernetes API server, or infrastructure endpoints, then setting a value of `--v=9` will show all the individual HTTP calls made. Be aware that using verbosity levels higher than `--v=1` will generate an increased amount of logs, prepare your deployments and storage accordingly." }
{"_id": "ans0050", "text": "There are 2 log format options, `text` and `json`. By default (`text`), the Cluster Autoscaler will output logs in the [klog native format](https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog-output).  ``` I0823 17:15:11.472183   29944 main.go:569] Cluster Autoscaler 1.28.0-beta.0 ```  Alternatively, adding the flag `--logging-format=json` changes the [log output to json](https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog-output).  ``` {\"ts\":1692825334994.433,\"caller\":\"cluster-autoscaler/main.go:569\",\"msg\":\"Cluster Autoscaler 1.28.0-beta.0\n\",\"v\":1} ```" }
{"_id": "ans0051", "text": "Whenever Cluster Autoscaler adds or removes nodes it will create events describing this action. It will also create events for some serious errors. Below is the non-exhaustive list of events emitted by CA (new events may be added in future):  * on kube-system/cluster-autoscaler-status config map:   * ScaledUpGroup - CA increased the size of node group, gives       both old and new group size.   * ScaleDownEmpty - CA removed a node with no pods running on it (except       system pods found on all nodes).   * ScaleDown - CA decided to remove a node with some pods running on it.       Event includes names of all pods that will be rescheduled to drain the       node. * on nodes:   * ScaleDown - CA is scaling down the node. Multiple ScaleDown events may be       recorded on the node, describing status of scale-down operation.   * ScaleDownFailed - CA tried to remove the node, but failed. The event       includes error message. * on pods:   * TriggeredScaleUp - CA decided to scale up cluster to make place for this       pod.   * NotTriggerScaleUp - CA couldn't find node group that can be scaled up to       make this pod schedulable.   * ScaleDown - CA will try to evict this pod as part of draining the node.  Example event:  ```sh $ kubectl describe pods memory-reservation-73rl0 --namespace e2e-tests-autoscaling-kncnx Name:   memory-reservation-73rl0  ...  Events:   FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason			Message   ---------	--------	-----	----			-------------	--------	------			-------   1m		1m		1	cluster-autoscaler			Normal		TriggeredScaleUp	pod triggered scale-up, group: https://content.googleapis.com/compute/v1/projects/maciekpytel-dev-playground/zones/us-central1-b/instanceGroups/e2e-test-maciekpytel-minion-group, sizes (current/new): 3/4 ```" }
{"_id": "ans0052", "text": "Cluster Autoscaler will not scale the cluster beyond these limits, but some other external factors could make this happen. Here are some common scenarios.  * Existing nodes were deleted from K8s and the cloud provider, which could cause the cluster fell below the minimum number of nodes. * New nodes were added directly to the cloud provider, which could cause the cluster exceeded the maximum number of nodes. * Cluster Autoscaler was turned on in the middle of the cluster lifecycle, and the initial number of nodes might beyond these limits.  By default, Cluster Autoscaler does not enforce the node group size. If your cluster is below the minimum number of nodes configured for CA, it will be scaled up _only_ in presence of unschedulable pods. On the other hand, if your cluster is above the maximum number of nodes configured for CA, it will be scaled down _only_ if it has unneeded nodes.  Starting with CA 1.26.0, a new flag `--enforce-node-group-min-size` was introduced to enforce the node group minimum size. For node groups with fewer nodes than the configuration, CA will scale them up to the minimum number of nodes. To enable this feature, please set it to `true` in the command." }
{"_id": "ans0053", "text": "Cluster Autoscaler will periodically try to increase the cluster and, once failed, move back to the previous size until the quota arrives or the scale-up-triggering pods are removed.  From version 0.6.2, Cluster Autoscaler backs off from scaling up a node group after failure. Depending on how long scale-ups have been failing, it may wait up to 30 minutes before next attempt.  # Developer" }
{"_id": "ans0054", "text": "Cluster Autoscaler generally tries to use the same go version that is used by embedded Kubernetes code. For example CA 1.21 will use the same go version as Kubernetes 1.21. Only the officially used go version is supported and CA may not compile using other versions.  The source of truth for the used go version is builder/Dockerfile.  Warning: do NOT rely on go version specified in go.mod file. It is only meant to control go mod behavior and is not indicative of the go version actually used by CA. In particular go 1.17 changes go mod behavior in a way that is incompatible with existing Kubernetes tooling. Following [Kubernetes example](https://github.com/kubernetes/kubernetes/pull/105563#issuecomment-960915506) we have decided to pin version specified in go.mod to 1.16 for now (even though both Kubernetes and CA no longer compile using go 1.16)." }
{"_id": "ans0055", "text": "1. Set up environment and build e2e.go as described in the [Kubernetes docs](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md#building-and-running-the-tests). 2. Set up the following env variables:      ```sh     export KUBE_AUTOSCALER_MIN_NODES=3     export KUBE_AUTOSCALER_MAX_NODES=6     export KUBE_ENABLE_CLUSTER_AUTOSCALER=true     export KUBE_AUTOSCALER_ENABLE_SCALE_DOWN=true     ```      This is the minimum number of nodes required for all e2e tests to pass. The tests should also pass if you set higher maximum nodes limit. 3. Run `go run hack/e2e.go -- --verbose-commands --up` to bring up your cluster. 4. SSH to the control plane (previously referred to as master) node and edit `/etc/kubernetes/manifests/cluster-autoscaler.manifest` (you will need sudo for this).     * If you want to test your custom changes set `image` to point at your own CA image.     * Make sure `--scale-down-enabled` parameter in `command` is set to `true`. 5. Run CA tests with:      ```sh     go run hack/e2e.go -- --verbose-commands --test --test_args=\"--ginkgo.focus=\\[Feature:ClusterSizeAutoscaling\"     ```      It will take >1 hour to run the full suite. You may want to redirect output to file, as there will be plenty of it.      Test runner may be missing default credentials. On GCE they can be provided with:      ```sh     gcloud beta auth application-default login     ```  A few tests are specific to GKE and will be skipped if you're running on a different provider.  Please open an issue if you find a failing or flaky test (a PR will be even more welcome)." }
{"_id": "ans0056", "text": "This answer only applies to pull requests containing non-trivial code changes.  Unfortunately we can't automatically run e2e tests on every pull request yet, so for now we need to follow a few manual steps to test that PR doesn't break basic Cluster Autoscaler functionality. We don't require you to follow this whole process for trivial bugfixes or minor changes that don't affect main loop. Just use common sense to decide what is and what isn't required for your change.  To test your PR:  1. Run Cluster Autoscaler e2e tests if you can. We are running our e2e tests on GCE and we    can't guarantee the tests are passing on every cloud provider. 2. If you can't run e2e we ask you to do a following manual test at the minimum, using Cluster-Autoscaler image containing your changes and using configuration required to activate them:   i. Create a deployment. Scale it up, so that some pods don't fit onto existing   nodes. Wait for new nodes to be added by Cluster Autoscaler and confirm all   pods have been scheduled successfully.   ii. Scale the deployment down to a single replica and confirm that the   cluster scales down. 3. Run a manual test following the basic use case of your change. Confirm that    nodes are added or removed as expected. Once again, we ask you to use common    sense to decide what needs to be tested. 4. Describe your testing in PR description or in a separate comment on your PR    (example:    <https://github.com/kubernetes/autoscaler/pull/74#issuecomment-302434795>).  We are aware that this process is tedious and we will work to improve it." }
{"_id": "ans0057", "text": "" }
{"_id": "ans0058", "text": "The Kubernetes Client and all its HttpClient provided implementations should be compatible with IPv6 Kubernetes Clusters." }
{"_id": "ans0059", "text": "Fabric8 Kubernetes Client version 6 introduced more options with regards to dependencies.  1. Have `compile` dependencies on `kubernetes-client` or `openshift-client` - this is no different than what was done with version 5 and before.    If you have done custom development involving effectively internal classes, you'll need to still use this option.  2. Have `compile` dependencies on `kubernetes-client-api` or `openshift-client-api`,    and a runtime dependency on `kubernetes-client` or `openshift-client`.      This option will provide your application with a cleaner compile time classpath.  Furthermore, you will also have choices in the HttpClient implementation that is utilized.    By default, kubernetes-client has a runtime dependency on Vert.x (`kubernetes-httpclient-vertx`).  If you wish to use another HttpClient implementation typically you will exclude `kubernetes-httpclient-vertx` and include the other runtime or compile dependency instead." }
{"_id": "ans0060", "text": "More than likely your project already has transitive dependencies to a conflicting version of the Fabric8 Kubernetes Client. For example spring-cloud-dependencies already depends upon the client. You should fully override the client version in this case via the `kubernetes-client-bom`:  ```xml <dependencyManagement>   <dependencies>     <dependency>       <groupId>io.fabric8</groupId>       <artifactId>kubernetes-client-bom</artifactId>       <version>${fabric8.client.version}</version>       <scope>import</scope>       <type>pom</type>     </dependency>   <!-- ... -->   </dependencies> </dependencyManagement> ```  That will align all of the transitive Fabric8 Kubernetes Client dependencies to you chosen version, fabric8.client.version. Please be aware though that substituting a different major version of the client may not be fully compatible with the existing usage in your dependency - if you run into an issue, please check for or create an issue for an updated version of that project's library that contains a later version of the Fabric8 Kubernetes Client." }
{"_id": "ans0061", "text": "There has been a lot of changes under the covers with thread utilization in the Fabric8 client over the 5.x and 6.x releases. So the exact details of what threads are created / used where will depend on the particular release version.  At the core the thread utilization will depend upon the HTTP client implementation. Per client OkHttp maintains a pool of threads for task execution. It will dedicate 2 threads out of that pool per WebSocket connection. If you have a lot of WebSocket usage (Informer or Watches) with OkHttp, you can expect to see a large number of threads in use.  With the JDK, Jetty, or Vert.x http clients they will maintain a smaller worker pool for all tasks that is typically sized by default based upon your available processors, and typically a selector / coordinator thread(s). It does not matter how many Informers or Watches you run, the same threads are shared.  To ease developer burden the callbacks on Watchers and ResourceEventHandlers will not be done directly by an HTTP client thread, but the order of calls will be guaranteed. This will make the logic you include in those callbacks tolerant to some blocking without compromising the on-going work at the HTTP client level. However, you should avoid truly long running operations as this will cause further events to queue and may eventually case memory issues.      > [!NOTE] > It is recommended with any HTTP client implementation that logic you supply via Watchers, ExecListeners, ResourceEventHandlers, Predicates, Interceptors, LeaderCallbacks, etc. does not execute long running tasks.   On top of the HTTP client threads the Fabric8 client maintains a task thread pool for scheduled tasks and for tasks that are called from WebSocket operations, such as handling input and output streams and ResourceEventHandler call backs. This thread pool defaults to an unlimited number of cached threads, which will be shutdown when the client is closed - that is a sensible default with as the amount of concurrently running async tasks will typically be low. If you would rather take full control over the threading use KubernetesClientBuilder.withExecutor or KubernetesClientBuilder.withExecutorSupplier - however note that constraining this thread pool too much will result in a build up of event processing queues.  Finally, the fabric8 client will use 1 thread per PortForward and an additional thread per socket connection - this may be improved upon in the future." }
{"_id": "ans0062", "text": "Like many Java application the Fabric8 Client utilizes [slf4j](https://www.slf4j.org/). You may configure support for whatever underlying logging framework suits your needs. The logging contexts for the Fabric8 Client follow the standard convention of the package structure - everything from within the client will be rooted at the io.fabric8 context. Third-party dependencies, including the chosen HTTP client implementation, will have different root contexts.  If you are using Pod exec, which can occur indirectly via pod operations like copying files, and not seeing the expected behavior - please enable debug logging for the io.fabric8.kubernetes.client.dsl.internal context. That will provide the stdErr and stdOut as debug logs to further diagnose what is occurring." }
{"_id": "ans0063", "text": "Fabric8 Kubernetes Client provides `ManagedKubernetesClient` and `ManagedOpenShiftClient` as [OSGi Declarative Service](https://docs.osgi.org/specification/osgi.cmpn/7.0.0/service.component.html). In order to use it, you must have [Service Component Runtime (SCR)](https://docs.osgi.org/specification/osgi.cmpn/7.0.0/service.component.html#service.component-service.component.runtime) feature enabled in your OSGi runtime. In [Apache Karaf](https://karaf.apache.org/), you can add this to the Karaf Maven Plugin configuration: ```xml <plugin>   <groupId>org.apache.karaf.tooling</groupId>   <artifactId>karaf-maven-plugin</artifactId>   <version>${karaf.version}</version>   <configuration>     <startupFeatures>       <feature>scr</feature>     </startupFeatures>   </configuration> </plugin> ```  You need to provide component configuration files for the client you're using. For example, in case of [Apache Karaf](https://karaf.apache.org/), place configuration files in this directory: ``` src/ └── main     └── resources         ├── assembly         │ └── etc         │     ├── io.fabric8.kubernetes.client.cfg         │     ├── io.fabric8.openshift.client.cfg ```  Once added KubernetesClient declarative services would be exposed automatically on startup. Then you can inject it in your project. Here is an example using Apache camel's `@BeanInject` annotation:  ```java   @BeanInject   private KubernetesClient kubernetesClient; ```" }
{"_id": "ans0064", "text": "Starting Fabric8 Kubernetes Client v6.1.0, we've changed `NO_PROXY` matching as simple as possible and not support any meta characters. It honors the [GNU WGet Spec](https://www.gnu.org/software/wget/manual/html_node/Proxies.html).  So instead of providing `NO_PROXY` like this:  (Unsupported) :x: ``` NO_PROXY: localhost,127.0.0.1,*.google.com, *.github.com ``` we should provide it like this:  (Supported) :heavy_check_mark: ``` NO_PROXY: localhost,127.0.0.1,.google.com,.github.com ```" }
{"_id": "ans0065", "text": "KubernetesClient loads proxy URL from the following sources (in decreasing order of precedence): - `ConfigBuilder.withHttpProxy` / `ConfigBuilder.withHttpsProxy` - Cluster's `proxy-url` in `~/.kube/config` - System Properties or Environment Variables   - `HTTP_PROXY` : Should be used for HTTP requests (when Kubernetes ApiServer is serving plain HTTP requests)   - `HTTPS_PROXY` : Should be used for HTTPS requests (when Kubernetes ApiServer is serving HTTPS)  URLs with `http`, `https`, and `socks5` schemes are supported." }
{"_id": "ans0066", "text": "Unfortunately it's a little complicated as it depends on what operation you are doing - we'll work towards ensuring the Javadocs are as informative as possible. Here is quick overview:  - Basic mutative operations such as update and all variations of patch (patch, edit, accept, serverSideApply) that operate on a given item - are all locked to the resourceVersion on that item.   If you don't want this behavior then set the resourceVersion to null:  ``` resource.accept(x -> {   resource.getMetadata().setResourceVersion(null);   resource... // some other modifications     }); ```  When the resourceVersion is null for an update the client obtains the latest resourceVersion prior to attempting the `PUT` operation - in a rare circumstance this may still fail due to a concurrent modification.  When the resourceVersion is null for a patch the server will always attempt to perform the patch - but of course there may be conflicts to deal with if there has been an intervening modification.  > [!NOTE] > When using informers - do not make modifications to the resources obtained from the cache - especially to the resourceVersion.  > [!NOTE] > It is not recommended to use serverSideApply directly against modified existing resources - the intent is to apply only the desired state owned by the applier.  See the next topic for more.  - Delete is not locked by default, you may use the applicable lockResourceVersion method if you want the delete to apply only to a specific resourceVersion  - Specialized mutative operations are generally unlocked - this includes scale, rolling operations (resume, pause, restart), and others.   The rationale is that you want the narrow operation to succeed even if there has been an intervening change.   If you encounter a situation where you require these operations be locked, please raise an issue so that we can see about making the locking function applicable.  - A handful of additional operations, such as undo, updateImage, and others are currently locked by default.   This may not be intentional - under the covers this is apply a json patch; older versions of json patching that created the resource diff were unlocked by default.   If you encounter an exception due to a concurrent modification performing an operation that seems like it should ignore that possibility by default please raise an issue.  - Legacy operations such as createOrReplace or replace were effectively unlocked - they would repeatedly retry the operation with the freshest resourceVersion until it succeeded.   These methods have been deprecated because of the complexity of their implementation and the broad unlocking behavior by default could be considered unsafe." }
{"_id": "ans0067", "text": "`createOrReplace` was introduced as an alternative to the kubectl apply operation. Over the years there were quite a few issues highlighting where the behavior was different, and there were only limited workarounds and improvements offered. Given the additional complexity of matching the kubectl client side apply behavior, that was never offered as an option. Now that there is first class support for serverSideApply it can be used, but it comes with a couple of caveats:  - You will want to use forceConflicts - `resource.forceConflicts().serverSideApply()` - this is especially true when acting as a controller updating a resource that [manages](https://kubernetes.io/docs/reference/using-api/server-side-apply/#using-server-side-apply-in-a-controller)  - For some resource types serverSideApply may cause vacuous revisions - see https://github.com/kubernetes/kubernetes/issues/118519 - if you are being informed of modifications on those resources you must either filter those out, don't perform the serverSideApply, or use the [java-operator-sdk](https://github.com/java-operator-sdk/java-operator-sdk) that should handle that possibility already.  - Keep in mind that serverSideApply is not the same as client side apply.   In particular serverSideApply does not treat list merging the same, which may lead to unexpected behavior when list item merge keys are modified by actors other than the manager of that field.   See the upstream issue: https://github.com/kubernetes/kubernetes/issues/118725  - A common pattern for createOrReplace was obtaining the current resource from the api server (possibly via an informer cache), making modifications, then calling createOrReplace.   Doing something similar with serverSideApply will fail as the managedFields will be populated.   While you may be tempted to simply clear the managedFields and the resourceVersion, this is generally not what you should be doing unless you want your logic to assume ownership of every field on the resource.   Instead you should construct the object with the desired state and invoke [serverSideApply()](https://github.com/fabric8io/kubernetes-client/blob/main/doc/CHEATSHEET.md#server-side-apply), letting the server figure how to merge the changes.   If you have a more involved situation please read the [upstream server side apply documentation](https://kubernetes.io/docs/reference/using-api/server-side-apply/) to understand topics like transferring ownership and partial patches.  If the limitations / changes necessary to use serverSideApply are too much, you may also use the createOr method. Instead of: ``` resource.createOrReplace() ``` you would use: ``` resource.unlock().createOr(NonDeletingOperation::update) ``` Or `NonDeletingOperation::patch`. The use of the unlock function is optional and is only needed if you are starting with a item that has the resourceVersion populated.   If you have any concern over replacing concurrent changes you should omit the usage of the unlock function.  The alternative to replace is either serverSideApply - with the same caveats as above - or to use update, but with resourceVersion set to null or usage of the unlock function.   > [!NOTE] > When using informers - do not make modifications to the resources obtained from the cache - especially to the resourceVersion. > If you use the unlock function it will make changes to a copy of your item." }
{"_id": "ans0068", "text": "" }
{"_id": "ans0069", "text": "Kubernetes is designed to be resilient to any individual node failure, master or worker. When a master fails the nodes of the cluster will keep operating, but there can be no changes including pod creation or service member changes until the master is available. When a worker fails, the master stops receiving messages from the worker. If the master does not receive status updates from the worker the node will be marked as NotReady. If a node is NotReady for 5 minutes, the master reschedules all pods that were running on the dead node to other available nodes." }
{"_id": "ans0070", "text": "There is a DNS server called skydns which runs in a pod in the cluster, in the `kube-system` namespace. That DNS server reads from etcd and can serve up dns entries for Kubernetes services to all pods. You can reach any service with the name `<service>.<namespace>.svc.cluster.local`. The resolver automatically searches `<namespace>.svc.cluster.local` dns so that you should be able to call one service to another in the same namespace with just `<service>`." }
{"_id": "ans0071", "text": "The only stateful part of a Kubernetes cluster is the etcd. The master server runs the controller manager, scheduler, and the API server and can be run as replicas. The controller manager and scheduler in the master servers use a leader election system, so only one controller manager and scheduler is active for the cluster at any time. So an HA cluster generally consists of an etcd cluster of 3+ nodes and multiple master nodes.  Learn more: http://kubernetes.io/docs/admin/high-availability/#master-elected-components" }
{"_id": "ans0072", "text": "Yes, network policies allow you to isolate namespaces at the network layer. Full isolation requires use of an overlay network such as Flannel, Calico, Weave, or Romana.  http://kubernetes.io/docs/user-guide/networkpolicies/  # Basic usage questions:" }
{"_id": "ans0073", "text": "Probably not, they are older and have fewer features than the newer Deployment objects." }
{"_id": "ans0074", "text": "Use `kubectl get deployment <deployment>`. If the `DESIRED`, `CURRENT`, `UP-TO-DATE` are all equal, then the Deployment has completed." }
{"_id": "ans0075", "text": "Make sure your `imagePullPolicy` is set to `Always`(this is the default). That means when a pod is deleted, a new pod will ensure it has the current version of the image. Then refresh all your pods.   The simplest way to refresh all your pods is to just delete them and they will be recreated with the latest image. This immediately destroys all your pods which will cause a service outage. Do this with `kubectl delete pod -l <name>=<value>` where name and value are the label selectors your deployment uses.   A better way is to edit your deployment and modify the deployment pod spec to add or change any annotation. This will cause all your pods to be deleted and rescheduled, but this method will also obey your `rollingUpdate` strategy, meaning no downtime assuming your `rollingUpdate` strategy already behaves properly. Setting a timestamp or a version number is convenient, but any change to pod annotations will cause a rolling update. For a deployment named nginx, this can be done with: ``` PATCH='{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"timestamp\":\"'$(date)'\"}}}}}' kubectl patch deployment nginx -p \"$PATCH\" ```  It is considered bad practice to rely on the `:latest` docker image tag in your deployments, because using `:latest` there is no way to rollback or specify what version of your image to use. It's better to update the deployment with an exact version of the image and use `--record` so that you can use `kubectl rollout undo deployment <deployment>` or other commands to manage rollouts." }
{"_id": "ans0076", "text": "A `Pending` pod is one that cannot be scheduled onto a node.  Doing a `kubectl describe pod <pod>` will usually tell you why. `kubectl logs <pod>` can also be helpful. There are several common reasons for pods stuck in Pending:  ** The pod is requesting more resources than are available, a pod has set a `request` for an amount of CPU or memory that is not available anywhere on any node. eg. requesting a 8 CPU cores when all your nodes only have 4 CPU cores. Doing a `kubectl describe node <node>` on each node will also show already requested resources. ** There are `taint`s that prevent a pod from scheduling on your nodes.  ** The nodes have been marked unschedulable with `kubectl cordon` ** There are no `Ready` nodes. `kubectl get nodes` will display the status of all nodes." }
{"_id": "ans0077", "text": "A `ContainerCreating` pod is one that has been scheduled on a node, but cannot startup properly. Doing a `kubectl describe pod <pod>` will usually tell you why. Common reasons include:  ** Container Networking Interface(CNI) errors are preventing the pod networking from being setup properly. Flannel and weave versions or configuration can sometimes cause this. ** Volume mounts failures are preventing startup. External volumes like EBS or GCE PD sometimes cannot be properly attached to the node." }
{"_id": "ans0078", "text": "This is the standard error message when a pod fails with an error. `kubectl describe pod <podid>` usually doesn't provide much helpful information, but `kubectl logs <podid>` would show the stdout from the pod during the most recent execution attempt. Another helpful technique is to change the `spec.containers.command` for your pod to `bash -c '<command> || sleep 10d'`. This will start your container and then if it exits with a non-zero error code it will sleep for 10 days. This will enable you then use `kubectl exec -it <podid> -- bash` to enter a shell in the container while it is still running but after the main command has exited so that you can debug it.  Another common reason is that a node is failing its health check and has been killed by Kubernetes. The pod will generally restart itself after some period of time(a backoff time, hence the CrashLoopBackoff), on the same node. A CrashLoopBackoff does not move a pod to a new node." }
{"_id": "ans0079", "text": "If you apply a change to a Deployment with the `--record` flag then Kubernetes stores the previous Deployment in its history. The `kubectl rollout history deployment <deployment>` command will show prior Deployments. The last Deployment can be restored with the `kubectl rollout undo deployment <deployment>` command. In progress Deployments can also be paused and resumed.   When a new version of a Deployment is applied, a new ReplicaSet object is created which is slowly scaled up while the old ReplicaSet is scaled down. You can look at each ReplicaSet that has been rolled out with `kubectl get replicaset`. Each ReplicaSet is named with the format <deployment>-<pod-template-hash>, so you can also do `kubectl describe replicaset <replicaset>`.  Learn more: http://kubernetes.io/docs/user-guide/kubectl/kubectl_rollout/" }
{"_id": "ans0080", "text": "A DaemonSet is a set of pods that is run only once on a host. It's used for host-layer features, for instance a network, host monitoring or storage plugin or other things which you would never want to run more than once on a host.   Learn more: http://kubernetes.io/docs/admin/daemons/" }
{"_id": "ans0081", "text": "In a regular Deployment all the instances of a pod are exactly the same, they are indistinguishable and are thus sometimes referred to as \"cattle\", these are typically stateless applications that can be easily scaled up and down. In a PetSet, each pod is unique and has an identity that needs to be maintained. This is commonly used for more stateful applications like databases.   Learn more: http://kubernetes.io/docs/user-guide/petset/  In 1.5, PetSets have been renamed to Stateful Sets  Learn more: http://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/" }
{"_id": "ans0082", "text": "An Ingress Controller is a pod that can act as an inbound traffic handler. It is a HTTP reverse proxy that is implemented as a somewhat customizable nginx. Among the features are HTTP path and service based routing and SSL termination.   Learn more: http://kubernetes.io/docs/user-guide/ingress/" }
{"_id": "ans0083", "text": "This occurs due to a race condition during pod deletion between the Ingress and the pod. When a pod is deleted, it can shut down before the Ingress knows to stop sending traffic. So the Ingress may continue to send traffic to a disabled pod.  The simplest way to avoid this is to prevent the pod from shutting down immediately with a `preStop` hook. Adding in a `preStop` hook to the deployment which does `sleep 5` should delay the pod termination long enough to let the Ingress update and remove the disabled pod from its upstream list.  https://github.com/kubernetes/kubernetes/issues/43576 https://github.com/kubernetes/ingress/issues/322 https://github.com/kubernetes/contrib/issues/1140 https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods" }
{"_id": "ans0084", "text": "Within the cluster, most Kubernetes services are implemented as a virtual IP called a ClusterIP. A ClusterIP has a list of pods which are in the service, the client sends IP traffic directly to a randomly selected pod in the service, so the ClusterIP isn't actually directly routable even from kubernetes nodes. This is all done with iptables routes. The iptables configuration is managed by the kube-proxy on each node. So only nodes running kube-proxy can talk to ClusterIP members. An alternative to the ClusterIP is to use a \"Headless\" service by specifying ClusterIP=None, this does not use a virtual IP, but instead just creates a DNS A record for a service that includes all the IP addresses of the pods. The live members of any service are stored in an API object called an `endpoint`. You can see the members of a service by doing a `kubectl get endpoints <service>`" }
{"_id": "ans0085", "text": "There are two ways:  1. Set the service type to NodePort. This makes every node in the cluster listen on the specified NodePort, then any node will forward traffic from that NodePort to a random pod in the service. 1. Set the service type to LoadBalancer. This provisions a NodePort as above, but then does an additional step to provision a load balancer in your cloud(AWS or GKE) automatically. In AWS it also modifies the Auto-Scaling Group of the cluster so all nodes of that ASG are added to the ELB." }
{"_id": "ans0086", "text": "A LoadBalancer by default is set up as a TCP Load Balancer with your cloud provider (AWS or GKE). There is no support in bare metal or OpenStack for Load Balancer types. The Kubernetes controller manager provisions a load balancer in your cloud and puts all of your Kubernetes nodes into the load balancer. Because each node is assumed to be running `kube-proxy` it should be listening on the appropriate NodePort and then it can forward incoming requests to a pod that is available for the service.  Because the LoadBalancer type is by default TCP, not HTTP many higher level features of a LoadBalancer are not available. For instance health checking from the LoadBalancer to the node is done with a TCP check. HTTP X-Forwarded-For information is not available, though it is possible to use proxy protocol in AWS.  http://kubernetes.io/docs/user-guide/services/" }
{"_id": "ans0087", "text": "There are two mechanism for this: taints and node selection.  Kubernetes node selection is described here: http://kubernetes.io/docs/user-guide/node-selection/  In kubernetes 1.3 the concept of a `taint` was implemented. A taint is a way of marking a node for a specific purpose. In order to be scheduled onto a tainted node, a pod must \"tolerate\" the taint.  Use a `taint` to limit the pods that can run on a node, for instance you want to dedicate instances for only a specific kind of pod. On the other hand, use a pod's node selector to limit where a pod can run, for instance if your pod needs specific features on a host like a GPU.  https://github.com/coreos/kubernetes/blob/master/docs/design/taint-toleration-dedicated.md" }
{"_id": "ans0088", "text": "Kubernetes by default does attempt node anti-affinity, but it is not a hard requirement, it is best effort, but will schedule multiple pods on the same node if that is the only way.  Learn more:  http://stackoverflow.com/questions/28918056/does-the-kubernetes-scheduler-support-anti-affinity http://kubernetes.io/docs/user-guide/node-selection/" }
{"_id": "ans0089", "text": "In Kubernetes 1.4 the nodename is available in the downward API in the `spec.nodeName` variable.   http://kubernetes.io/docs/user-guide/downward-api/  In AWS specifically, It may be easier to use the AWS metadata API and just `curl 169.254.169.254/1.0/meta-data/local-ipv4`" }
{"_id": "ans0090", "text": "In v1.3, add a `subdomain` field to the pod specification, then create a Headless service which has the same name as the `subdomain`, then each pod gets a DNS record for <podname>.<subdomain>.<servicename>.svc.cluster.local  In v1.2, a similar mechanism exists but using annotations.  http://kubernetes.io/docs/admin/dns/#a-records-and-hostname-based-on-pods-hostname-and-subdomain-fields" }
{"_id": "ans0091", "text": "See the above answer on \"getting the host IP address from inside a pod\" for an example of using the API inside a pod." }
{"_id": "ans0092", "text": "You can use the following command to get all the pods on a node in kubernetes 1.4  ``` kubectl get po --all-namespaces  -o jsonpath='{range .items[?(@.spec.nodeName ==\"nodename\")]}{.metadata.name}{\"\n\"}{end}' ```" }
{"_id": "ans0093", "text": "Yes, there's an example here of both an NFS client and server running within pods in the cluster: https://github.com/jsafrane/kubernetes-nfs-example" }
{"_id": "ans0094", "text": "Yes. But one major downside of that is that ClusterIPs are implemented as iptables rules on cluster clients, so you'd lose the ability to see Cluster IPs and service changes. Because the iptables are managed by kube-proxy you could do this by running a kube-proxy, which is similar to just joining the cluster. You could make all your services Headless(ClusterIP = None), this would give your external servers the ability to talk directly to services if they could use the kubernetes dns. Headless services don't use ClusterIPs, but instead just create a DNS A record for all pods in the service. kube-dns is run inside the cluster as a ClusterIP, so there's a chicken and egg problem with DNS you would have to deal with." }
{"_id": "ans0095", "text": "Look at the Downward API: http://kubernetes.io/docs/user-guide/downward-api/ It allows your pods to see labels and annotations and a few other variables using either a mount point or environment variables inside the pod. If those don't contain the information you need, you'll likely need to resort to either using the Kubernetes API from within the pod or something entirely separate." }
{"_id": "ans0096", "text": "There is no built in functionality for this. [Helm](https://github.com/kubernetes/helm) is a popular third party choice for this. Some people use scripts or templating tools like jinja." }
{"_id": "ans0097", "text": "Think of `request` as a node scheduling unit and a `limit` as a hard limit when it is already running. Kubernetes will attempt to schedule your pods onto nodes based only on the sum of the existing requests on the node plus the new pod request. A request is only used for scheduling which node a pod runs on. A limit is monitored after a pod has been scheduled and is running. Setting a limit enforces a cap and ensures that a pod does not exceed the limit on a node, killing it does.  For simplicity you can just set request and limit to be the same, but you won't be able to pack things tightly onto a node for efficiency. The converse problem is when you set limits which are much larger than requests there is a danger that pods use resources all the way up to their limit and overrun the node or starve other pods on the node. CPU may not hard-capped depending on the version of Kubernetes/Docker, but memory is. Pods exceeding their memory limit will be terminated and rescheduled." }
{"_id": "ans0098", "text": "Pending usually means that a pod cannot be scheduled, because of a resource limitation, most commonly the cluster can't find a node which has the available CPU and memory requests to satisfy the scheduler. `kubectl describe pod <podid>` will show the reason why the pod can't be scheduled. Pods can remain in the Pending state indefinitely until the resources are available or until you reduce the number of required replicas." }
{"_id": "ans0099", "text": "There's an issue with the kubernetes client not handling terminals correctly. I have a script that I use that solves most of these problems. https://github.com/hubt/kubernetes-faq/tree/master/kshell  https://github.com/kubernetes/kubernetes/issues/13585" }
{"_id": "ans0100", "text": "[Heapster](https://github.com/kubernetes/heapster) is included and its metrics are how Kubernetes measures CPU and memory in order to use horizontal pod autoscaling (HPA). Heapster can be queried directly with its REST API. Prometheus is also more full featured and popular." }
{"_id": "ans0101", "text": "Containers within a pod share networking space and can reach other on `localhost`. For instance, if you have two containers within a pod, a MySQL container running on port 3306, and a PHP container running on port 80, the PHP container could access the MySQL one through `localhost:3306`.  Learn more: https://github.com/kubernetes/kubernetes/blob/release-1.4/docs/design/networking.md#container-to-container" }
{"_id": "ans0102", "text": "Create a special secret in a your namespace that provides the registry and credentials to authenticate with. Then use that secret in the `spec.imagePullSecrets` field of your pod specification. http://kubernetes.io/docs/user-guide/production-pods/#authenticating-with-a-private-image-registry http://kubernetes.io/docs/user-guide/images/#using-a-private-registry" }
{"_id": "ans0103", "text": "Yes. The two tricks are:  Your pod must run in privileged mode. kubelet must run with `--allow-privileged=true`(this is the default) and the pod must run with `securityContext.privileged: true`. This will allow your pod to mount the host docker socket directly.    You must mount the host docker socket by specifying `volumes.hostPath.path: /var/run/docker.sock` in your pod spec.   Here is a simple alpine docker deployment which can run docker commands: ``` apiVersion: extensions/v1beta1 kind: Deployment metadata:   labels:     app: docker   name: docker spec:   template:     metadata:       labels:         app: docker     spec:       containers:       - command: [\"/bin/sleep\",\"365d\"]         image: hubt/alpine-docker         imagePullPolicy: Always         name: docker         securityContext:           privileged: true         terminationMessagePath: /dev/termination-log         volumeMounts:         - mountPath: /var/run/docker.sock           name: docker-socket       imagePullSecrets:       - name: registrypullsecret       volumes:       - hostPath:           path: /var/run/docker.sock         name: docker-socket ```" }
{"_id": "ans0104", "text": "With kubernetes 1.5, Docker versions 1.10.3 - 1.12.3 are supported, with some known issues.  https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#external-dependency-version-information" }
{"_id": "ans0105", "text": "In Kubernetes 1.6, a significant change was made to how the master treats nodes whose state cannot be determined. Pods from the node are put into an `Unknown` state and the master will not attempt to reschedule them.  In Kubernetes 1.5 and earlier, if the node has not sent a heartbeat to the master in 5 minutes, the node is deleted from the masters and the pods are rescheduled automatically.  You can force delete pods with ``` kubectl delete pods <pod> --grace-period=0 --force ``` https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#force-deletion A longer discussion of the reasoning and the change is here: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/pod-safety.md    # AWS Questions:" }
{"_id": "ans0106", "text": "`kube-up.sh` is being deprecated, it will work for relatively straightforward installs, but won't be actively developed anymore. [Kops](https://github.com/kubernetes/kops) is the new recommended deployment tool especially on AWS. It doesn't support other cloud environments yet, but that is planned." }
{"_id": "ans0107", "text": "In addition to regular EC2 ip addresses, Kubernetes creates its own cluster internal network. In AWS, each instance in a kubernetes cluster hosts a small subnet(by default a /24), and each pod on that host get its own IP addresses within that node's /24 address space. Whenever a cluster node is added or deleted, the Kubernetes controller updates the route table so that all nodes in the VPC can route pod IPs directly to that node's subnet." }
{"_id": "ans0108", "text": "If you used `kube-up.sh` or `kops` to provision your cluster, then it created an AutoScaling Group automatically. You can re-scale that with kops, or update the ASG directly, to grow/shrink the cluster. New instances are provisioned for you and should join the cluster automatically (my experience has been it takes 5-7 minutes for nodes to join).   With `kops` the recommended process is to edit the InstanceGroup (ig) and then update your cluster. `kops` also supports multiple instance groups per cluster so you can have multiple Auto Scaling Groups to run multiple types of instances within your cluster. Spot instances are also supported." }
{"_id": "ans0109", "text": "The following project is an autoscaler: https://github.com/kubernetes/contrib/tree/master/cluster-autoscaler  Learn more: https://github.com/kubernetes/kops/blob/master/docs/instance_groups.md" }
{"_id": "ans0110", "text": "Add the following metadata annotation to your LoadBalancer service ``` service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0 ``` Then delete and re-create the service object and it should be a new private ELB." }
{"_id": "ans0111", "text": "Add the following metadata annotation to your LoadBalancer service with a comma separated list of CIDRs: ``` service.beta.kubernetes.io/load-balancer-source-ranges ``` Each ELB gets its own security group and this annotation will add those CIDR addresses to the allowed source IPs  https://github.com/kubernetes/kubernetes/blob/d95b9238877d5a74895189069121328c16e420f5/pkg/api/service/annotations.go#L27-L34" }
{"_id": "ans0112", "text": "Add the following metadata annotations to your service ``` service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http ``` or ``` service.beta.kubernetes.io/aws-load-balancer-backend-protocol: https ``` https://github.com/kubernetes/kubernetes/pull/23495" }
{"_id": "ans0113", "text": "Add the following metadata annotations to your service ``` service.beta.kubernetes.io/aws-load-balancer-ssl-cert=arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012 ```" }
{"_id": "ans0114", "text": "Create a service which listens on port 80 and 443, attach an SSL certificate as above. Then add the following metadata annotation to your service ``` service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\" ``` This tells the ELB that the SSL certificate will only be used for 443 and not 80." }
{"_id": "ans0115", "text": "There is a route table entry for every instance in the cluster which allows other nodes to route to the pods on that node. AWS has a soft limit of 50 routes per route table and a hard limit of 100 routes per route table. So with the default VPC networking you will run into one of these limits. Using one of the overlay networks (Flannel, Weave, Romana) can get around this limit. And kops-routing is a planned feature to do this directly from AWS nodes. A planned feature is for all these networking plugins to be drop in additions to an existing cluster." }
{"_id": "ans0116", "text": "Yes and \"Not out of the box\". Provision with kops and specify the AZ's you want and your cluster will be a multi-AZ cluster within a single region. The AutoScalingGroups can add nodes to any region you specify. The planned solution for a multi-region cluster is to build separate clusters in each region and use federation to manage multiple clusters. http://kubernetes.io/docs/admin/federation/. It is also possible to build a multi-region cluster using an overlay network like Flannel, Calico or Weave." }
{"_id": "ans0117", "text": "The new way going forward to do this will be with the external-dns project. https://github.com/kubernetes-incubator/external-dns  Older project with similar functionality: https://github.com/wearemolecule/route53-kubernetes Future work in core kops: https://github.com/kubernetes/kops/tree/master/dns-controller" }
{"_id": "ans0118", "text": "Yes. When you declare a PersistentVolumeClaim add the annotation: ```     volume.alpha.kubernetes.io/storage-class: \"foo\" ``` And the PersistentVolumeClaim will automatically create a volume for you and delete it when the PersistentVolumeClaim is deleted, \"foo\" is meaningless, the annotation just needs to be set." }
{"_id": "ans0119", "text": "It just works. EBS volumes are specific to an Availability Zone, and Kubernetes knows which AZ a volume is in. When a new pod needs that volume it the pod is  automatically scheduled in the Availability Zone of the volume." }
{"_id": "ans0120", "text": "Not currently." }
{"_id": "ans0121", "text": "This is a third party tool that enables this: https://github.com/jtblin/kube2iam" }
{"_id": "ans0122", "text": "A Kubernetes node has some useful pieces of information attached as labels here are some examples: ```     beta.kubernetes.io/instance-type: c3.xlarge     failure-domain.beta.kubernetes.io/region: us-east-1     failure-domain.beta.kubernetes.io/zone: us-east-1b     kubernetes.io/hostname: ip-172-31-0-10.us-east-1.compute.internal ``` You can use these for AZ awareness or attach your own labels and use the Downward API for additional flexibility." }
{"_id": "ans0123", "text": "With kops, this is possible. But having more than one Kubernetes cluster in a VPC is not supported." }
