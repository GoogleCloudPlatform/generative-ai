{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Search tuning in Vertex AI Search\n",
    "\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\" width=\"25%\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\">\n",
    "      <img width=\"32\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\" width=\"25%\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fsearch%2Fvertexai-search-options%2Fvertexai_search_options.ipynb\">\n",
    "      <img width=\"32\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\" width=\"25%\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/search/vertexai-search-options/vertexai_search_options.ipynb\">\n",
    "      <img width=\"32\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\" width=\"25%\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\">\n",
    "      <img width=\"32\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    " \n",
    "<b>Share to:</b>\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\" width=\"10%\">\n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\" target=\"_blank\">\n",
    "        <img width=\"20\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "    </a>\n",
    "   </td>\n",
    "  <td style=\"text-align: center\" width=\"10%\">\n",
    "    <a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\" target=\"_blank\">\n",
    "        <img width=\"20\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\" width=\"10%\">\n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\" target=\"_blank\">\n",
    "        <img width=\"20\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\" width=\"10%\">\n",
    "    <a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\" target=\"_blank\">\n",
    "        <img width=\"20\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\" width=\"10%\">\n",
    "    <a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/search/vertexai-search-options/vertexai_search_options.ipynb\" target=\"_blank\">\n",
    "        <img width=\"20\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "    </a>\n",
    "  </td>\n",
    "</table>            "
   ],
   "id": "74215b2fa2b4f198"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "When users try to provide a search service over their archived documents and data, search performance may not meet the performance expectation all the time. The performance of Vertex AI Search can be measured in two aspects: the accuracy and the relevance of the search results, and the correctness of the summarized responses from the search results with correct annotations and references to the source document. Among the two aspects of the search performances, the accuracy and the relevance of the search results should be enhanced by generating embedding vectors which are more relevant semantically with document chunking and other document processing methods. The correctness of the summarized responses generated from the backend LLM (Gemini) behind the Vertex AI Search endpoint can be enhanced by tuning the backend LLM with some additional relevant data. The process of tuning the backend LLM with some domain-specific data is what Vertex AI Search Tuning is for.\n",
    "  \n",
    "Before we tune the backend LLM behind Vertex AI Search, we should the prepare the raw text data in a specific JSONL format with a question-answer mapping file in the tab-separated table format. We will use some FAQ documents from an open source project (Kubernetes) to tune the backend LLM to enhance answers on the questions on Kubernetes. After we learn how we prepare the tuning data in JSONL and TSV format, we will learn how we can configure a search tuning job and submit it to Vertex AI.\n",
    "     \n",
    "To learn more about the search tuning process, please refer to the following documents in the Google Cloud Documentation.\n",
    "     \n",
    "[Improve search results with search tuning](https://cloud.google.com/generative-ai-app-builder/docs/tune-search)\n",
    "\n",
    "[Create a search data store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es)\n",
    "\n",
    "[Create a search app](https://cloud.google.com/generative-ai-app-builder/docs/create-engine-es)\n",
    "\n",
    "| |                                                 |\n",
    "|-|-------------------------------------------------|\n",
    "| Author(s) | [Jincheol Kim](https://gitlab.com/jincheolkim/) |"
   ],
   "id": "8446af8a539120fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "![Key user journey of the search tuning in Vertex AI Search](./images/key_user_journey_search_tuning.png)\n",
    "\n",
    "* Prepare your data for tuning\n",
    "    - The datasets should be prepared in JSONL format with identifier-text pairs.\n",
    "    - The mapping between query and answer texts should be described in tab-separated values (TSV) formats.\n",
    "* Update the datastore with the additional documents\n",
    "    - Before we update the datastore attached to the search app, the additional documents and data for tuning should be uploaded to the bucket in Cloud Storage.\n",
    "    - After uploading the new documents and data onto the bucket in Cloud Storage, the datastore is refreshed just by creating the datastore with the same configuration used in the previous creation. In the refresh process, we can see that only the files just added are used to generate new search indexes at the console interface.\n",
    "* Rebuild the search app with the updated datastore\n",
    "    - After the refresh of the datastore is completed, the search app must be rebuilt to be connected to the updated datastore.\n",
    "\n",
    "In order to obtain the correct results with the additional documents and data, users must rebuild the search app after they rebuilt the datastore.\n"
   ],
   "id": "5ec7fbc5a206adfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get started",
   "id": "abf41fc85c6d0fc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Install Vertex AI SDK and other required packages\n",
    "\n",
    "We will install some dependencies to run the cells in this notebook. \n"
   ],
   "id": "a847fb1ab5311789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform\n",
    "%pip install google-cloud-discoveryengine\n",
    "%pip install langchain_google_community\n",
    "%pip install langchain langchain-google-vertexai\n",
    "%pip install langchain-google-community[vertexaisearch]\n",
    "%pip install shortuuid"
   ],
   "id": "4aaab592f06ded75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it has restarted, continue to the next step."
   ],
   "id": "2da8d7afe7c4a6b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ],
   "id": "ca6e3138c3b5e373",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ],
   "id": "3dffeef69cc7ef0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ],
   "id": "cb5b07a72ccfce8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ],
   "id": "3b8eff89d532ddee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PROJECT_ID = \"genai-customersupport\"  # @param {type:\"string\"}\n",
    "LOCATION = \"global\"  # @param {type:\"string\"}\n",
    "STORAGE_LOCATION = \"us\"\n",
    "\n",
    "import vertexai\n",
    "import shortuuid\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ],
   "id": "af609df1879f8360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!gcloud auth login",
   "id": "b6e4fa49185abfff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ],
   "id": "eb15ebd0fd6e1ae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BUCKET_URI = f\"gs://sample-search-tuning-{PROJECT_ID}\"  # @param {type:\"string\"}\n",
    "TUNING_DATA_PATH_LOCAL = f\"./tuning_data\"\n",
    "TUNING_DATA_PATH_REMOTE = f\"{BUCKET_URI}/tuning_data\"\n",
    "SEARCH_DATASTORE_PATH_REMOTE = f\"{BUCKET_URI}/rlhf-datastore\"\n",
    "SEARCH_DATASTORE_ID = f\"search-datastore-{PROJECT_ID}-{shortuuid.uuid().lower()}\"\n",
    "SEARCH_DATASTORE_NAME = \"RLHF-ARTICLE-DATASTORE\""
   ],
   "id": "8e9d2a96700723f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.",
   "id": "be7a8ce876b5143c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! gcloud storage buckets create --location={STORAGE_LOCATION} --project={PROJECT_ID} --enable-hierarchical-namespace --uniform-bucket-level-access -b {BUCKET_URI}",
   "id": "28712547d2994229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare the data\n",
    "\n",
    "We will use the following datasets for this notebook. \n",
    "\n",
    "(1) FAQ data from the open source projects Kubernetes and Kubernetes Client. This data is a short list of questions and answers which can be useful to test the working of this notebook in a short period of time.\n",
    "\n",
    "(2) BEIR ([Benchmarking IR datasets](https://github.com/beir-cellar/beir)): BEIR is a heterogeneous benchmark containing diverse IR tasks. It also provides a common and easy framework for evaluation of your NLP-based retrieval models within the benchmark. This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset.\n",
    "    - For an overview, checkout our new wiki page: https://github.com/beir-cellar/beir/wiki.\n",
    "    - For models and datasets, checkout out Hugging Face (HF) page: https://huggingface.co/BeIR.\n",
    "      \n",
    "(3) SciFact ([SciFact](https://huggingface.co/datasets/allenai/scifact)): SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts, and annotated with labels and rationales.\n",
    "                                                                                     \n",
    "For BEIR and SciFact, the datasets are already prepared in JSONL and TSV formats. You can use them for testing the search tuning feature without any data preprocessing chore. However, the amount of the data of the BEIR and SciFact is large which make the tuning job run too long. Given that, we will try to generate a small amount of the data first to check if the search tuning feature is working correctly with the FAQ data from the Kubernetes project."
   ],
   "id": "4c70983b038ed88f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This ```generate_source_dataset``` function is a function to read the raw FAQ data from the FAQ and the README documents of the Kubernetes project and to generate the ```corpus_file.jsonl``` and ```query_file.jsonl``` for the tuning job.",
   "id": "68baa7f1370fa510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def generate_source_dataset(source_file, corpus_filepath, query_filepath, cleanup_at_start=True):\n",
    "    import json, os\n",
    "    import logging\n",
    "    \n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    if cleanup_at_start:\n",
    "        if (os.path.isfile(corpus_filepath)):\n",
    "            print(\"Removing previous file: %s\".format(corpus_filepath))\n",
    "            os.remove(corpus_filepath)\n",
    "        if (os.path.isfile(query_filepath)):\n",
    "            print(\"Removing previous file: %s\".format(query_filepath))\n",
    "            os.remove(query_filepath)\n",
    "    \n",
    "    logging.info(\"{}: {}\".format(generate_source_dataset.__name__, 1))    \n",
    "    with open(source_file, 'r') as f:\n",
    "        line_str = f.readline()\n",
    "        answer = \"\"\n",
    "        answer_flag = False\n",
    "        while line_str:\n",
    "            if re.match(r\"^(#{3})\\s+(.+)$\", line_str):\n",
    "                question = re.split(r\"^(#{3})\\s+(.+)$\", line_str)\n",
    "                question_str = \"\"\n",
    "                len_question = len(question) - 1\n",
    "                reidx = 0\n",
    "                while not (question[len_question - reidx] == \"###\"):\n",
    "                    question_str += question[len_question - reidx]\n",
    "                    reidx += 1\n",
    "                questions.append(question_str)\n",
    "                # print(\"Question: %s\" % question_str)\n",
    "                answer_flag = True\n",
    "                answers.append(str.strip(answer, \"\"))\n",
    "                # print(\"Answer: %s\" % answer)\n",
    "                answer = \"\"\n",
    "            elif answer_flag == True:\n",
    "                answer += line_str\n",
    "            line_str = f.readline()\n",
    "\n",
    "    logging.info(\"{}: {}\".format(generate_source_dataset.__name__, 2))\n",
    "    corpus_idx_start = 0\n",
    "    try:\n",
    "        with open(corpus_filepath, \"r\") as cf:\n",
    "            corpus_idx_start = len(list(enumerate(cf)))\n",
    "    except:\n",
    "        corpus_idx_start = 0\n",
    "\n",
    "    with open(corpus_filepath, \"a\") as cf:\n",
    "        jsonfile = \"\"\n",
    "        idx = corpus_idx_start\n",
    "        print(f'start idx:%d' % idx)\n",
    "        for answer in answers:\n",
    "            idx += 1\n",
    "            answer = answer.replace('\\\\[', '\\\\\\\\[')\n",
    "            answer = answer.replace('\\\\]', '\\\\\\\\]')\n",
    "            answer = answer.replace('\"', '\\\\\"')\n",
    "            json_line = \"{{\\\"_id\\\": \\\"ans{:04d}\\\", \\\"text\\\": \\\"{}\\\" }}\\n\".format(idx, str.strip(answer).replace(\"\\n\", \" \"))\n",
    "            jsonfile += json_line\n",
    "        cf.writelines(jsonfile)\n",
    "\n",
    "    logging.info(\"{}: {}\".format(generate_source_dataset.__name__, 3))\n",
    "    query_idx_start = 0\n",
    "    try:\n",
    "        with open(query_filepath, \"r\") as qf:\n",
    "            query_idx_start = len(list(enumerate(qf)))\n",
    "    except:\n",
    "        query_idx_start = 0\n",
    "\n",
    "    with open(query_filepath, \"a\") as qf:\n",
    "        jsonfile = \"\"\n",
    "        idx = query_idx_start\n",
    "        print(f'start idx:%d' % idx)\n",
    "        for question in questions:\n",
    "            idx += 1\n",
    "            question = question.replace('\\\\[', '\\\\\\\\[')\n",
    "            question = question.replace('\\\\]', '\\\\\\\\]')\n",
    "            question = question.replace('\"', '\\\\\"')\n",
    "            json_line = \"{{ \\\"_id\\\": \\\"que{:04d}\\\", \\\"text\\\": \\\"{}\\\" }}\\n\".format(idx, str.strip(question).replace(\"\\n\", \" \"))\n",
    "            jsonfile += json_line\n",
    "        qf.writelines(jsonfile)\n"
   ],
   "id": "97b4b5fa91353a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This ```generate_training_test_dataset``` generates the query-answer mapping in a tab-separated value format to help the tuning job to map the queries and the texts for the answers to the queries from the FAQ.",
   "id": "1946458814368e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_training_test_dataset(corpus_filepath, query_filepath,\n",
    "                                   training_filepath, test_filepath,\n",
    "                                   cleanup_at_start=True):\n",
    "    import json, os\n",
    "    import logging\n",
    "\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    if cleanup_at_start:\n",
    "        if (os.path.isfile(training_filepath)):\n",
    "            print(\"Removing previous file: %s\".format(training_filepath))\n",
    "            os.remove(training_filepath)\n",
    "        if (os.path.isfile(test_filepath)):\n",
    "            print(\"Removing previous file: %s\".format(test_filepath))\n",
    "            os.remove(test_filepath)\n",
    "\n",
    "    with open(corpus_filepath, \"r\") as corpus_file:\n",
    "        line_str = corpus_file.readline()\n",
    "        while line_str:\n",
    "            jsonl = json.loads(line_str, strict=False)\n",
    "            questions.append(jsonl['text'])\n",
    "            line_str = corpus_file.readline()\n",
    "\n",
    "    logging.info(\"{}: {}\".format(generate_training_test_dataset.__name__, 1))\n",
    "\n",
    "    with open(query_filepath, \"r\") as query_file:\n",
    "        line_str = query_file.readline()\n",
    "        while line_str:\n",
    "            jsonl = json.loads(line_str, strict=False)\n",
    "            answers.append(jsonl['text'])\n",
    "            line_str = query_file.readline()\n",
    "\n",
    "    logging.info(\"{}: {}\".format(generate_training_test_dataset.__name__, 2))\n",
    "    \n",
    "    with open(training_filepath, \"a\") as trf:\n",
    "        jsonfile = \"\"\n",
    "        json_line = 'query-id\\tcorpus-id\\tscore\\n'\n",
    "        idx = 1\n",
    "        jsonfile += json_line\n",
    "        len_questions = len(questions)\n",
    "        for question in questions:\n",
    "            json_line = \"que{:04d}\\tans{:04d}\\t1\\n\".format(idx, idx)\n",
    "            jsonfile += json_line\n",
    "            idx = idx + 1\n",
    "            if idx >  0.85 * len_questions:\n",
    "                break\n",
    "        trf.write(jsonfile)\n",
    "\n",
    "    logging.info(\"{}: {}\".format(generate_training_test_dataset.__name__, 3))\n",
    "    \n",
    "    with open(test_filepath, \"a\") as tef:\n",
    "        jsonfile = \"\"\n",
    "        json_line = 'query-id\\tcorpus-id\\tscore\\n'\n",
    "        idx = 1\n",
    "        len_questions = len(questions)\n",
    "        jsonfile += json_line\n",
    "        for question in questions:\n",
    "            if idx <= 0.85 * len_questions:\n",
    "                idx = idx + 1\n",
    "                pass\n",
    "            elif idx > 0.85 * len_questions and idx <= len_questions:\n",
    "                json_line = \"que{:04d}\\tans{:04d}\\t1\\n\".format(idx, idx)\n",
    "                jsonfile += json_line\n",
    "                idx = idx + 1\n",
    "        tef.write(jsonfile)\n",
    "\n",
    "    logging.info(\"{}: {}\".format(generate_training_test_dataset.__name__, 4))\n"
   ],
   "id": "6d4598f5670bb23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = ['./tuning_data/FAQ.md',\n",
    "                './tuning_data/FAQ-Kubernetes-Client.md',\n",
    "                './tuning_data/README.md']\n",
    "    if os.path.isfile('./tuning_data/corpus_file.jsonl'):\n",
    "        os.remove('./tuning_data/corpus_file.jsonl')\n",
    "    if os.path.isfile('./tuning_data/query_file.jsonl'):\n",
    "        os.remove('./tuning_data/query_file.jsonl')\n",
    "    \n",
    "    for file in datasets:\n",
    "        print(file)\n",
    "        generate_source_dataset( file,\n",
    "                                 './tuning_data/corpus_file.jsonl',\n",
    "                                 './tuning_data/query_file.jsonl',\n",
    "                                 cleanup_at_start=False)\n",
    "    generate_training_test_dataset( './tuning_data/corpus_file.jsonl',\n",
    "                                    './tuning_data/query_file.jsonl',\n",
    "                                    './tuning_data/training_data.tsv',\n",
    "                                    './tuning_data/test_data.tsv')\n"
   ],
   "id": "b435bb83f80d3dc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We create pdf files for the FAQ documents which are importable to the datastore of Vertex AI Search.",
   "id": "7de793f3deef39ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For Linux\n",
    "!pandoc --pdf-engine=pdflatex FAQ-Kubernetes-Client.md -o FAQ-Kubernetes-Client.pdf\n",
    "!pandoc --pdf-engine=pdflatex FAQ.md -o FAQ.pdf\n",
    "!pandoc --pdf-engine=pdflatex README.md -o README.pdf\n",
    "## For MacOS\n",
    "#!pandoc --pdf-engine=xelatex FAQ-Kubernetes-Client.md -o FAQ-Kubernetes-Client.pdf\n",
    "#!pandoc --pdf-engine=xelatex FAQ-Kubernetes-Client.md -o FAQ-Kubernetes-Client.pdf\n",
    "#!pandoc --pdf-engine=xelatex FAQ-Kubernetes-Client.md -o FAQ-Kubernetes-Client.pdf"
   ],
   "id": "9035fccc66e97cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After generating the test tuning datasets, we will upload the datasets to the bucket in Cloud Storage which will be used as a data store for the search tuning.",
   "id": "1f1e9d231bde69d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!gcloud storage buckets create $BUCKET_URI\n",
    "!echo \"Source data: {TUNING_DATA_PATH_LOCAL}\"\n",
    "!echo \"Destination path: {TUNING_DATA_PATH_REMOTE}\"\n",
    "!gcloud storage folders create \"{TUNING_DATA_PATH_REMOTE}\"\n",
    "!gcloud storage cp $TUNING_DATA_PATH_LOCAL/* $TUNING_DATA_PATH_REMOTE"
   ],
   "id": "a1f6cd52d581c502",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Uploading data for a search app datastore (papers on RLHF)\n",
    "\n",
    "To create a Vertex AI search app, we will upload some pdf files on Reinforcement Learning on Human Feedback from [Awesome RLHF](https://github.com/opendilab/awesome-RLHF.git) github repository to a bucket in Cloud Storage which will be used as a search datastore. The pdf files are available at [Awesome RLHF - PDF Files](https://gitlab.com/jincheolkim/awesome-rlhf)."
   ],
   "id": "67bbcaefe4914117"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://gitlab.com/jincheolkim/awesome-rlhf.git\n",
    "!cd awesome-rlhf\n",
    "!echo {SEARCH_DATASTORE_PATH_REMOTE}\n",
    "!gcloud storage folders create \"{SEARCH_DATASTORE_PATH_REMOTE}\"\n",
    "!gcloud storage cp --recursive \"awesome-rlhf/*\" \"{SEARCH_DATASTORE_PATH_REMOTE}\""
   ],
   "id": "84f31030267f3132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a data store for a search app with the cloud storage bucket with PDF documents\n",
    "\n",
    "This ```create_data_store``` function creates a datastore for an agent app with the identifier of an datastore with the ```data_store_id``` and the ```data_store_name```."
   ],
   "id": "1994ad67d25f8ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "def create_data_store(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        data_store_id: str,\n",
    "        data_store_name: str,\n",
    ") -> str:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.DataStoreServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the collection\n",
    "    # e.g. projects/{project}/locations/{location}/collections/default_collection\n",
    "    parent = client.collection_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        collection=\"default_collection\",\n",
    "    )\n",
    "\n",
    "    data_store = discoveryengine.DataStore(\n",
    "        display_name=data_store_name,\n",
    "        # Options: GENERIC, MEDIA, HEALTHCARE_FHIR\n",
    "        industry_vertical=discoveryengine.IndustryVertical.GENERIC,\n",
    "        # Options: SOLUTION_TYPE_RECOMMENDATION, SOLUTION_TYPE_SEARCH, SOLUTION_TYPE_CHAT, SOLUTION_TYPE_GENERATIVE_CHAT\n",
    "        solution_types=[discoveryengine.SolutionType.SOLUTION_TYPE_SEARCH],\n",
    "        # Options: NO_CONTENT, CONTENT_REQUIRED, PUBLIC_WEBSITE\n",
    "        content_config=discoveryengine.DataStore.ContentConfig.CONTENT_REQUIRED,\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.CreateDataStoreRequest(\n",
    "        parent=parent,\n",
    "        data_store_id=data_store_id,\n",
    "        data_store=data_store,\n",
    "        # Optional: For Advanced Site Search Only\n",
    "        # create_advanced_site_search=True,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.create_data_store(request=request)\n",
    "\n",
    "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # After the operation is complete,\n",
    "    # get information from operation metadata\n",
    "    metadata = discoveryengine.CreateDataStoreMetadata(operation.metadata)\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "    print(metadata)\n",
    "\n",
    "    return operation.operation.name\n"
   ],
   "id": "6b98aab02dae8368",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We create a datastore with the datastore bucket in Cloud Storage with the PDF files on RLHF.",
   "id": "87f60e116ce41c59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!echo \"Datastore ID: {SEARCH_DATASTORE_ID}\"\n",
    "!echo \"Datastore Name: {SEARCH_DATASTORE_NAME}\"\n",
    "create_datastore_op_name = create_data_store(PROJECT_ID, LOCATION, SEARCH_DATASTORE_ID, SEARCH_DATASTORE_NAME)"
   ],
   "id": "be84ae06c9683c72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the new datastore, we will import and make an index over the documents in the datastore. The ```ImportDocumentsRequests``` generates a REST API request message in the JSON format and the ```import_documents``` method of the DocumentServiceClient class lets you import the documents and make an index over the document set with the information in the ```ImportDocumentRequest``` request.",
   "id": "b860f9d685860ad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "client_options = (\n",
    "    ClientOptions(api_endpoint=f\"{LOCATION}-discoveryengine.googleapis.com\")\n",
    "    if LOCATION != \"global\"\n",
    "    else None\n",
    ")"
   ],
   "id": "8708af034de51c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a client\n",
    "client = discoveryengine.DocumentServiceClient(client_options=client_options)"
   ],
   "id": "39f505f6d99c5163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The full resource name of the search engine branch.\n",
    "# e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\n",
    "parent = client.branch_path(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    data_store=SEARCH_DATASTORE_ID,\n",
    "    branch=\"default_branch\",\n",
    ")"
   ],
   "id": "6797fc7dd0a96223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "request = discoveryengine.ImportDocumentsRequest(\n",
    "    parent=parent,\n",
    "    gcs_source=discoveryengine.GcsSource(\n",
    "        # Multiple URIs are supported\n",
    "        input_uris=[f\"{SEARCH_DATASTORE_PATH_REMOTE}/*\"],\n",
    "        # Options:\n",
    "        # - `content` - Unstructured documents (PDF, HTML, DOC, TXT, PPTX)\n",
    "        # - `custom` - Unstructured documents with custom JSONL metadata\n",
    "        # - `document` - Structured documents in the discoveryengine.Document format.\n",
    "        # - `csv` - Unstructured documents with CSV metadata\n",
    "        data_schema=\"content\",\n",
    "    ),\n",
    "    # Options: `FULL`, `INCREMENTAL`\n",
    "    reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\n",
    ")"
   ],
   "id": "f5cb2466e54e38e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make the request\n",
    "operation = client.import_documents(request=request)\n",
    "\n",
    "print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "response = operation.result()"
   ],
   "id": "1e3d3f9933927a0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The document import operation in the above can be monitored after it was submitted with the metadata (especially the operation ID) in the response.",
   "id": "715e8e54e5492eda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# After the operation is complete,\n",
    "# get information from operation metadata\n",
    "metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)"
   ],
   "id": "74049154bae75839",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Handle the response\n",
    "print(response)\n",
    "print(metadata)"
   ],
   "id": "d6708616e71c326",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a search app using the Vertex AI Search SDK\n",
    "\n",
    "As we just created a datastore and made an index over the documents in it in the above, we will create a search app with the datastore. \n"
   ],
   "id": "c9926ab3690deac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SEARCH_DATASTORE_REF_ID = f\"projects/{PROJECT_ID}/locations/{LOCATION}/collections/default_collection/dataStores/{SEARCH_DATASTORE_ID}\"\n",
    "SEARCH_APP_ID = f\"search-app-{PROJECT_ID}-{shortuuid.uuid().lower()}\"\n",
    "SEARCH_APP_NAME=\"RLHF_SEARCH_APP\""
   ],
   "id": "bd5941f83b792b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine_v1 as discoveryengine\n",
    "\n",
    "def create_search_engine(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        engine_name: str,\n",
    "        engine_id: str,\n",
    "        data_store_ids: List[str]\n",
    ") -> str:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.EngineServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the collection\n",
    "    # e.g. projects/{project}/locations/{location}/collections/default_collection\n",
    "    parent = client.collection_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        collection=\"default_collection\",\n",
    "    )\n",
    "\n",
    "    engine = discoveryengine.Engine(\n",
    "        display_name=engine_name,\n",
    "        # Options: GENERIC, MEDIA, HEALTHCARE_FHIR\n",
    "        industry_vertical=discoveryengine.IndustryVertical.GENERIC,\n",
    "        # Options: SOLUTION_TYPE_RECOMMENDATION, SOLUTION_TYPE_SEARCH, SOLUTION_TYPE_CHAT, SOLUTION_TYPE_GENERATIVE_CHAT\n",
    "        solution_type=discoveryengine.SolutionType.SOLUTION_TYPE_SEARCH,\n",
    "        # For search apps only\n",
    "        search_engine_config=discoveryengine.Engine.SearchEngineConfig(\n",
    "            # Options: SEARCH_TIER_STANDARD, SEARCH_TIER_ENTERPRISE\n",
    "            search_tier=discoveryengine.SearchTier.SEARCH_TIER_ENTERPRISE,\n",
    "            # Options: SEARCH_ADD_ON_LLM, SEARCH_ADD_ON_UNSPECIFIED\n",
    "            search_add_ons=[discoveryengine.SearchAddOn.SEARCH_ADD_ON_LLM],\n",
    "        ),\n",
    "        # For generic recommendation apps only\n",
    "        # similar_documents_config=discoveryengine.Engine.SimilarDocumentsEngineConfig,\n",
    "        data_store_ids=data_store_ids,\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.CreateEngineRequest(\n",
    "        parent=parent,\n",
    "        engine=engine,\n",
    "        engine_id=engine_id,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.create_engine(request=request)\n",
    "\n",
    "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # After the operation is complete,\n",
    "    # get information from operation metadata\n",
    "    metadata = discoveryengine.CreateEngineMetadata(operation.metadata)\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "    print(metadata)\n",
    "\n",
    "    return operation.operation.name\n"
   ],
   "id": "83b3337e02d93c25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_search_app_op_name = create_search_engine(PROJECT_ID, LOCATION, SEARCH_APP_NAME, SEARCH_APP_ID, [SEARCH_DATASTORE_ID])",
   "id": "1727f61485962416",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test the search app with a test prompt\n",
    "\n",
    "We will test the search app we just created with information about a paper regarding a world model for autonomous driving which is described in a paper among the documents in the datastore."
   ],
   "id": "975317de4d9396a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "QUERY_PROMPT = \"\"\"\n",
    "    What is the name of the world model for autonomous driving developed recently?\n",
    "\"\"\""
   ],
   "id": "862fd372f504eb70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine_v1 as discoveryengine\n",
    "\n",
    "def search(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        engine_id: str,\n",
    "        search_query: str,\n",
    ") -> List[discoveryengine.SearchResponse]:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the search app serving config\n",
    "    serving_config = f\"projects/{project_id}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config\"\n",
    "\n",
    "    # Optional - only supported for unstructured data: Configuration options for search.\n",
    "    # Refer to the `ContentSearchSpec` reference for all supported fields:\n",
    "    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest.ContentSearchSpec\n",
    "    content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(\n",
    "        # For information about snippets, refer to:\n",
    "        # https://cloud.google.com/generative-ai-app-builder/docs/snippets\n",
    "        snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(\n",
    "            return_snippet=True\n",
    "        ),\n",
    "        # For information about search summaries, refer to:\n",
    "        # https://cloud.google.com/generative-ai-app-builder/docs/get-search-summaries\n",
    "        summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n",
    "            summary_result_count=5,\n",
    "            include_citations=True,\n",
    "            ignore_adversarial_query=True,\n",
    "            ignore_non_summary_seeking_query=True,\n",
    "            model_prompt_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec.ModelPromptSpec(\n",
    "                preamble=\"YOUR_CUSTOM_PROMPT\"\n",
    "            ),\n",
    "            model_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec.ModelSpec(\n",
    "                version=\"stable\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Refer to the `SearchRequest` reference for all supported fields:\n",
    "    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config,\n",
    "        query=search_query,\n",
    "        page_size=10,\n",
    "        content_search_spec=content_search_spec,\n",
    "        query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(\n",
    "            condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,\n",
    "        ),\n",
    "        spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(\n",
    "            mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    response = client.search(request)\n",
    "    print(response)\n",
    "\n",
    "    return response\n"
   ],
   "id": "e93eed5b6ca28fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that the search app returns a list of relevant documents with references to the related documents in the datastore. Please keep the \n",
    "result in your mind to compare it with the results after the search tuning is performed."
   ],
   "id": "cc0f097bd8ab58b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "search_response = search(PROJECT_ID, LOCATION, SEARCH_APP_ID, QUERY_PROMPT)",
   "id": "6643e2c14ebd271f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuring and submitting a search tuning job\n",
    "\n",
    "With the search app ready, we will perform a search tuning with a test tuning data on Kubernetes.\n",
    "\n",
    "First, we will upload the documents of FAQs about Kubernetes and Kubernetes Client API. The original documents were in the Markdown format but we transform them to PDF format files as the Vertex AI Search cannot accept Markdown files but only HTML, PDF and PDF with embedded text, TXT, JSON, XHTML, and XML format. PPTX, DOCX and XLSX formats are available in Preview. The PDF files are uploaded to the buckets for the datastore of the search app."
   ],
   "id": "6519ac0dfe57ac24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Uploading the additional PDF files for tuning to the bucket of the datastore",
   "id": "c56b455078446740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!gcloud storage cp {TUNING_DATA_PATH_LOCAL}/*.jsonl \"{TUNING_DATA_PATH_REMOTE}\"\n",
    "!gcloud storage cp {TUNING_DATA_PATH_LOCAL}/*.tsv \"{TUNING_DATA_PATH_REMOTE}\"\n",
    "!gcloud storage ls \"{TUNING_DATA_PATH_REMOTE}\""
   ],
   "id": "dd6a16623bf75d98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Uploading the datasets for the search tuning and perform the tuning\n",
    "\n",
    "These are the information on the tuning dataset files to be used to tune the backend LLM behind the search app. Please refer to the [Prepare data for ingesting](https://cloud.google.com/generative-ai-app-builder/docs/prepare-data#website) in the Google Cloud Documentation."
   ],
   "id": "4443de868003f507"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO(developer): Uncomment these variables before running the sample.\n",
    "data_store_id = f\"{SEARCH_DATASTORE_ID}\"\n",
    "corpus_data_path = f\"{TUNING_DATA_PATH_REMOTE}/corpus_file.jsonl\"\n",
    "query_data_path = f\"{TUNING_DATA_PATH_REMOTE}/query_file.jsonl\"\n",
    "train_data_path = f\"{TUNING_DATA_PATH_REMOTE}/training_data.tsv\"\n",
    "test_data_path = f\"{TUNING_DATA_PATH_REMOTE}/test_data.tsv\""
   ],
   "id": "9bc58b6948c60387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.operation import Operation\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "def train_custom_model(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        data_store_id: str,\n",
    "        corpus_data_path: str,\n",
    "        query_data_path: str,\n",
    "        train_data_path: str,\n",
    "        test_data_path: str,\n",
    ") -> Operation:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "    # Create a client\n",
    "    client = discoveryengine.SearchTuningServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the data store\n",
    "    data_store = f\"projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{data_store_id}\"\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.train_custom_model(\n",
    "        request=discoveryengine.TrainCustomModelRequest(\n",
    "            gcs_training_input=discoveryengine.TrainCustomModelRequest.GcsTrainingInput(\n",
    "                corpus_data_path=corpus_data_path,\n",
    "                query_data_path=query_data_path,\n",
    "                train_data_path=train_data_path,\n",
    "                test_data_path=test_data_path,\n",
    "            ),\n",
    "            data_store=data_store,\n",
    "            model_type=\"search-tuning\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional: Wait for training to complete\n",
    "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # After the operation is complete,\n",
    "    # get information from operation metadata\n",
    "    metadata = discoveryengine.TrainCustomModelMetadata(operation.metadata)\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "    print(metadata)\n",
    "    print(operation)\n",
    "\n",
    "    return operation\n"
   ],
   "id": "24ee81fc9b8612b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This ```train_custom_model``` function is to submit a search tuning job with the datasets we just prepared.",
   "id": "6349ad8e5053b0d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tuning_op = train_custom_model(PROJECT_ID, LOCATION, data_store_id, corpus_data_path, query_data_path, train_data_path, test_data_path)",
   "id": "1e507b47cd2fec01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that three additional documents related to the tuning task was uploaded to the datastore bucket, ```FAQ-Kubernetes-Client.pdf, FAQ.pdf, README.pdf.``` With these new documents, we should perform the indexing again by calling the ```import_documents``` method of the clinet again.",
   "id": "aa93841ca57b7341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!gcloud storage cp \"{TUNING_DATA_PATH_LOCAL}/*.pdf\" \"{SEARCH_DATASTORE_PATH_REMOTE}\"\n",
    "!gcloud storage ls \"{SEARCH_DATASTORE_PATH_REMOTE}\""
   ],
   "id": "8ee3d2ba8083b67e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "request = discoveryengine.ImportDocumentsRequest(\n",
    "    parent=parent,\n",
    "    gcs_source=discoveryengine.GcsSource(\n",
    "        # Multiple URIs are supported\n",
    "        input_uris=[f\"{SEARCH_DATASTORE_PATH_REMOTE}/*\"],\n",
    "        # Options:\n",
    "        # - `content` - Unstructured documents (PDF, HTML, DOC, TXT, PPTX)\n",
    "        # - `custom` - Unstructured documents with custom JSONL metadata\n",
    "        # - `document` - Structured documents in the discoveryengine.Document format.\n",
    "        # - `csv` - Unstructured documents with CSV metadata\n",
    "        data_schema=\"content\",\n",
    "    ),\n",
    "    # Options: `FULL`, `INCREMENTAL`\n",
    "    reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\n",
    ")"
   ],
   "id": "183670cd81fd26a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make the request\n",
    "operation = client.import_documents(request=request)\n",
    "\n",
    "print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "response = operation.result()"
   ],
   "id": "20c363fc4a1f72f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# After the operation is complete,\n",
    "# get information from operation metadata\n",
    "metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)"
   ],
   "id": "58ecb7487d5a8220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Handle the response\n",
    "print(response)\n",
    "print(metadata)"
   ],
   "id": "750c414530b9b024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Testing the tuned search app endpoint with a question on Kubernetes\n",
    "\n",
    "The tuning job will take about 30 to 60 minutes. After the tuning job completed, we test the search app with a query prompt regarding Kubernetes which is the information in the documents indexed additionally with the tuning."
   ],
   "id": "c9f9719a3f25a544"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "QUERY_PROMPT = \"\"\"\n",
    "    How do I determine the status of a deployment of Kubernetes?\n",
    "\"\"\""
   ],
   "id": "50d44f32302fe61a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that the information on the deployment of Kubernetes which was described in the FAQ documents are correctly returned with the new documents indexed in the tuning.",
   "id": "ac4ac605b472d563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "search_response = search(PROJECT_ID, LOCATION, SEARCH_APP_ID, QUERY_PROMPT)",
   "id": "2f3c21b333eec9be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean up\n",
    "\n",
    "We should clean up the deployed resources and data not to create unnecessary costs."
   ],
   "id": "b60079454674a4ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Deleting the search app",
   "id": "a504806d5df4d387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine_v1 as discoveryengine\n",
    "\n",
    "def delete_engine(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        engine_id: str,\n",
    ") -> str:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.EngineServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the engine\n",
    "    # e.g. projects/{project}/locations/{location}/collections/default_collection/engines/{engine_id}\n",
    "    name = client.engine_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        collection=\"default_collection\",\n",
    "        engine=engine_id,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.delete_engine(name=name)\n",
    "\n",
    "    print(f\"Operation: {operation.operation.name}\")\n",
    "\n",
    "    return operation.operation.name"
   ],
   "id": "e12116977c1a4c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "delete_search_app_op_name = delete_engine(PROJECT_ID, LOCATION, SEARCH_APP_ID)",
   "id": "26e09ecd09e8e07b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Deleting the documents in the datastore",
   "id": "6d461efaeeba6a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "def purge_documents(\n",
    "        project_id: str, location: str, data_store_id: str\n",
    ") -> discoveryengine.PurgeDocumentsMetadata:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.DocumentServiceClient(client_options=client_options)\n",
    "\n",
    "    operation = client.purge_documents(\n",
    "        request=discoveryengine.PurgeDocumentsRequest(\n",
    "            # The full resource name of the search engine branch.\n",
    "            # e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\n",
    "            parent=client.branch_path(\n",
    "                project=project_id,\n",
    "                location=location,\n",
    "                data_store=data_store_id,\n",
    "                branch=\"default_branch\",\n",
    "            ),\n",
    "            filter=\"*\",\n",
    "            # If force is set to `False`, return the expected purge count without deleting any documents.\n",
    "            force=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # After the operation is complete,\n",
    "    # get information from operation metadata\n",
    "    metadata = discoveryengine.PurgeDocumentsMetadata(operation.metadata)\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "    print(metadata)\n",
    "\n",
    "    return metadata\n"
   ],
   "id": "af8bed8df788b8d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "purge_document_metadata = purge_documents(PROJECT_ID, LOCATION, SEARCH_DATASTORE_ID)",
   "id": "98f39ac2f99c8951",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Deleting the datastore",
   "id": "fac9f59f9c0aef34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "def delete_data_store(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        data_store_id: str,\n",
    ") -> str:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.DataStoreServiceClient(client_options=client_options)\n",
    "\n",
    "    request = discoveryengine.DeleteDataStoreRequest(\n",
    "        # The full resource name of the data store\n",
    "        name=client.data_store_path(project_id, location, data_store_id)\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.delete_data_store(request=request)\n",
    "\n",
    "    print(f\"Operation: {operation.operation.name}\")\n",
    "\n",
    "    return operation.operation.name\n"
   ],
   "id": "e52a77f5af512752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "delete_datastore_op_name = delete_data_store(PROJECT_ID, LOCATION, SEARCH_DATASTORE_ID)",
   "id": "33cfacc74d15a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!gcloud storage rm -r \"{BUCKET_URI}\"",
   "id": "8284c6a5bc2ecbc8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
