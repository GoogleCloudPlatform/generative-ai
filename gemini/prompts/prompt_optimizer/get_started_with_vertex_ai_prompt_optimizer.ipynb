{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Get Started with Vertex AI Prompt Optimizer\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fget_started_with_vertex_ai_prompt_optimizer.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/get_started_with_vertex_ai_prompt_optimizer.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Ivan Nardini](https://github.com/inardini) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Get Started with Vertex AI Prompt Optimizer\n",
    "\n",
    "When developing with large language models, crafting the perfect prompt—a process known as prompt engineering—is both an art and a science. It can be time-consuming and challenging to write prompts that consistently produce the desired results. Furthermore, as new and improved models are released, prompts that worked well before may need to be updated.\n",
    "\n",
    "To address these challenges, Vertex AI offers the **Prompt Optimizer**, a prompt optimization tool to help you refine and enhance your prompts automatically. This notebook serves as a comprehensive guide to both of its  approaches: the **Zero-Shot Optimizer** and the **Data-Driven Optimizer**.\n",
    "\n",
    "### The two approaches to prompt optimization\n",
    "\n",
    "#### 1\\. Zero-Shot Optimizer\n",
    "\n",
    "This is your go-to tool for rapid prompt refinement and generation *without* needing an evaluation dataset.\n",
    "\n",
    "  * **Generate from Scratch**: Simply describe a task in plain language, and it will generate a complete, well-structured system instruction for you.\n",
    "  * **Refine Existing Prompts**: Provide an existing prompt, and it will rewrite it based on established best practices for clarity, structure, and effectiveness.\n",
    "\n",
    "#### 2\\. Data-Driven Optimizer\n",
    "\n",
    "This tool performs a deep, performance-based optimization that uses your data to measure success.\n",
    "\n",
    "  * **Tune for Performance**: You provide a dataset of sample inputs and expected outputs, and it systematically tests and rewrites your system instructions to find the version that scores highest on the evaluation metrics you define.\n",
    "  * **Task-Specific**: It's the ideal choice when you want to fine-tune a prompt for a specific task and have data to prove what \"better\" looks like.\n",
    "\n",
    "In this tutorial, we will walk through both methods. First, we'll explore the **Zero-Shot Optimizer** for quick, data-free improvements. Then, we'll dive deep into the **Data-Driven Optimizer**, learning how to leverage a dataset to achieve the best possible performance for a specific task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started\n",
    "\n",
    "Before we can start optimizing, we need to set up our Python environment and configure our Google Cloud project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install required packages\n",
    "\n",
    "This command installs the necessary Python libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install \"google-cloud-aiplatform>=1.108.0\" \"pydantic\" \"etils\" \"protobuf==4.25.3\" \"gradio\" --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook in Google Colab, this cell handles authentication, allowing the notebook to securely access your Google Cloud resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information\n",
    "\n",
    "Here, we define essential variables for our Google Cloud project. The Prompt Optimizer job will run within a Google Cloud project. You need to [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com) and use the specified Cloud Storage bucket to read input data and write results.\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "PROJECT_NUMBER = !gcloud projects describe {PROJECT_ID} --format=\"get(projectNumber)\"[0]\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")\n",
    "\n",
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type: \"string\", placeholder: \"[your-bucket-name]\", isTemplate: true}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\" \n",
    "\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "client = vertexai.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaksyUomxawt"
   },
   "source": [
    "### Service account and permissions\n",
    "\n",
    "The Prompt Optimizer runs as a backend job that needs permission to perform actions on your behalf. We grant the necessary IAM roles to the default Compute Engine service account, which the job uses to operate.\n",
    "\n",
    "  * `Vertex AI User`: Allows the job to call Vertex AI models.\n",
    "  * `Storage Object Admin`: Allows the job to read your dataset from and write results to your GCS bucket.\n",
    "  * `Artifact Registry Reader`: Allows the job to download necessary components.\n",
    "\n",
    "[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to learn how to grant those permissions to a single service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7MNJEFP7-S9"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "for role in ['aiplatform.user', 'storage.objectAdmin', 'artifactregistry.reader']:\n",
    "\n",
    "    ! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
    "      --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
    "      --role=roles/{role} --condition=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from etils import epath\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import gradio as gr\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EfCy5RI19vt"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yKz4QlHvo2u"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def get_best_vapo_results(\n",
    "    base_path: str,\n",
    "    metric_name: Optional[str] = None\n",
    ") -> Tuple[str, List[str]]:\n",
    "    \"\"\"Get the best system instruction and demonstrations across all VAPO runs.\"\"\"\n",
    "    # Find all valid runs\n",
    "    required_files = [\"eval_results.json\", \"templates.json\"]\n",
    "    runs = find_directories_with_files(base_path, required_files)\n",
    "\n",
    "    if not runs:\n",
    "        raise ValueError(f\"No valid runs found in {base_path}\")\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    best_instruction = \"\"\n",
    "    best_demonstrations = []\n",
    "\n",
    "    for run_path in runs:\n",
    "        try:\n",
    "            # Check main templates.json first\n",
    "            templates_path = f\"{run_path}/templates.json\"\n",
    "            with epath.Path(templates_path).open(\"r\") as f:\n",
    "                templates_data = json.load(f)\n",
    "\n",
    "            if templates_data:\n",
    "                df = pd.json_normalize(templates_data)\n",
    "\n",
    "                # Find metric column\n",
    "                metric_columns = [col for col in df.columns if \"metric\" in col and \"mean\" in col]\n",
    "                if metric_columns:\n",
    "                    # Select appropriate metric\n",
    "                    if metric_name:\n",
    "                        metric_col = next((col for col in metric_columns if metric_name in col), None)\n",
    "                    else:\n",
    "                        composite_cols = [col for col in metric_columns if \"composite_metric\" in col]\n",
    "                        metric_col = composite_cols[0] if composite_cols else metric_columns[0]\n",
    "\n",
    "                    if metric_col:\n",
    "                        best_idx = df[metric_col].argmax()\n",
    "                        score = float(df.iloc[best_idx][metric_col])\n",
    "\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_row = df.iloc[best_idx]\n",
    "\n",
    "                            # Extract instruction if present\n",
    "                            if 'prompt' in best_row or 'instruction' in best_row:\n",
    "                                instruction = best_row.get('prompt', best_row.get('instruction', ''))\n",
    "                                if instruction:\n",
    "                                    instruction = instruction.replace(\"store('answer', llm())\", \"{{llm()}}\")\n",
    "                                    best_instruction = instruction\n",
    "\n",
    "                            # Extract demonstrations if present\n",
    "                            if 'demonstrations' in best_row or 'demo_set' in best_row:\n",
    "                                demos = best_row.get('demonstrations', best_row.get('demo_set', []))\n",
    "                                best_demonstrations = format_demonstrations(demos)\n",
    "\n",
    "            # Check instruction-specific optimization\n",
    "            instruction_path = f\"{run_path}/instruction/templates.json\"\n",
    "            try:\n",
    "                with epath.Path(instruction_path).open(\"r\") as f:\n",
    "                    instruction_data = json.load(f)\n",
    "\n",
    "                if instruction_data:\n",
    "                    inst_df = pd.json_normalize(instruction_data)\n",
    "                    metric_columns = [col for col in inst_df.columns if \"metric\" in col and \"mean\" in col]\n",
    "\n",
    "                    if metric_columns:\n",
    "                        if metric_name:\n",
    "                            metric_col = next((col for col in metric_columns if metric_name in col), None)\n",
    "                        else:\n",
    "                            composite_cols = [col for col in metric_columns if \"composite_metric\" in col]\n",
    "                            metric_col = composite_cols[0] if composite_cols else metric_columns[0]\n",
    "\n",
    "                        if metric_col and metric_col in inst_df.columns:\n",
    "                            inst_best_idx = inst_df[metric_col].argmax()\n",
    "                            inst_score = float(inst_df.iloc[inst_best_idx][metric_col])\n",
    "\n",
    "                            if inst_score > best_score:\n",
    "                                best_score = inst_score\n",
    "                                best_row = inst_df.iloc[inst_best_idx]\n",
    "\n",
    "                                instruction = best_row.get('prompt', best_row.get('instruction', ''))\n",
    "                                if instruction:\n",
    "                                    instruction = instruction.replace(\"store('answer', llm())\", \"{{llm()}}\")\n",
    "                                    best_instruction = instruction\n",
    "                                # In instruction-only mode, there might not be demonstrations\n",
    "                                if 'demonstrations' not in best_row and 'demo_set' not in best_row:\n",
    "                                    best_demonstrations = []\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "            # Check demonstration-specific optimization\n",
    "            demo_path = f\"{run_path}/demonstration/templates.json\"\n",
    "            try:\n",
    "                with epath.Path(demo_path).open(\"r\") as f:\n",
    "                    demo_data = json.load(f)\n",
    "\n",
    "                if demo_data:\n",
    "                    demo_df = pd.json_normalize(demo_data)\n",
    "                    metric_columns = [col for col in demo_df.columns if \"metric\" in col and \"mean\" in col]\n",
    "\n",
    "                    if metric_columns:\n",
    "                        if metric_name:\n",
    "                            metric_col = next((col for col in metric_columns if metric_name in col), None)\n",
    "                        else:\n",
    "                            composite_cols = [col for col in metric_columns if \"composite_metric\" in col]\n",
    "                            metric_col = composite_cols[0] if composite_cols else metric_columns[0]\n",
    "\n",
    "                        if metric_col and metric_col in demo_df.columns:\n",
    "                            demo_best_idx = demo_df[metric_col].argmax()\n",
    "                            demo_score = float(demo_df.iloc[demo_best_idx][metric_col])\n",
    "\n",
    "                            if demo_score > best_score:\n",
    "                                best_score = demo_score\n",
    "                                best_row = demo_df.iloc[demo_best_idx]\n",
    "\n",
    "                                demos = best_row.get('demonstrations', best_row.get('demo_set', []))\n",
    "                                best_demonstrations = format_demonstrations(demos)\n",
    "                                # In demo-only mode, there might not be an instruction\n",
    "                                if 'prompt' not in best_row and 'instruction' not in best_row:\n",
    "                                    best_instruction = \"\"\n",
    "                                else:\n",
    "                                    instruction = best_row.get('prompt', best_row.get('instruction', ''))\n",
    "                                    if instruction:\n",
    "                                        instruction = instruction.replace(\"store('answer', llm())\", \"{{llm()}}\")\n",
    "                                        best_instruction = instruction\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing run {run_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if best_score == float('-inf'):\n",
    "        raise ValueError(\"Could not find any valid results\")\n",
    "\n",
    "    return best_instruction, best_demonstrations\n",
    "\n",
    "def format_demonstrations(demos: any) -> List[str]:\n",
    "    \"\"\"Format demonstrations into list of strings.\"\"\"\n",
    "    if isinstance(demos, str):\n",
    "        try:\n",
    "            demos = json.loads(demos)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    if not isinstance(demos, list):\n",
    "        return []\n",
    "\n",
    "    formatted_demos = []\n",
    "    for demo in demos:\n",
    "        if isinstance(demo, dict):\n",
    "            # Format dict as \"key: value\" pairs\n",
    "            demo_str = \"\\n\".join([f\"{k}: {v}\" for k, v in demo.items()])\n",
    "            formatted_demos.append(demo_str)\n",
    "        else:\n",
    "            formatted_demos.append(str(demo))\n",
    "\n",
    "    return formatted_demos\n",
    "\n",
    "def split_gcs_path(gcs_path: str) -> tuple[str, str]:\n",
    "    \"\"\"Split GCS path into bucket name and prefix.\"\"\"\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        raise ValueError(f\"Invalid GCS path. Must start with gs://\")\n",
    "\n",
    "    path = gcs_path[len(\"gs://\"):]\n",
    "    parts = path.split(\"/\", 1)\n",
    "    return parts[0], parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "\n",
    "def list_gcs_objects(gcs_path: str) -> List[str]:\n",
    "    \"\"\"List all objects under given GCS path.\"\"\"\n",
    "    bucket_name, prefix = split_gcs_path(gcs_path)\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    return [blob.name for blob in blobs]\n",
    "\n",
    "\n",
    "def find_directories_with_files(base_path: str, required_files: List[str]) -> List[str]:\n",
    "    \"\"\"Find directories containing all required files.\"\"\"\n",
    "    bucket_name, prefix = split_gcs_path(base_path)\n",
    "    all_paths = list_gcs_objects(base_path)\n",
    "\n",
    "    # Group files by directory\n",
    "    directories = {}\n",
    "    for path in all_paths:\n",
    "        dir_path = \"/\".join(path.split(\"/\")[:-1])\n",
    "        filename = path.split(\"/\")[-1]\n",
    "\n",
    "        if dir_path not in directories:\n",
    "            directories[dir_path] = set()\n",
    "        directories[dir_path].add(filename)\n",
    "\n",
    "    # Find directories with all required files\n",
    "    matching_dirs = []\n",
    "    for dir_path, files in directories.items():\n",
    "        if all(req_file in files for req_file in required_files):\n",
    "            matching_dirs.append(f\"gs://{bucket_name}/{dir_path}\")\n",
    "\n",
    "    return matching_dirs\n",
    "\n",
    "\n",
    "class VAPOResultsViewer:\n",
    "    \"\"\"Gradio-based viewer for VAPO optimization results.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_path = None\n",
    "        self.runs = []\n",
    "        self.templates = []\n",
    "        self.eval_results = []\n",
    "        self.current_run = None\n",
    "\n",
    "    def clear_all(self) -> Tuple[str, gr.Dropdown, gr.Dropdown, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Clear all data and reset the interface.\"\"\"\n",
    "        self.base_path = None\n",
    "        self.runs = []\n",
    "        self.templates = []\n",
    "        self.eval_results = []\n",
    "        self.current_run = None\n",
    "\n",
    "        return (\n",
    "            \"\",  # Clear base path input\n",
    "            gr.Dropdown(choices=[], value=None),  # Clear run dropdown\n",
    "            gr.Dropdown(choices=[], value=None),  # Clear template dropdown\n",
    "            pd.DataFrame(),  # Clear template display\n",
    "            pd.DataFrame()   # Clear eval display\n",
    "        )\n",
    "\n",
    "    def load_runs(self, base_path: str) -> gr.Dropdown:\n",
    "        \"\"\"Load available runs from the base path.\"\"\"\n",
    "        if not base_path:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "        try:\n",
    "            self.base_path = base_path\n",
    "            required_files = [\"eval_results.json\", \"templates.json\"]\n",
    "            self.runs = find_directories_with_files(base_path, required_files)\n",
    "\n",
    "            if not self.runs:\n",
    "                return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "            return gr.Dropdown(choices=self.runs, value=self.runs[0])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading runs: {e}\")\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "    def load_run_data(self, run_path: str) -> Tuple[gr.Dropdown, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load data for a specific run.\"\"\"\n",
    "        if not run_path:\n",
    "            return gr.Dropdown(choices=[], value=None), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            self.current_run = run_path\n",
    "\n",
    "            # Load templates\n",
    "            templates_path = f\"{run_path}/templates.json\"\n",
    "            with epath.Path(templates_path).open(\"r\") as f:\n",
    "                templates_data = json.load(f)\n",
    "\n",
    "            # Load evaluation results\n",
    "            eval_path = f\"{run_path}/eval_results.json\"\n",
    "            with epath.Path(eval_path).open(\"r\") as f:\n",
    "                eval_data = json.load(f)\n",
    "\n",
    "            # Process data\n",
    "            self.templates = [pd.json_normalize(t) for t in templates_data]\n",
    "            self.eval_results = [self._process_eval_result(r) for r in eval_data]\n",
    "\n",
    "            # Handle potential mismatch\n",
    "            if len(self.templates) == len(self.eval_results) + 1:\n",
    "                self.templates = self.templates[1:]\n",
    "            elif len(self.templates) != len(self.eval_results):\n",
    "                raise ValueError(\n",
    "                    f\"Mismatch: {len(self.templates)} templates vs \"\n",
    "                    f\"{len(self.eval_results)} results\"\n",
    "                )\n",
    "\n",
    "            # Create template options\n",
    "            template_options = self._create_template_options()\n",
    "\n",
    "            # Load first template by default\n",
    "            if template_options:\n",
    "                template_df, eval_df = self._get_template_data(0)\n",
    "                return (\n",
    "                    gr.Dropdown(choices=template_options, value=template_options[0]),\n",
    "                    template_df,\n",
    "                    eval_df\n",
    "                )\n",
    "\n",
    "            return gr.Dropdown(choices=[], value=None), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading run data: {e}\")\n",
    "            return gr.Dropdown(choices=[], value=None), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def _process_eval_result(self, result: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"Process evaluation result for display.\"\"\"\n",
    "        df = pd.read_json(io.StringIO(result[\"metrics_table\"]))\n",
    "\n",
    "        # Remove potentially confusing columns\n",
    "        columns_to_drop = [\n",
    "            col for col in df.columns\n",
    "            if any(term in col for term in [\"confidence\", \"raw_eval_resp\", \"instruction\", \"context\"])\n",
    "        ]\n",
    "\n",
    "        return df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    def _create_template_options(self) -> List[str]:\n",
    "        \"\"\"Create dropdown options for templates.\"\"\"\n",
    "        options = []\n",
    "\n",
    "        for i, template_df in enumerate(self.templates):\n",
    "            # Extract metrics for display\n",
    "            metrics = []\n",
    "            for col in template_df.columns:\n",
    "                if \"metric\" in col and \"mean\" in col:\n",
    "                    value = template_df[col].iloc[0]\n",
    "                    metric_name = self._extract_metric_name(col)\n",
    "                    metrics.append(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "            metrics_str = \" | \".join(metrics) if metrics else \"No metrics\"\n",
    "            options.append(f\"Template {i} - {metrics_str}\")\n",
    "\n",
    "        return options\n",
    "\n",
    "    def _extract_metric_name(self, column: str) -> str:\n",
    "        \"\"\"Extract clean metric name from column.\"\"\"\n",
    "        match = re.search(r\"\\.(\\w+)/\", column)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "        parts = column.split(\".\")\n",
    "        return parts[-1].split(\"/\")[0] if parts else column\n",
    "\n",
    "    def _get_template_data(self, index: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Get template and evaluation data for a specific index.\"\"\"\n",
    "        if 0 <= index < len(self.templates):\n",
    "            # Transpose template data for better display\n",
    "            template_df = self.templates[index].T.reset_index()\n",
    "            template_df.columns = [\"Field\", \"Value\"]\n",
    "\n",
    "            eval_df = self.eval_results[index]\n",
    "            return template_df, eval_df\n",
    "\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def display_template(self, template_selection: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Display selected template and evaluation results.\"\"\"\n",
    "        if not template_selection:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            # Extract index from selection\n",
    "            index = int(template_selection.split()[1])\n",
    "            return self._get_template_data(index)\n",
    "        except:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def create_interface(self) -> gr.Blocks:\n",
    "        \"\"\"Create the Gradio interface.\"\"\"\n",
    "        with gr.Blocks(title=\"VAPO Results Viewer\", theme=gr.themes.Soft()) as interface:\n",
    "            gr.Markdown(\"# VAPO Results Viewer\")\n",
    "            gr.Markdown(\"View and analyze Vertex AI Prompt Optimizer (VAPO) results\")\n",
    "\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    base_path_input = gr.Textbox(\n",
    "                        label=\"GCS Base Path\",\n",
    "                        placeholder=\"gs://your-bucket/vapo-results\",\n",
    "                        info=\"Enter the base GCS path containing VAPO runs\"\n",
    "                    )\n",
    "                with gr.Column(scale=1):\n",
    "                    with gr.Row():\n",
    "                        load_btn = gr.Button(\"Load Runs\", variant=\"primary\")\n",
    "                        clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
    "\n",
    "            with gr.Row():\n",
    "                run_dropdown = gr.Dropdown(\n",
    "                    label=\"Select Run\",\n",
    "                    choices=[],\n",
    "                    interactive=True\n",
    "                )\n",
    "                template_dropdown = gr.Dropdown(\n",
    "                    label=\"Select Template\",\n",
    "                    choices=[],\n",
    "                    interactive=True\n",
    "                )\n",
    "\n",
    "            with gr.Tabs():\n",
    "                with gr.Tab(\"Template Details\"):\n",
    "                    template_display = gr.DataFrame(\n",
    "                        label=\"Template Information\",\n",
    "                        wrap=True,\n",
    "                        interactive=False\n",
    "                    )\n",
    "\n",
    "                with gr.Tab(\"Evaluation Results\"):\n",
    "                    eval_display = gr.DataFrame(\n",
    "                        label=\"Evaluation Metrics\",\n",
    "                        wrap=True,\n",
    "                        interactive=False\n",
    "                    )\n",
    "\n",
    "            # Event handlers\n",
    "            load_btn.click(\n",
    "                fn=self.load_runs,\n",
    "                inputs=[base_path_input],\n",
    "                outputs=[run_dropdown]\n",
    "            )\n",
    "\n",
    "            clear_btn.click(\n",
    "                fn=self.clear_all,\n",
    "                inputs=[],\n",
    "                outputs=[base_path_input, run_dropdown, template_dropdown, template_display, eval_display]\n",
    "            )\n",
    "\n",
    "            run_dropdown.change(\n",
    "                fn=self.load_run_data,\n",
    "                inputs=[run_dropdown],\n",
    "                outputs=[template_dropdown, template_display, eval_display]\n",
    "            )\n",
    "\n",
    "            template_dropdown.change(\n",
    "                fn=self.display_template,\n",
    "                inputs=[template_dropdown],\n",
    "                outputs=[template_display, eval_display]\n",
    "            )\n",
    "\n",
    "        return interface\n",
    "\n",
    "\n",
    "def launch_app(share: bool = False, server_port: int = 7860, server_name: str = \"0.0.0.0\"):\n",
    "    \"\"\"Launch the Gradio application.\n",
    "\n",
    "    Args:\n",
    "        share: Whether to create a public share link\n",
    "        server_port: Port to run the server on\n",
    "        server_name: Server name/IP to bind to\n",
    "    \"\"\"\n",
    "    viewer = VAPOResultsViewer()\n",
    "    interface = viewer.create_interface()\n",
    "    interface.launch(\n",
    "        share=share,\n",
    "        server_port=server_port,\n",
    "        server_name=server_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL5FRIvTTDfk"
   },
   "source": [
    "## **Part 1: Zero-Shot Optimizer**\n",
    "\n",
    "We'll begin with the zero-shot approach. The following section will guide you through the process of optimizing your prompt without providing additional examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3Bp8o8NAO04"
   },
   "source": [
    "### Run a Zero-shot optimization job\n",
    "\n",
    "To run a `Zero-shot optimization job`, you can use the `optimize_prompt` method. The service will use a research-based metaprompt to optimize your initial prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLLA617pbuHg"
   },
   "outputs": [],
   "source": [
    "prompt = \"Generate system instructions for a question-answering assistant\"\n",
    "response = client.prompt_optimizer.optimize_prompt(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET3LUw5cFRQE"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response.suggested_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUr06qOWxuy9"
   },
   "source": [
    "## **Part 2: The Data-Driven Optimizer**\n",
    "\n",
    "The following sections will guide you through setting up your environment, preparing your data, and running an optimization job to find a better prompt using the data-driven optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIrgkunU8EhT"
   },
   "source": [
    "### The optimization configuration\n",
    "\n",
    "This is the most critical part of setting up the optimization job.\n",
    "\n",
    "The `OptimizationConfig` class, built using `pydantic`, acts as a structured and validated blueprint for our optimization task. It ensures all necessary parameters are defined before we submit the job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-yioYmFCBEB"
   },
   "outputs": [],
   "source": [
    "class OptimizationConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    A comprehensive prompt optimization configuration model.\n",
    "    \"\"\"\n",
    "    # Basic Configuration\n",
    "    system_instruction: str = Field(..., description=\"System instructions for the target model. String. This field is required.\")\n",
    "    prompt_template: str = Field(..., description=\"Template for prompts. String. This field is required.\")\n",
    "    target_model: str = Field(\"gemini-2.5-flash\", description='Target model for optimization. Supported models: \"gemini-2.5-flash\", \"gemini-2.5-pro\"')\n",
    "    thinking_budget: int = Field(-1, description=\"Thinking budget for thinking models. -1 means auto/no thinking. Integer.\")\n",
    "    optimization_mode: str = Field(\"instruction\", description='Optimization mode. Supported modes: \"instruction\", \"demonstration\", \"instruction_and_demo\".')\n",
    "    project: str = Field(..., description=\"Google Cloud project ID. This field is required.\")\n",
    "\n",
    "    # Evaluation Settings\n",
    "    # custom_metric_name=\"custom_engagement_personalization_score\",  # Metric name, as defined by the key that corresponds in the dictionary returned from Cloud function. String.\n",
    "    # custom_metric_cloud_function_name=\"custom_engagement_personalization_metric\",  # Cloud Run function name you previously deployed. String.\n",
    "    eval_metrics_types: List[str] = Field(\n",
    "        description='List of evaluation metrics. E.g., \"bleu\", \"rouge_l\", \"safety\".'\n",
    "    )\n",
    "    eval_metrics_weights: List[float] = Field(\n",
    "        description=\"Weights for evaluation metrics. Length must match eval_metrics_types and should sum to 1.\"\n",
    "    )\n",
    "    aggregation_type: str = Field(\"weighted_sum\", description='Aggregation type for metrics. Supported: \"weighted_sum\", \"weighted_average\".')\n",
    "\n",
    "    # Data and I/O Paths\n",
    "    input_data_path: str = Field(..., description=\"Cloud Storage URI to input optimization data. This field is required.\")\n",
    "    output_path: str = Field(..., description=\"Cloud Storage URI to save optimization results. This field is required.\")\n",
    "\n",
    "    # (Optional) Advanced Configuration\n",
    "    num_steps: int = Field(10, ge=10, le=20, description=\"Number of iterations in instruction optimization mode. Integer between 10 and 20.\")\n",
    "    num_demo_set_candidates: int = Field(10, ge=10, le=30, description=\"Number of demonstrations evaluated. Integer between 10 and 30.\")\n",
    "    demo_set_size: int = Field(3, ge=3, le=6, description=\"Number of demonstrations generated per prompt. Integer between 3 and 6.\")\n",
    "\n",
    "\n",
    "    # (Optional) Model Locations and QPS\n",
    "    target_model_location: str = Field(\"us-central1\", description=\"Location of the target model. Default us-central1.\")\n",
    "    target_model_qps: int = Field(1, ge=1, description=\"QPS for the target model. Integer >= 1, based on your quota.\")\n",
    "    optimizer_model_location: str = Field(\"us-central1\", description=\"Location of the optimizer model. Default us-central1.\")\n",
    "    optimizer_model_qps: int = Field(1, ge=1, description=\"QPS for the optimization model. Integer >= 1, based on your quota.\")\n",
    "    source_model: str = Field(\"\", description=\"Google model previously used with these prompts. Not needed if providing a target column.\")\n",
    "    source_model_location: str = Field(\"us-central1\", description=\"Location of the source model. Default us-central1.\")\n",
    "    source_model_qps: Optional[int] = Field(None, ge=1, description=\"Optional QPS for the source model. Integer >= 1.\")\n",
    "    eval_qps: int = Field(1, ge=1, description=\"QPS for the eval model. Integer >= 1, based on your quota.\")\n",
    "\n",
    "    # (Optional) Response, Language, and Data Handling\n",
    "    response_mime_type: str = Field(\"text/plain\", description=\"MIME response type from the target model. E.g., 'text/plain', 'application/json'.\")\n",
    "    response_schema: str = Field(\"\", description=\"The Vertex AI Controlled Generation response schema.\")\n",
    "    language: str = Field(\"English\", description='Language of the system instructions. E.g., \"English\", \"Japanese\".')\n",
    "    placeholder_to_content: Dict[str, Any] = Field({}, description=\"Dictionary of placeholders to replace parameters in the system instruction.\")\n",
    "    data_limit: int = Field(10, ge=5, le=100, description=\"Amount of data used for validation. Integer between 5 and 100.\")\n",
    "    translation_source_field_name: str = Field(\"\", description=\"Field name for source text if using translation metrics (Comet, MetricX).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfpGmIWrVEt1"
   },
   "source": [
    "### Preparing the Data and Running the Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xXys6YmDS2v"
   },
   "source": [
    "#### The dataset\n",
    "\n",
    "The optimizer's performance depends heavily on the quality of your sample data.\n",
    "\n",
    "For this example, we use a question-answering dataset where each row contains a `question`, context (`ctx`), and a ground-truth `target` answer. The `{target}` variable is crucial for computation-based evaluation metrics like `question_answering_correctness`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWMSgAdWDWwW"
   },
   "outputs": [],
   "source": [
    "input_data_path = \"gs://github-repo/prompts/prompt_optimizer/rag_qa_dataset.jsonl\"\n",
    "prompt_optimization_df = pd.read_json(input_data_path, lines=True)\n",
    "prompt_optimization_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceZNbD_YzLEY"
   },
   "source": [
    "#### Set optimization configuration\n",
    "\n",
    "Now, we'll create a dictionary with our specific settings and use it to instantiate our `OptimizationConfig` class. This populates our configuration blueprint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40Pyzkot040M"
   },
   "outputs": [],
   "source": [
    "output_path = f\"{BUCKET_URI}/optimization_results/\"\n",
    "\n",
    "vapo_data_settings = {\n",
    "    \"system_instruction\": \"You are an helpful assistant. Given a question with context, provide the correct answer to the question.\",\n",
    "    \"prompt_template\":  \"Some examples of correct answer to a question are:\\nQuestion: {question}\\nContext: {ctx}\\nAnswer: {target}\",\n",
    "    \"target_model\": \"gemini-2.5-flash\",\n",
    "    \"thinking_budget\": -1,\n",
    "    \"optimization_mode\": \"instruction\",\n",
    "    \"eval_metrics_types\": [\"question_answering_correctness\", \"fluency\"],\n",
    "    \"eval_metrics_weights\": [0.8, 0.2],\n",
    "    \"aggregation_type\": \"weighted_sum\",\n",
    "    \"input_data_path\": input_data_path,\n",
    "    \"output_path\": output_path,\n",
    "    \"project\": PROJECT_ID,\n",
    "}\n",
    "\n",
    "vapo_data_config = OptimizationConfig(**vapo_data_settings)\n",
    "vapo_data_config_json = vapo_data_config.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92qSHhIT838O"
   },
   "source": [
    "#### Upload configuration to Cloud Storage\n",
    "\n",
    "Write the Prompt Optimizer configuration to the file in your GCS bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PG_a6ss4J1l"
   },
   "outputs": [],
   "source": [
    "config_path = f'{BUCKET_URI}/config.json'\n",
    "\n",
    "with epath.Path(config_path).open(\"w\") as config_file:\n",
    "    json.dump(vapo_data_config_json, config_file)\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpRGZTk68-Nu"
   },
   "source": [
    "#### Run the prompt optimization job\n",
    "\n",
    "This is the final step. We pass the path to our configuration file and the service account to the Vertex AI client. The `optimize` method starts the custom job on the Vertex AI backend. We set `wait_for_completion` to `True` so the script will pause until the job is finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGZKNjsu6EEw"
   },
   "outputs": [],
   "source": [
    "vapo_data_run_config = {\n",
    "    \"config_path\": config_path,\n",
    "    \"wait_for_completion\": True,\n",
    "    \"service_account\": SERVICE_ACCOUNT\n",
    "}\n",
    "\n",
    "result = client.prompt_optimizer.optimize(method=\"vapo\", config=vapo_data_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viiv5r23KC-X"
   },
   "source": [
    "### Visualize results with the interactive app\n",
    "\n",
    "The tutorial includes a helper function to launch a Gradio-based web interface. This is a great way to visually explore all the different instructions the optimizer generated and compare their evaluation scores side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS4NybYBxMSd"
   },
   "outputs": [],
   "source": [
    "launch_app(share=True, server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJL6tRAWKyXz"
   },
   "source": [
    "### Get and use the best prompt programmatically\n",
    "\n",
    "For use in an application, you can programmatically retrieve the top-performing instruction from the output files stored in GCS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b_LRAhyxOvQ"
   },
   "outputs": [],
   "source": [
    "best_instruction, _ = get_best_vapo_results(output_path)\n",
    "print(\"The optimized instruction is:\" , best_instruction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_vertex_ai_prompt_optimizer.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
