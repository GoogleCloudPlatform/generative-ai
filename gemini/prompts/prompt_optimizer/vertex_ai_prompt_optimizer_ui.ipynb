{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlI1rYKa2IGx"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN8N3O43QDT5"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fvertex_ai_prompt_optimizer_ui.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHyuJTFr2IGx"
      },
      "source": [
        "# Overview\n",
        "Welcome to Vertex AI Prompt Optimizer (VAPO)! This Notebook showcases VAPO, a tool that iteratively optimizes prompts to suit a target model (e.g., `gemini-1.5-pro`) using target-specific metric(s).\n",
        "\n",
        "Key Use Cases:\n",
        "\n",
        "* Prompt Optimization: Enhance the quality of an initial prompt by refining its structure and content to match the target model's optimal input characteristics.\n",
        "\n",
        "* Prompt Translation: Adapt prompts optimized for one model to work effectively with a different target model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTtKHedrO1Rx"
      },
      "source": [
        "# Step 0: Install packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Zw72vFORz_"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U google-cloud-aiplatform -q\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from google.auth import default\n",
        "from google.cloud import aiplatform, storage\n",
        "from google.colab import auth, output\n",
        "import gspread\n",
        "import ipywidgets as widgets\n",
        "import jinja2\n",
        "from jinja2 import BaseLoader, Environment\n",
        "import jinja2.meta\n",
        "import pandas as pd\n",
        "import tensorflow.io.gfile as gfile\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "from io import StringIO\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "def authenticate():\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    return gspread.authorize(creds)\n",
        "\n",
        "\n",
        "def is_target_required_metric(eval_metric: str) -> bool:\n",
        "    return eval_metric in [\n",
        "        \"bleu\",\n",
        "        \"exact_match\",\n",
        "        \"question_answering_correctness\",\n",
        "        \"rouge_1\",\n",
        "        \"rouge_2\",\n",
        "        \"rouge_l\",\n",
        "        \"rouge_l_sum\",\n",
        "        \"tool_call_valid\",\n",
        "        \"tool_name_match\",\n",
        "        \"tool_parameter_key_match\",\n",
        "        \"tool_parameter_kv_match\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def is_run_target_required(eval_metric_types: list[str], source_model: str) -> bool:\n",
        "    if source_model:\n",
        "        return False\n",
        "\n",
        "    label_required = False\n",
        "    for metric in eval_metric_types:\n",
        "        label_required = label_required or is_target_required_metric(metric)\n",
        "    return label_required\n",
        "\n",
        "\n",
        "_TARGET_KEY = \"target\"\n",
        "\n",
        "\n",
        "def validate_prompt_and_data(\n",
        "    template: str,\n",
        "    dataset_path: str,\n",
        "    placeholder_to_content: str,\n",
        "    label_enforced: bool,\n",
        ") -> None:\n",
        "    \"\"\"Validates the prompt template and the dataset.\"\"\"\n",
        "    placeholder_to_content = json.loads(placeholder_to_content)\n",
        "    with gfile.GFile(dataset_path, \"r\") as f:\n",
        "        data = [json.loads(line) for line in f.readlines()]\n",
        "\n",
        "    env = jinja2.Environment()\n",
        "    try:\n",
        "        parsed_content = env.parse(template)\n",
        "    except jinja2.exceptions.TemplateSyntaxError as e:\n",
        "        raise ValueError(f\"Invalid template: {template}\") from e\n",
        "\n",
        "    template_variables = jinja2.meta.find_undeclared_variables(parsed_content)\n",
        "    extra_keys = set()\n",
        "    for ex in data:\n",
        "        ex.update(placeholder_to_content)\n",
        "        missing_keys = [key for key in template_variables if key not in ex]\n",
        "        extra_keys.update([key for key in ex if key not in template_variables])\n",
        "        if label_enforced:\n",
        "            if _TARGET_KEY not in ex:\n",
        "                raise ValueError(\n",
        "                    f\"The example {ex} doesn't have a key corresponding to the target\"\n",
        "                    f\" var: {_TARGET_KEY}\"\n",
        "                )\n",
        "            if not ex[_TARGET_KEY]:\n",
        "                raise ValueError(f\"The following example has an empty target: {ex}\")\n",
        "        if missing_keys:\n",
        "            raise ValueError(\n",
        "                f\"The example {ex} doesn't have a key corresponding to following\"\n",
        "                f\" template vars: {missing_keys}\"\n",
        "            )\n",
        "    if extra_keys:\n",
        "        raise Warning(\n",
        "            \"Warning: extra keys in the examples not used in the context/task\"\n",
        "            f\" template {extra_keys}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def run_custom_job(\n",
        "    display_name: str,\n",
        "    container_uri: str,\n",
        "    container_args: dict[str, str],\n",
        ") -> None:\n",
        "    \"\"\"A sample to create custom jobs.\"\"\"\n",
        "    worker_pool_specs = [\n",
        "        {\n",
        "            \"replica_count\": 1,\n",
        "            \"container_spec\": {\n",
        "                \"image_uri\": container_uri,\n",
        "                \"args\": [f\"--{k}={v}\" for k, v in container_args.items()],\n",
        "            },\n",
        "            \"machine_spec\": {\n",
        "                \"machine_type\": \"n1-standard-4\",\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    custom_job = aiplatform.CustomJob(\n",
        "        display_name=display_name,\n",
        "        worker_pool_specs=worker_pool_specs,\n",
        "    )\n",
        "    custom_job.submit()\n",
        "    return custom_job\n",
        "\n",
        "\n",
        "def run_apd(config: dict[str, str], bucket_uri: str, display_name: str) -> None:\n",
        "    \"\"\"A function to the vertex prompt optimizer.\"\"\"\n",
        "    print(f\"\\n\\nJob display name: {display_name}\")\n",
        "    version = \"preview_v1_0\"\n",
        "    container_uri = \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd\"\n",
        "    config_path = f\"{bucket_uri}/{display_name}/input_config.json\"\n",
        "\n",
        "    with gfile.GFile(config_path, \"w\") as f:\n",
        "        json.dump(config, f)\n",
        "\n",
        "    aiplatform.init(\n",
        "        project=config[\"project\"],\n",
        "        location=config[\"target_model_location\"],\n",
        "        staging_bucket=f\"{bucket_uri}/{display_name}\",\n",
        "    )\n",
        "\n",
        "    return run_custom_job(\n",
        "        display_name=display_name,\n",
        "        container_uri=f\"{container_uri}:{version}\",\n",
        "        container_args={\"config\": config_path},\n",
        "    )\n",
        "\n",
        "\n",
        "def update_best_display(\n",
        "    df: pd.DataFrame,\n",
        "    textarea: widgets.Textarea,\n",
        "    best_score_label: widgets.Label,\n",
        "    eval_metric: str,\n",
        ") -> None:\n",
        "    \"\"\"Update the best prompt display.\"\"\"\n",
        "\n",
        "    df[\"score\"] = df[f\"metrics.{eval_metric}/mean\"]\n",
        "\n",
        "    best_template = df.loc[df[\"score\"].argmax(), \"prompt\"]\n",
        "    best_score = df.loc[df[\"score\"].argmax(), \"score\"]\n",
        "    original_score = df.loc[0, \"score\"]\n",
        "\n",
        "    def placeholder_llm():\n",
        "        return \"{{llm()}}\"\n",
        "\n",
        "    env = Environment(loader=BaseLoader())\n",
        "    env.globals[\"llm\"] = placeholder_llm\n",
        "\n",
        "    best_template = best_template.replace(\"store('answer', llm())\", \"llm()\")\n",
        "    textarea.value = best_template\n",
        "    improvement = best_score - original_score\n",
        "    no_improvement_str = \"\\nNo better template is found yet.\" if not improvement else \"\"\n",
        "    best_score_label.value = (\n",
        "        f\"Score: {best_score}\" f\" Improvement: {improvement: .3f} {no_improvement_str}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_dataframe(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"Generates a pandas dataframe from a json file.\"\"\"\n",
        "    if not gfile.exists(filename):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    with gfile.GFile(filename, \"r\") as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "        except:\n",
        "            return pd.DataFrame()\n",
        "        return pd.json_normalize(data)\n",
        "\n",
        "\n",
        "def left_aligned_df_html(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Displays a Pandas DataFrame in Colab with left-aligned values.\"\"\"\n",
        "\n",
        "    # Convert to HTML table, but keep the HTML in a variable\n",
        "    html_table = df.to_html(index=False, classes=\"left-aligned\")\n",
        "\n",
        "    # Add CSS styling to left-align table data cells and override default styles\n",
        "    styled_html = f\"\"\"\n",
        "    <style>\n",
        "        .left-aligned td, .left-aligned th {{ text-align: left !important; }}\n",
        "    </style>\n",
        "    {html_table}\n",
        "    \"\"\"\n",
        "\n",
        "    # Display the styled HTML table\n",
        "    return HTML(styled_html)\n",
        "\n",
        "\n",
        "def extract_top_level_function_name(source_code: str) -> str | None:\n",
        "    match = re.search(r\"^def\\s+([a-zA-Z_]\\w*)\\s*\\(\", source_code, re.MULTILINE)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "\n",
        "class ProgressForm:\n",
        "    \"\"\"A class to display the progress of the optimization job.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.instruction_progress_bar = None\n",
        "        self.instruction_display = None\n",
        "        self.instruction_best = None\n",
        "        self.instruction_score = None\n",
        "\n",
        "        self.demo_progress_bar = None\n",
        "        self.demo_display = None\n",
        "        self.demo_best = None\n",
        "        self.demo_score = None\n",
        "\n",
        "        self.job_state_display = None\n",
        "\n",
        "        self.instruction_df = None\n",
        "        self.demo_df = None\n",
        "\n",
        "        self.started = False\n",
        "\n",
        "    def init(self, params: dict[str, str]):\n",
        "        \"\"\"Initialize the progress form.\"\"\"\n",
        "        self.job_state_display = display(\n",
        "            HTML(\"<span>Job State: Not Started!</span>\"), display_id=True\n",
        "        )\n",
        "        self.status_display = display(HTML(\"\"), display_id=True)\n",
        "\n",
        "        if params[\"optimization_mode\"] in [\"instruction\", \"instruction_and_demo\"]:\n",
        "            (\n",
        "                self.instruction_progress_bar,\n",
        "                self.instruction_display,\n",
        "                self.instruction_best,\n",
        "                self.instruction_score,\n",
        "            ) = self.create_progress_ui(\"Instruction\", params[\"num_steps\"])\n",
        "\n",
        "        if params[\"optimization_mode\"] in [\"demonstration\", \"instruction_and_demo\"]:\n",
        "            (\n",
        "                self.demo_progress_bar,\n",
        "                self.demo_display,\n",
        "                self.demo_best,\n",
        "                self.demo_score,\n",
        "            ) = self.create_progress_ui(\n",
        "                \"Demonstration\", params[\"num_demo_set_candidates\"]\n",
        "            )\n",
        "\n",
        "        eval_metric = \"composite_metric\"\n",
        "        if len(params[\"eval_metrics_types\"]) == 1:\n",
        "            eval_metric = params[\"eval_metrics_types\"][0]\n",
        "\n",
        "        if eval_metric != \"composite_metric\" and \"custom_metric_source_code\" in params:\n",
        "            self.eval_metric = extract_top_level_function_name(\n",
        "                params[\"custom_metric_source_code\"]\n",
        "            )\n",
        "        else:\n",
        "            self.eval_metric = eval_metric\n",
        "\n",
        "        self.output_path = params[\"output_path\"]\n",
        "        self.started = True\n",
        "\n",
        "    def update_progress(\n",
        "        self,\n",
        "        progress_bar: widgets.IntProgress,\n",
        "        templates_file: str,\n",
        "        df: pd.DataFrame | None,\n",
        "        df_display: display,\n",
        "        best_textarea: widgets.Textarea,\n",
        "        best_score: widgets.Label,\n",
        "        eval_metric: str,\n",
        "    ):\n",
        "        \"\"\"Update the progress of the optimization job.\"\"\"\n",
        "\n",
        "        def get_last_step(df: pd.DataFrame):\n",
        "            if df.empty:\n",
        "                return -1\n",
        "            return int(df[\"step\"].max())\n",
        "\n",
        "        if progress_bar is None or df is None:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        new_df = generate_dataframe(templates_file)\n",
        "\n",
        "        last_step = get_last_step(df)\n",
        "        new_last_step = get_last_step(new_df)\n",
        "        if new_last_step > last_step:\n",
        "            df_display.update(left_aligned_df_html(new_df))\n",
        "            update_best_display(new_df, best_textarea, best_score, eval_metric)\n",
        "            progress_bar.value = progress_bar.value + new_last_step - last_step\n",
        "\n",
        "        return new_df\n",
        "\n",
        "    def create_progress_ui(\n",
        "        self, opt_mode: str, num_opt_steps: int\n",
        "    ) -> tuple[widgets.IntProgress, display, widgets.Textarea, widgets.Label]:\n",
        "        \"\"\"Create the progress UI for a specific optimization mode.\"\"\"\n",
        "        print(f\"\\n\\n{opt_mode} Optimization\")\n",
        "        progress_bar = widgets.IntProgress(\n",
        "            value=0, min=0, max=num_opt_steps, step=1, description=\"Progress\"\n",
        "        )\n",
        "        display(progress_bar)\n",
        "        print(\"\\nGenerated Templates:\")\n",
        "        templates_display = display(\"No template is evaluated yet!\", display_id=True)\n",
        "\n",
        "        print(\"\\nBest Template so far:\")\n",
        "        best_textarea = widgets.Textarea(\n",
        "            value=\"NA\",\n",
        "            disabled=False,\n",
        "            layout=widgets.Layout(width=\"80%\", height=\"150px\"),\n",
        "        )\n",
        "        display(best_textarea)\n",
        "\n",
        "        best_score = widgets.Label(value=\"Score: NA Improvement: NA\")\n",
        "        display(best_score)\n",
        "\n",
        "        return progress_bar, templates_display, best_textarea, best_score\n",
        "\n",
        "    def monitor_progress(self, job: aiplatform.CustomJob, params: dict[str, str]):\n",
        "        \"\"\"Monitor the progress of the optimization job.\"\"\"\n",
        "        if not self.started:\n",
        "            self.init(params)\n",
        "\n",
        "        self.job_state_display.update(HTML(f\"<span>Job State: {job.state.name}</span>\"))\n",
        "\n",
        "        # Initial display of the dataframe\n",
        "        instruction_templates_file = f\"{self.output_path}/instruction/templates.json\"\n",
        "        demo_templates_file = f\"{self.output_path}/demonstration/templates.json\"\n",
        "\n",
        "        if not job.done():\n",
        "            self.instruction_df = self.update_progress(\n",
        "                self.instruction_progress_bar,\n",
        "                instruction_templates_file,\n",
        "                self.instruction_df,\n",
        "                self.instruction_display,\n",
        "                self.instruction_best,\n",
        "                self.instruction_score,\n",
        "                self.eval_metric,\n",
        "            )\n",
        "            self.demo_df = self.update_progress(\n",
        "                self.demo_progress_bar,\n",
        "                demo_templates_file,\n",
        "                self.demo_df,\n",
        "                self.demo_display,\n",
        "                self.demo_best,\n",
        "                self.demo_score,\n",
        "                self.eval_metric,\n",
        "            )\n",
        "            return True\n",
        "\n",
        "        if job.state.name != \"JOB_STATE_SUCCEEDED\":\n",
        "            errors = [f\"Error: Job failed with error {job.error}.\"]\n",
        "            for err_file in [\n",
        "                f\"{self.output_path}/instruction/error.json\",\n",
        "                f\"{self.output_path}/demonstration/error.json\",\n",
        "            ]:\n",
        "                if gfile.exists(err_file):\n",
        "                    with gfile.GFile(err_file, \"r\") as f:\n",
        "                        error_json = json.load(f)\n",
        "                    errors.append(f\"Detailed error: {error_json}\")\n",
        "                    errors.append(\n",
        "                        f\"Please feel free to send {err_file} to the VAPO team to help\"\n",
        "                        \" resolving the issue.\"\n",
        "                    )\n",
        "\n",
        "            errors.append(\n",
        "                \"All the templates found before failure can be found under\"\n",
        "                f\" {self.output_path}\"\n",
        "            )\n",
        "            errors.append(\n",
        "                \"Please consider rerunning to make sure the failure is intransient.\"\n",
        "            )\n",
        "            err = \"\\n\".join(errors)\n",
        "            self.status_display.update(HTML(f'<span style=\"color: red;\">{err}</span>'))\n",
        "        else:\n",
        "            self.status_display.update(\n",
        "                HTML(\n",
        "                    '<span style=\"color: green;\">Job succeeded!</span> <span>All the'\n",
        "                    f\" artifacts can be found under {self.output_path}</span>\"\n",
        "                )\n",
        "            )\n",
        "        return False\n",
        "\n",
        "\n",
        "def display_dataframe(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Display a pandas dataframe in Colab.\"\"\"\n",
        "\n",
        "    # Function to wrap text in a scrollable div\n",
        "    def wrap_in_scrollable_div(text):\n",
        "        return f'<div class=\"scrollable\">{text}</div>'\n",
        "\n",
        "    # Apply the function to every cell using the format method\n",
        "    styled_html = df.style.format(wrap_in_scrollable_div).to_html(index=False)\n",
        "\n",
        "    # Display the HTML in the notebook\n",
        "    display(HTML(styled_html))\n",
        "\n",
        "\n",
        "def split_gcs_path(gcs_path: str) -> tuple[str, str]:\n",
        "    \"\"\"Splits a full GCS path into bucket name and prefix.\"\"\"\n",
        "    if gcs_path.startswith(\"gs://\"):\n",
        "        path_without_scheme = gcs_path[5:]  # Remove the 'gs://' part\n",
        "        parts = path_without_scheme.split(\"/\", 1)\n",
        "        bucket_name = parts[0]\n",
        "        prefix = parts[1] if len(parts) > 1 else \"\"\n",
        "        return bucket_name, prefix\n",
        "    else:\n",
        "        raise ValueError(\"Invalid GCS path. Must start with 'gs://'\")\n",
        "\n",
        "\n",
        "def list_gcs_objects(full_gcs_path: str) -> list[str]:\n",
        "    \"\"\"Lists all the objects in the given GCS path.\"\"\"\n",
        "    bucket_name, prefix = split_gcs_path(full_gcs_path)\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(\n",
        "        prefix=prefix\n",
        "    )  # List all objects that start with the prefix\n",
        "\n",
        "    return [blob.name for blob in blobs]\n",
        "\n",
        "\n",
        "def find_directories_with_files(\n",
        "    full_gcs_path: str, required_files: list[str]\n",
        ") -> list[str]:\n",
        "    \"\"\"Finds directories containing specific files under the given full GCS path.\"\"\"\n",
        "    bucket_name, prefix = split_gcs_path(full_gcs_path)\n",
        "    all_paths = list_gcs_objects(f\"gs://{bucket_name}/{prefix}\")\n",
        "    directories = set()\n",
        "\n",
        "    # Create a dictionary to track files found in each directory\n",
        "    file_presence = {}\n",
        "    for path in all_paths:\n",
        "        directory = \"/\".join(path.split(\"/\")[:-1])  # Get the directory part of the path\n",
        "        filename = path.split(\"/\")[-1]  # Get the filename part of the path\n",
        "        if directory:\n",
        "            if directory not in file_presence:\n",
        "                file_presence[directory] = set()\n",
        "            file_presence[directory].add(filename)\n",
        "\n",
        "    # Check which directories have all required files\n",
        "    for directory, files in file_presence.items():\n",
        "        if all(file in files for file in required_files):\n",
        "            directories.add(f\"gs://{bucket_name}/{directory}\")\n",
        "\n",
        "    return list(directories)\n",
        "\n",
        "\n",
        "def extract_metric_name(metric_string: str):\n",
        "    # Use a regular expression to find the metric name\n",
        "    match = re.search(r\"\\.(\\w+)/\", metric_string)\n",
        "    # Return the matched group if found\n",
        "    return match.group(1) if match else metric_string\n",
        "\n",
        "\n",
        "def read_file_from_gcs(filename: str):\n",
        "    with gfile.GFile(filename, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def process_results(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Process the results removing columns that could be confusing.\"\"\"\n",
        "    columns_to_drop = []\n",
        "    # Dropping columns that could be confusing.\n",
        "    for col in df.columns:\n",
        "        if \"confidence\" in col:\n",
        "            columns_to_drop.append(col)\n",
        "        if \"raw_eval_resp\" in col:\n",
        "            columns_to_drop.append(col)\n",
        "        if col == \"instruction\":\n",
        "            columns_to_drop.append(col)\n",
        "        if col == \"context\":\n",
        "            columns_to_drop.append(col)\n",
        "    return df.drop(columns=columns_to_drop)\n",
        "\n",
        "\n",
        "class ResultsUI:\n",
        "    \"\"\"A UI to display the results of a VAPO run.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str):\n",
        "        required_files = [\"eval_results.json\", \"templates.json\"]\n",
        "        runs = find_directories_with_files(path, required_files)\n",
        "\n",
        "        self.run_label = widgets.Label(\"Select Run:\")\n",
        "        self.run_dropdrown = widgets.Dropdown(\n",
        "            options=runs, value=runs[0], layout=widgets.Layout(width=\"200px\")\n",
        "        )\n",
        "        self.run_dropdrown.observe(self.display_run_handler, names=\"value\")\n",
        "\n",
        "        # Create a label widget for the description\n",
        "        self.dropdown_description = widgets.Label(\"Select Template:\")\n",
        "        self.template_dropdown = widgets.Dropdown(\n",
        "            options=[],\n",
        "            value=None,\n",
        "            layout=widgets.Layout(width=\"400px\"),\n",
        "            disabled=True,\n",
        "        )\n",
        "        self.template_dropdown.observe(self.display_template_handler, names=\"value\")\n",
        "        self.results_output = widgets.Output(\n",
        "            layout=widgets.Layout(\n",
        "                height=\"600px\", overflow=\"auto\", margin=\"20px 0px 0px 0px\"\n",
        "            )\n",
        "        )\n",
        "        self.display_run(runs[0])\n",
        "\n",
        "    def display_template_handler(self, change: dict[str, str]) -> None:\n",
        "        \"\"\"Display the template and the corresponding evaluation results.\"\"\"\n",
        "        if change[\"new\"] is None:\n",
        "            return\n",
        "        df_index = int(change[\"new\"].split(\" \")[1])\n",
        "        self.display_eval_results(df_index)\n",
        "\n",
        "    def display_run_handler(self, change) -> None:\n",
        "        if change[\"new\"] is None:\n",
        "            return\n",
        "\n",
        "        path = change[\"new\"]\n",
        "        self.display_run(path)\n",
        "\n",
        "    def display_run(self, path: str) -> None:\n",
        "        \"\"\"Display the results of a VAPO run.\"\"\"\n",
        "        self.run_dropdrown.disabled = True\n",
        "        filename = f\"{path}/eval_results.json\"\n",
        "        eval_results = json.loads(read_file_from_gcs(filename))\n",
        "\n",
        "        filename = f\"{path}/templates.json\"\n",
        "        templates = json.loads(read_file_from_gcs(filename))\n",
        "\n",
        "        if len(templates) == len(eval_results):\n",
        "            offset = 0\n",
        "        elif len(templates) == len(eval_results) + 1:\n",
        "            # In some setups it is possible to have 1 more template than results.\n",
        "            offset = 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Number of templates doesn't match number of eval results\"\n",
        "                f\" {len(templates)} vs {len(eval_results)}\"\n",
        "            )\n",
        "        self.templates = [\n",
        "            pd.json_normalize(template) for template in templates[offset:]\n",
        "        ]\n",
        "        metric_columns = [col for col in self.templates[0].columns if \"metric\" in col]\n",
        "\n",
        "        self.eval_results = [\n",
        "            process_results(pd.read_json(StringIO(result[\"metrics_table\"])))\n",
        "            for result in eval_results\n",
        "        ]\n",
        "        options = []\n",
        "        for i, template in enumerate(self.templates):\n",
        "            metrics = []\n",
        "            for col in metric_columns:\n",
        "                value = template[col].tolist()[0]\n",
        "                short_col = extract_metric_name(col)\n",
        "                metrics.append(f\"{short_col}: {value}\")\n",
        "            metrics_str = \" \".join(metrics)\n",
        "            options.append(f\"Template {i} {metrics_str}\")\n",
        "\n",
        "        self.template_dropdown.disabled = False\n",
        "        self.template_dropdown.options = options\n",
        "        self.run_dropdrown.disabled = False\n",
        "\n",
        "    def display_eval_results(self, index: int) -> None:\n",
        "        \"\"\"Display the evaluation results for a specific template.\"\"\"\n",
        "        with self.results_output:\n",
        "            self.results_output.clear_output(wait=True)  # Clear previous output\n",
        "            display_dataframe(self.templates[index])\n",
        "            print()\n",
        "            display_dataframe(self.eval_results[index])\n",
        "\n",
        "    def get_container(self) -> widgets.Output:\n",
        "        \"\"\"Get the container widget for the results UI.\"\"\"\n",
        "        return widgets.VBox(\n",
        "            [\n",
        "                self.run_label,\n",
        "                self.run_dropdrown,\n",
        "                self.dropdown_description,\n",
        "                self.template_dropdown,\n",
        "                self.results_output,\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p59jd5rOp4q"
      },
      "source": [
        "# Step 1: Configure your prompt template\n",
        "Prompts consist of two key parts:\n",
        "* System Instruction (SI) Template: A fixed instruction shared across all queries for a given task.\n",
        "* Task/Context Template: A dynamic part that changes based on the task.\n",
        "\n",
        "APD enables the translation and optimization of the System Instruction Template, while the Task/Context Template remains essential for evaluating different SI templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJG1pVZO317x"
      },
      "outputs": [],
      "source": [
        "SYSTEM_INSTRUCTION = \"Answer the following question. Let's think step by step.\\n\"  # @param {type:\"string\"}\n",
        "PROMPT_TEMPLATE = (\n",
        "    \"Question: {{question}}\\n\\nAnswer:{{target}}\"  # @param {type:\"string\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y-cmg0TQP6v"
      },
      "source": [
        "# Step 2: Input your data\n",
        "To optimize the model, provide a CSV or JSONL file containing labeled validation samples\n",
        "* Focus on examples that specifically demonstrate the issues you want to address.\n",
        "* Recommendation: Use 50-100 distinct samples for reliable results. However, the tool can still be effective with as few as 5 samples.\n",
        "\n",
        "For prompt translation:\n",
        "* Consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfgi_oR6tTIB"
      },
      "outputs": [],
      "source": [
        "# @markdown **Project setup**: <br/>\n",
        "PROJECT_ID = \"[YOUR_PROJECT]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "OUTPUT_PATH = \"[OUTPUT_PATH]\"  # @param {type:\"string\"}\n",
        "# @markdown * GCS path of your bucket, e.g., gs://prompt_translation_demo, used to store all artifacts.\n",
        "INPUT_DATA_PATH = \"[INPUT_DATA_PATH]\"  # @param {type:\"string\"}\n",
        "# @markdown * Specify a GCS path for the input data, e.g., gs://prompt_translation_demo/input_data.jsonl."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucebZHkHRxKH"
      },
      "source": [
        "# Step 3: Configure optimization settings\n",
        "The optimization configs are defaulted to the values that are most commonly used and which we recommend using initially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2R3P8mMvK9q"
      },
      "outputs": [],
      "source": [
        "TARGET_MODEL = \"gemini-1.5-flash-001\"  # @param [\"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\"]\n",
        "SOURCE_MODEL = \"\"  # @param [\"\", \"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
        "# @markdown * If set, it will be used to generate ground truth responses for the input examples. This is useful to migrate the prompt from a source model.\n",
        "OPTIMIZATION_MODE = \"instruction_and_demo\"  # @param [\"instruction\", \"demonstration\", \"instruction_and_demo\"]\n",
        "OPTIMIZATION_METRIC = \"question_answering_correctness\"  # @param [\"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO7fO0qTSNLs"
      },
      "source": [
        "# Step 4: Configure advanced optimization settings [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRHHTpaV4Xyo"
      },
      "outputs": [],
      "source": [
        "# @markdown **Instruction Optimization Configs**: <br/>\n",
        "NUM_INST_OPTIMIZATION_STEPS = 10  # @param {type:\"integer\"}\n",
        "NUM_TEMPLATES_PER_STEP = 2  # @param {type:\"integer\"}\n",
        "# @markdown * Number of prompt templates generated and evaluated at each optimization step.\n",
        "\n",
        "# @markdown **Demonstration Optimization Configs**: <br/>\n",
        "NUM_DEMO_OPTIMIZATION_STEPS = 10  # @param {type:\"integer\"}\n",
        "NUM_DEMO_PER_PROMPT = 3  # @param {type:\"integer\"}\n",
        "# @markdown * Number of the demonstrations to include in each prompt.\n",
        "\n",
        "# @markdown **Model Configs**: <br/>\n",
        "TARGET_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
        "SOURCE_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
        "OPTIMIZER_MODEL = \"gemini-1.5-flash-001\"  # @param [\"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
        "# @markdown * The model used to generated alternative prompts in the instruction optimization mode.\n",
        "OPTIMIZER_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
        "EVAL_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
        "# @markdown * The QPS for calling the eval model, which is currently gemini-1.5-pro-001.\n",
        "\n",
        "# @markdown **Multi-metric Configs**: <br/>\n",
        "# @markdown Use this section only if you need more than one metric for optimization. This will override the metric you picked above.\n",
        "OPTIMIZATION_METRIC_1 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "OPTIMIZATION_METRIC_1_WEIGHT = 0.0  # @param {type:\"number\"}\n",
        "OPTIMIZATION_METRIC_2 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "OPTIMIZATION_METRIC_2_WEIGHT = 0.0  # @param {type:\"number\"}\n",
        "OPTIMIZATION_METRIC_3 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "OPTIMIZATION_METRIC_3_WEIGHT = 0.0  # @param {type:\"number\"}\n",
        "METRIC_AGGREGATION_TYPE = \"weighted_sum\"  # @param [\"weighted_sum\", \"weighted_average\"]\n",
        "\n",
        "# @markdown **Misc Configs**: <br/>\n",
        "PLACEHOLDER_TO_VALUE = \"{}\"  # @param\n",
        "# @markdown * This variable is used for long prompt optimization to not optimize parts of prompt identified by placeholders. It provides a mapping from the placeholder variables to their content. See link for details.\n",
        "RESPONSE_MIME_TYPE = \"application/json\"  # @param [\"text/plain\", \"application/json\"]\n",
        "# @markdown * This variable determines the format of the output for the target model. See link for details.\n",
        "TARGET_LANGUAGE = \"English\"  # @param [\"English\", \"French\", \"German\", \"Hebrew\", \"Hindi\", \"Japanese\", \"Korean\", \"Portuguese\", \"Simplified Chinese\", \"Spanish\", \"Traditional Chinese\"]\n",
        "# @markdown * The language of the system instruction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Mgb0EHSSFk"
      },
      "source": [
        "# Step 5: Run Prompt Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8NvNLTfxPTf"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "display_name = f\"pt_{timestamp}\"\n",
        "\n",
        "in_colab_enterprise = \"GOOGLE_CLOUD_PROJECT\" in os.environ\n",
        "if not in_colab_enterprise:\n",
        "    gc = authenticate()\n",
        "\n",
        "label_enforced = is_run_target_required(\n",
        "    [\n",
        "        OPTIMIZATION_METRIC,\n",
        "        OPTIMIZATION_METRIC_1,\n",
        "        OPTIMIZATION_METRIC_2,\n",
        "        OPTIMIZATION_METRIC_3,\n",
        "    ],\n",
        "    SOURCE_MODEL,\n",
        ")\n",
        "input_data_path = f\"{INPUT_DATA_PATH}\"\n",
        "validate_prompt_and_data(\n",
        "    \"\\n\".join([SYSTEM_INSTRUCTION, PROMPT_TEMPLATE]),\n",
        "    input_data_path,\n",
        "    PLACEHOLDER_TO_VALUE,\n",
        "    label_enforced,\n",
        ")\n",
        "\n",
        "output_path = f\"{OUTPUT_PATH}/{display_name}\"\n",
        "\n",
        "params = {\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"num_steps\": NUM_INST_OPTIMIZATION_STEPS,\n",
        "    \"prompt_template\": SYSTEM_INSTRUCTION,\n",
        "    \"demo_and_query_template\": PROMPT_TEMPLATE,\n",
        "    \"target_model\": TARGET_MODEL,\n",
        "    \"target_model_qps\": TARGET_MODEL_QPS,\n",
        "    \"target_model_location\": LOCATION,\n",
        "    \"source_model\": SOURCE_MODEL,\n",
        "    \"source_model_qps\": SOURCE_MODEL_QPS,\n",
        "    \"source_model_location\": LOCATION,\n",
        "    \"eval_model_qps\": EVAL_MODEL_QPS,\n",
        "    \"eval_model_location\": LOCATION,\n",
        "    \"optimization_mode\": OPTIMIZATION_MODE,\n",
        "    \"num_demo_set_candidates\": NUM_DEMO_OPTIMIZATION_STEPS,\n",
        "    \"demo_set_size\": NUM_DEMO_PER_PROMPT,\n",
        "    \"aggregation_type\": METRIC_AGGREGATION_TYPE,\n",
        "    \"data_limit\": 50,\n",
        "    \"optimizer_model\": OPTIMIZER_MODEL,\n",
        "    \"optimizer_model_qps\": OPTIMIZER_MODEL_QPS,\n",
        "    \"optimizer_model_location\": LOCATION,\n",
        "    \"num_template_eval_per_step\": NUM_TEMPLATES_PER_STEP,\n",
        "    \"input_data_path\": input_data_path,\n",
        "    \"output_path\": output_path,\n",
        "    \"response_mime_type\": RESPONSE_MIME_TYPE,\n",
        "    \"language\": TARGET_LANGUAGE,\n",
        "    \"placeholder_to_content\": json.loads(PLACEHOLDER_TO_VALUE),\n",
        "}\n",
        "\n",
        "if OPTIMIZATION_METRIC_1 == \"NA\":\n",
        "    params[\"eval_metrics_types\"] = [OPTIMIZATION_METRIC]\n",
        "    params[\"eval_metrics_weights\"] = [1.0]\n",
        "else:\n",
        "    metrics = []\n",
        "    weights = []\n",
        "    for metric in [OPTIMIZATION_METRIC_1, OPTIMIZATION_METRIC_2, OPTIMIZATION_METRIC_3]:\n",
        "        if metric == \"NA\":\n",
        "            break\n",
        "        metrics.append(metric)\n",
        "        weights.append(OPTIMIZATION_METRIC_1_WEIGHT)\n",
        "    params[\"eval_metrics_types\"] = metrics\n",
        "    params[\"eval_metrics_weights\"] = weights\n",
        "\n",
        "job = run_apd(params, OUTPUT_PATH, display_name)\n",
        "print(f\"Job ID: {job.name}\")\n",
        "\n",
        "progress_form = ProgressForm()\n",
        "while progress_form.monitor_progress(job, params):\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo5mcTzwSgBP"
      },
      "source": [
        "# Step 6: Inspect the Results\n",
        "You can use the following cell to inspect all the predictions made by all the\n",
        "generated templates during one or multiple VAPO runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x6HSty759jY"
      },
      "outputs": [],
      "source": [
        "RESULT_PATH = \"[GCS_PATH]\"  # @param {type:\"string\"}\n",
        "# @markdown * Specify a GCS path that contains artifacts of a single or multiple VAPO runs.\n",
        "\n",
        "results_ui = ResultsUI(RESULT_PATH)\n",
        "\n",
        "results_df_html = \"\"\"\n",
        "<style>\n",
        "  .scrollable {\n",
        "    width: 100%;\n",
        "    height: 80px;\n",
        "    overflow-y: auto;\n",
        "    overflow-x: hidden;  /* Hide horizontal scrollbar */\n",
        "  }\n",
        "  tr:nth-child(odd) {\n",
        "    background: var(--colab-highlighted-surface-color);\n",
        "  }\n",
        "  tr:nth-child(even) {\n",
        "    background-color: var(--colab-primary-surface-color);\n",
        "  }\n",
        "  th {\n",
        "    background-color: var(--colab-highlighted-surface-color);\n",
        "  }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(results_df_html))\n",
        "display(results_ui.get_container())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertex_ai_prompt_optimizer_ui.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
