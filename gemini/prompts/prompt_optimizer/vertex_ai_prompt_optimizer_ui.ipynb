{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlI1rYKa2IGx"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN8N3O43QDT5"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fvertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHyuJTFr2IGx"
   },
   "source": [
    "# Overview\n",
    "Welcome to Vertex AI Prompt Optimizer (VAPO)! This Notebook showcases VAPO, a tool that iteratively optimizes prompts to suit a target model (e.g., `gemini-1.5-pro`) using target-specific metric(s).\n",
    "\n",
    "Key Use Cases:\n",
    "\n",
    "* Prompt Optimization: Enhance the quality of an initial prompt by refining its structure and content to match the target model's optimal input characteristics.\n",
    "\n",
    "* Prompt Translation: Adapt prompts optimized for one model to work effectively with a different target model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTtKHedrO1Rx"
   },
   "source": [
    "# Step 0: Install packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-Zw72vFORz_"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py\n",
    "import vapo_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p59jd5rOp4q"
   },
   "source": [
    "# Step 1: Configure your prompt template\n",
    "Prompts consist of two key parts:\n",
    "* System Instruction (SI) Template: A fixed instruction shared across all queries for a given task.\n",
    "* Task/Context Template: A dynamic part that changes based on the task.\n",
    "\n",
    "APD enables the translation and optimization of the System Instruction Template, while the Task/Context Template remains essential for evaluating different SI templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJG1pVZO317x"
   },
   "outputs": [],
   "source": [
    "SYSTEM_INSTRUCTION = \"Answer the following question. Let's think step by step.\\n\"  # @param {type:\"string\"}\n",
    "PROMPT_TEMPLATE = \"Question: {question}\\n\\nAnswer:{target}\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y-cmg0TQP6v"
   },
   "source": [
    "# Step 2: Input your data\n",
    "To optimize the model, provide a CSV or JSONL file containing labeled validation samples\n",
    "* Focus on examples that specifically demonstrate the issues you want to address.\n",
    "* Recommendation: Use 50-100 distinct samples for reliable results. However, the tool can still be effective with as few as 5 samples.\n",
    "\n",
    "For prompt translation:\n",
    "* Consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfgi_oR6tTIB"
   },
   "outputs": [],
   "source": [
    "# @markdown **Project setup**: <br/>\n",
    "PROJECT_ID = \"[YOUR_PROJECT]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "OUTPUT_PATH = \"[OUTPUT_PATH]\"  # @param {type:\"string\"}\n",
    "# @markdown * GCS path of your bucket, e.g., gs://prompt_translation_demo, used to store all artifacts.\n",
    "INPUT_DATA_PATH = \"[INPUT_DATA_PATH]\"  # @param {type:\"string\"}\n",
    "# @markdown * Specify a GCS path for the input data, e.g., gs://prompt_translation_demo/input_data.jsonl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucebZHkHRxKH"
   },
   "source": [
    "# Step 3: Configure optimization settings\n",
    "The optimization configs are defaulted to the values that are most commonly used and which we recommend using initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2R3P8mMvK9q"
   },
   "outputs": [],
   "source": [
    "SOURCE_MODEL = \"\"  # @param [\"\", \"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
    "# @markdown * If set, it will be used to generate ground truth responses for the input examples. This is useful to migrate the prompt from a source model.\n",
    "TARGET_MODEL = \"gemini-1.5-flash-001\"  # @param [\"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\"]\n",
    "OPTIMIZATION_MODE = \"instruction_and_demo\"  # @param [\"instruction\", \"demonstration\", \"instruction_and_demo\"]\n",
    "EVAL_METRIC = \"question_answering_correctness\"  # @param [\"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO7fO0qTSNLs"
   },
   "source": [
    "# Step 4: Configure advanced optimization settings [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRHHTpaV4Xyo"
   },
   "outputs": [],
   "source": [
    "# @markdown **Instruction Optimization Configs**: <br/>\n",
    "NUM_INST_OPTIMIZATION_STEPS = 10  # @param {type:\"integer\"}\n",
    "NUM_TEMPLATES_PER_STEP = 2  # @param {type:\"integer\"}\n",
    "# @markdown * Number of prompt templates generated and evaluated at each optimization step.\n",
    "\n",
    "# @markdown **Demonstration Optimization Configs**: <br/>\n",
    "NUM_DEMO_OPTIMIZATION_STEPS = 10  # @param {type:\"integer\"}\n",
    "NUM_DEMO_PER_PROMPT = 3  # @param {type:\"integer\"}\n",
    "# @markdown * Number of the demonstrations to include in each prompt.\n",
    "\n",
    "# @markdown **Model Configs**: <br/>\n",
    "TARGET_MODEL_QPS = 3.0  # @param {type:\"number\"}\n",
    "SOURCE_MODEL_QPS = 3.0  # @param {type:\"number\"}\n",
    "EVAL_QPS = 3.0  # @param {type:\"number\"}\n",
    "# @markdown * The QPS for calling the eval model, which is currently gemini-1.5-pro-001.\n",
    "\n",
    "# @markdown **Multi-metric Configs**: <br/>\n",
    "# @markdown Use this section only if you need more than one metric for optimization. This will override the metric you picked above.\n",
    "EVAL_METRIC_1 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
    "EVAL_METRIC_1_WEIGHT = 0.0  # @param {type:\"number\"}\n",
    "EVAL_METRIC_2 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
    "EVAL_METRIC_2_WEIGHT = 0.0  # @param {type:\"number\"}\n",
    "EVAL_METRIC_3 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
    "EVAL_METRIC_3_WEIGHT = 0.0  # @param {type:\"number\"}\n",
    "METRIC_AGGREGATION_TYPE = \"weighted_sum\"  # @param [\"weighted_sum\", \"weighted_average\"]\n",
    "\n",
    "# @markdown **Misc Configs**: <br/>\n",
    "PLACEHOLDER_TO_VALUE = \"{}\"  # @param\n",
    "# @markdown * This variable is used for long prompt optimization to not optimize parts of prompt identified by placeholders. It provides a mapping from the placeholder variables to their content. See link for details.\n",
    "RESPONSE_MIME_TYPE = \"application/json\"  # @param [\"text/plain\", \"application/json\"]\n",
    "# @markdown * This variable determines the format of the output for the target model. See link for details.\n",
    "TARGET_LANGUAGE = \"English\"  # @param [\"English\", \"French\", \"German\", \"Hebrew\", \"Hindi\", \"Japanese\", \"Korean\", \"Portuguese\", \"Simplified Chinese\", \"Spanish\", \"Traditional Chinese\"]\n",
    "# @markdown * The language of the system instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7Mgb0EHSSFk"
   },
   "source": [
    "# Step 5: Run Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8NvNLTfxPTf"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from google.colab import auth\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "display_name = f\"pt_{timestamp}\"\n",
    "\n",
    "in_colab_enterprise = \"GOOGLE_CLOUD_PROJECT\" in os.environ\n",
    "if not in_colab_enterprise:\n",
    "    auth.authenticate_user()\n",
    "\n",
    "label_enforced = vapo_lib.is_run_target_required(\n",
    "    [\n",
    "        EVAL_METRIC,\n",
    "        EVAL_METRIC_1,\n",
    "        EVAL_METRIC_2,\n",
    "        EVAL_METRIC_3,\n",
    "    ],\n",
    "    SOURCE_MODEL,\n",
    ")\n",
    "input_data_path = f\"{INPUT_DATA_PATH}\"\n",
    "vapo_lib.validate_prompt_and_data(\n",
    "    \"\\n\".join([SYSTEM_INSTRUCTION, PROMPT_TEMPLATE]),\n",
    "    input_data_path,\n",
    "    PLACEHOLDER_TO_VALUE,\n",
    "    label_enforced,\n",
    ")\n",
    "\n",
    "output_path = f\"{OUTPUT_PATH}/{display_name}\"\n",
    "\n",
    "params = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"num_steps\": NUM_INST_OPTIMIZATION_STEPS,\n",
    "    \"system_instruction\": SYSTEM_INSTRUCTION,\n",
    "    \"prompt_template\": PROMPT_TEMPLATE,\n",
    "    \"target_model\": TARGET_MODEL,\n",
    "    \"target_model_qps\": TARGET_MODEL_QPS,\n",
    "    \"target_model_location\": LOCATION,\n",
    "    \"source_model\": SOURCE_MODEL,\n",
    "    \"source_model_qps\": SOURCE_MODEL_QPS,\n",
    "    \"source_model_location\": LOCATION,\n",
    "    \"eval_qps\": EVAL_QPS,\n",
    "    \"eval_model_location\": LOCATION,\n",
    "    \"optimization_mode\": OPTIMIZATION_MODE,\n",
    "    \"num_demo_set_candidates\": NUM_DEMO_OPTIMIZATION_STEPS,\n",
    "    \"demo_set_size\": NUM_DEMO_PER_PROMPT,\n",
    "    \"aggregation_type\": METRIC_AGGREGATION_TYPE,\n",
    "    \"data_limit\": 50,\n",
    "    \"num_template_eval_per_step\": NUM_TEMPLATES_PER_STEP,\n",
    "    \"input_data_path\": input_data_path,\n",
    "    \"output_path\": output_path,\n",
    "    \"response_mime_type\": RESPONSE_MIME_TYPE,\n",
    "    \"language\": TARGET_LANGUAGE,\n",
    "    \"placeholder_to_content\": json.loads(PLACEHOLDER_TO_VALUE),\n",
    "}\n",
    "\n",
    "if EVAL_METRIC_1 == \"NA\":\n",
    "    params[\"eval_metrics_types\"] = [EVAL_METRIC]\n",
    "    params[\"eval_metrics_weights\"] = [1.0]\n",
    "else:\n",
    "    metrics = []\n",
    "    weights = []\n",
    "    for metric in [EVAL_METRIC_1, EVAL_METRIC_2, EVAL_METRIC_3]:\n",
    "        if metric == \"NA\":\n",
    "            break\n",
    "        metrics.append(metric)\n",
    "        weights.append(EVAL_METRIC_1_WEIGHT)\n",
    "    params[\"eval_metrics_types\"] = metrics\n",
    "    params[\"eval_metrics_weights\"] = weights\n",
    "\n",
    "job = vapo_lib.run_apd(params, OUTPUT_PATH, display_name)\n",
    "print(f\"Job ID: {job.name}\")\n",
    "\n",
    "progress_form = vapo_lib.ProgressForm(params)\n",
    "while progress_form.monitor_progress(job):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo5mcTzwSgBP"
   },
   "source": [
    "# Step 6: Inspect the Results\n",
    "You can use the following cell to inspect all the predictions made by all the\n",
    "generated templates during one or multiple VAPO runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1x6HSty759jY"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "RESULT_PATH = \"[OUTPUT_PATH]\"  # @param {type:\"string\"}\n",
    "# @markdown * Specify a GCS path that contains artifacts of a single or multiple VAPO runs.\n",
    "\n",
    "results_ui = vapo_lib.ResultsUI(RESULT_PATH)\n",
    "\n",
    "results_df_html = \"\"\"\n",
    "<style>\n",
    "  .scrollable {\n",
    "    width: 100%;\n",
    "    height: 80px;\n",
    "    overflow-y: auto;\n",
    "    overflow-x: hidden;  /* Hide horizontal scrollbar */\n",
    "  }\n",
    "  tr:nth-child(odd) {\n",
    "    background: var(--colab-highlighted-surface-color);\n",
    "  }\n",
    "  tr:nth-child(even) {\n",
    "    background-color: var(--colab-primary-surface-color);\n",
    "  }\n",
    "  th {\n",
    "    background-color: var(--colab-highlighted-surface-color);\n",
    "  }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(results_df_html))\n",
    "display(results_ui.get_container())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "vertex_ai_prompt_optimizer_ui.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
