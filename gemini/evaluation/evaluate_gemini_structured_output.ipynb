{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Evaluate Gemini Structured Output\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2F%2Fgemini%2Fevaluation%2Fevaluate_gemini_structured_output.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main//gemini/evaluation/evaluate_gemini_structured_output.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Steve Phillips](https://github.com/stevie-p) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook uses the [*GenAI Evaluation Service*](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/evaluation) to evaluate and compare the performance of Gemini models for an extraction task.\n",
        "\n",
        "The task is to accurately extract information from a scanned, handwritten order form for \"Acme Corporation\".\n",
        "\n",
        "Within this notebook, we:\n",
        "\n",
        "* Use Gemini models with [*structured output*](https://ai.google.dev/gemini-api/docs/structured-output) to ensure well-structured JSON output\n",
        "* Extract the data using Gemini models\n",
        "* Use the *GenAI Evaluation service* to run the evaluation experiments with a custom *accuracy* metric\n",
        "\n",
        "The [models](https://ai.google.dev/gemini-api/docs/models) under test are:\n",
        "* Gemini 2.0 Flash\n",
        "* Gemini 2.5 Flash\n",
        "* Gemini 2.5 Pro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai jsonschema IPython==7.34.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "795uBvHqqy4V"
      },
      "source": [
        "Restart the runtime to use the newly installed packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Create your own project and insert the project ID here ---->\n",
        "\n",
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "EXPERIMENT_NAME = \"eval-gemini-structured\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from vertexai.evaluation import EvalTask, CustomMetric, notebook_utils\n",
        "from google import genai\n",
        "from google.cloud import storage  # type: ignore\n",
        "from google.genai.types import (\n",
        "    GenerateContentConfig,\n",
        "    Part,\n",
        ")\n",
        "import json\n",
        "import io\n",
        "from jsonschema import validate\n",
        "from IPython.display import display, Image, Markdown\n",
        "from datetime import datetime\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Q3T_1ox99_"
      },
      "source": [
        "## View the images\n",
        "\n",
        "Let's have a look at the images we want to extract data from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7u9dNhytp5s"
      },
      "outputs": [],
      "source": [
        "images = [\n",
        "    {\n",
        "        \"image_uri\": \"gs://eval-extraction-examples/Acme Order Form.jpg\",\n",
        "        \"image_type\": \"image/jpeg\",\n",
        "        \"image_name\": \"Acme Order Form.jpg\",\n",
        "        \"reference\": {  # The Ground Truth\n",
        "          \"order_number\": \"98-X42-77A\",\n",
        "          \"order_date\": \"2025-09-01\",\n",
        "          \"customer_name\": \"WILE E. COYOTE (ESQ., PH.D, S.G.)\",\n",
        "          \"customer_address\": \"HIGH MESA, CORNER OF X-MARK AND DETONATION CANYON, ANVIL FALLS, AZ\",\n",
        "          \"line_items\": [\n",
        "            {\n",
        "                \"item_description\": \"Jet Propelled Unicycle\",\n",
        "                \"quantity\": 1,\n",
        "                \"unit_price\": 99.99,\n",
        "                \"delivery_option\": \"Next Day\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Instant Hole Kit\",\n",
        "                \"quantity\": 3,\n",
        "                \"unit_price\": 45.00,\n",
        "                \"delivery_option\": \"Standard\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"TNT High Explosives x24\",\n",
        "                \"quantity\": 1,\n",
        "                \"unit_price\": 120.00,\n",
        "                \"delivery_option\": \"Fast\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Super Magnet (XL)\",\n",
        "                \"quantity\": 1,\n",
        "                \"unit_price\": 150.00,\n",
        "                \"delivery_option\": \"Fast\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Rocket-Powered Roller skates\",\n",
        "                \"quantity\": 2,\n",
        "                \"unit_price\": 79.99,\n",
        "                \"delivery_option\": \"Next Day\"\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"image_uri\": \"gs://eval-extraction-examples/EF0004.jpg\",\n",
        "        \"image_type\": \"image/jpeg\",\n",
        "        \"image_name\": \"EF0004.jpg\",\n",
        "        \"reference\": {  # The Ground Truth\n",
        "          \"order_number\": \"EF0004\",\n",
        "          \"order_date\": \"2025-10-26\",\n",
        "          \"customer_name\": \"Elmer J. Fudd\",\n",
        "          \"customer_address\": \"Happy Hunter's Hollow, Looney Tune Forest, CA\",\n",
        "          \"line_items\": [\n",
        "            {\n",
        "                \"item_description\": \"Silent Sneak Shoes\",\n",
        "                \"quantity\": 1,\n",
        "                \"unit_price\": 35.99,\n",
        "                \"delivery_option\": \"Standard\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Invisible Rabbit Trap\",\n",
        "                \"quantity\": 2,\n",
        "                \"unit_price\": 75.00,\n",
        "                \"delivery_option\": \"Standard\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Giant Butterfly Net\",\n",
        "                \"quantity\": 1,\n",
        "                \"unit_price\": 49.50,\n",
        "                \"delivery_option\": \"Fast\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Instant Camouflage Kit\",\n",
        "                \"quantity\": 3,\n",
        "                \"unit_price\": 65.00,\n",
        "                \"delivery_option\": \"Next Day\"\n",
        "            },\n",
        "            {\n",
        "                \"item_description\": \"Repellent Spray\",\n",
        "                \"quantity\": 4,\n",
        "                \"unit_price\": 29.99,\n",
        "                \"delivery_option\": \"Next Day\"\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCjahTXbx9c2"
      },
      "outputs": [],
      "source": [
        "# Extract bucket name and blob path from the GCS URI\n",
        "from google.cloud import storage\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "for image_info in images:\n",
        "    image_uri = image_info['image_uri']\n",
        "    bucket_name = image_uri.split('gs://')[1].split('/')[0]\n",
        "    blob_path = '/'.join(image_uri.split('gs://')[1].split('/')[1:])\n",
        "\n",
        "    # Download the image to a temporary file\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_path)\n",
        "    temp_image_path = f'/tmp/{blob_path.replace(\"/\", \"_\")}' # Using /tmp as it's a common writable directory in Colab and creating unique filenames\n",
        "    blob.download_to_filename(temp_image_path)\n",
        "\n",
        "    # Display the image with a maximum height\n",
        "    print(image_info['image_name'])\n",
        "    display(Image(filename=temp_image_path, height=800))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gOpub_1GYB"
      },
      "source": [
        "These are mock order forms for *Acme Corporation*, for customers to order various products, and select a delivery option for each; either \"Standard\", \"Fast\" or \"Next Day\".\n",
        "\n",
        "We will use this form to evaluate the performance of Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWKVr7mmjxy0"
      },
      "source": [
        "## Extract the data using Gemini\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0UJShmGMv3h"
      },
      "source": [
        "### Select the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FWWciVnjxy0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the client and which models to use\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Define which models to compare\n",
        "\n",
        "models = [\n",
        "    # Gemini 2.0 family\n",
        "    \"gemini-2.0-flash\",\n",
        "\n",
        "    # Gemini 2.5 family\n",
        "    \"gemini-2.5-flash\",\n",
        "    \"gemini-2.5-pro\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQ4jCWNM3lj"
      },
      "source": [
        "### Define the prompt and schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQScA9Wujxy0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the prompt and the structured output schema\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Analyze the attached scanned form and extract the information in the table in accordance with the schema.\n",
        "\n",
        "    Provide the output in a clean JSON format.\n",
        "\n",
        "    If any date field is formatted ambiguously, assume the dates are in dd/mm/yyyy format.\n",
        "\n",
        "    If a field is blank, illegible, or cannot be found, return null for its value.\n",
        "\n",
        "    If there are blank rows, do not include them in the output.\n",
        "\n",
        "    If there is no image attached, return null for all fields.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Use structured output to ensure well formatted and consistent JSON output\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"order_number\": {\n",
        "            \"type\": \"string\"\n",
        "        },\n",
        "        \"order_date\": {\n",
        "            \"type\": \"string\",\n",
        "            \"format\": \"date\" # Note: Enforces a full date output in the RFC 3339 format (\"YYYY-MM-DD\")\n",
        "        },\n",
        "        \"customer_name\": {\n",
        "            \"type\": \"string\"\n",
        "        },\n",
        "        \"customer_address\": {\n",
        "            \"type\": \"string\"\n",
        "        },\n",
        "        \"line_items\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"item_description\": {\n",
        "                        \"type\": \"string\"\n",
        "                    },\n",
        "                    \"quantity\": {\n",
        "                        \"type\": \"integer\"\n",
        "                    },\n",
        "                    \"unit_price\": {\n",
        "                        \"type\": \"number\"\n",
        "                    },\n",
        "                    \"delivery_option\": { # Note: We do not tell Gemini how to interpret the checkboxes as \"Standard\", \"Fast\" or \"Next Day\"\n",
        "                        \"type\": \"string\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "generate_content_config = GenerateContentConfig(\n",
        "    response_mime_type=\"application/json\",\n",
        "    response_schema=schema,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F57ILmc0M8ZE"
      },
      "source": [
        "### Run the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCqG3xE_jxy0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run the prompt for each model in `models` and each image in `images`, storing the output in `gemini_response`\n",
        "\n",
        "gemini_response = {}\n",
        "run_id = notebook_utils.generate_uuid(8)\n",
        "\n",
        "\n",
        "for image_info in images:\n",
        "    image = Part.from_uri(file_uri=image_info['image_uri'], mime_type=image_info['image_type'])\n",
        "    image_name = image_info['image_name']\n",
        "\n",
        "    gemini_response[image_name] = {}\n",
        "\n",
        "    for model in models:\n",
        "        run_name = f\"{run_id}-{model}-{image_name}\"\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=[\n",
        "                prompt,\n",
        "                image\n",
        "            ],\n",
        "            config=generate_content_config\n",
        "        )\n",
        "\n",
        "        # Parse the response text as JSON, and then pretty-print\n",
        "        try:\n",
        "            response_json_data = json.loads(response.text)\n",
        "            response_json_string = json.dumps(response_json_data, indent=4)\n",
        "        except json.JSONDecodeError:\n",
        "            # Handle cases where the response text is not valid JSON\n",
        "            response_json_string = response.text\n",
        "            print(f\"Warning: Response for {run_name} is not valid JSON.\")\n",
        "\n",
        "        print(f\"----------------------------------\")\n",
        "        print(f\"{run_name}: {response_json_string}\")\n",
        "        gemini_response[image_name][model] = response_json_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUNVUz6nNHQE"
      },
      "source": [
        "## Perform the Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Prepare the evaluation dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajaEV5i6auy"
      },
      "source": [
        "Now we have the outputs from the Gemini models we can run the evaulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUGFuYawEJYX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create the Evaluation Dataset\n",
        "\n",
        "eval_dataset_rows = []\n",
        "for image_info in images:\n",
        "    image_name = image_info['image_name']\n",
        "    image_uri = image_info['image_uri']\n",
        "    image_type = image_info['image_type']\n",
        "    reference_str = json.dumps(image_info['reference'], indent=4) # Convert the reference (ground truth) to pretty-printed JSON\n",
        "\n",
        "    if image_name in gemini_response:\n",
        "        models_data = gemini_response[image_name]\n",
        "        for model_name, response_text in models_data.items():\n",
        "            eval_dataset_rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"prompt\": prompt, # Assuming the same prompt is used for all Gemini calls\n",
        "                \"image\": image_name,\n",
        "                \"reference\": reference_str,\n",
        "                \"response\": response_text\n",
        "            })\n",
        "\n",
        "eval_dataset = pd.DataFrame(eval_dataset_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrTqbS6NAKHk"
      },
      "source": [
        "This evaluation data set now contains the reference (ground truth) and response for each combination of model and image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ByFZhujxy1",
        "tags": []
      },
      "source": [
        "### Define custom metrics for JSON schema validation and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2v2WdYbjxy1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define a custom evaluation metric to assess whether the response complies with the schema\n",
        "\n",
        "def is_valid_schema(instance: dict[str,str]) -> dict[str, float]:\n",
        "    \"\"\"Return 1 if the response complies with the schema, 0 if not\"\"\"\n",
        "\n",
        "    response = instance[\"response\"]\n",
        "\n",
        "    try:\n",
        "        validate(instance=json.loads(response), schema=schema)\n",
        "    except Exception:\n",
        "        return {\n",
        "            \"valid_schema\": 0\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"valid_schema\": 1\n",
        "    }\n",
        "\n",
        "valid_schema = CustomMetric(name=\"valid_schema\", metric_function=is_valid_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztImgetQjxy1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define a custom evaluation metric to assess the accuracy of the response compared with the reference (ground truth)\n",
        "\n",
        "\n",
        "\n",
        "def string_similarity(str1, str2):\n",
        "    \"\"\"Calculates a simple character-based similarity score between two strings.\"\"\"\n",
        "    if not str1 and not str2:\n",
        "        return 1.0 # Both empty strings are a perfect match\n",
        "    if not str1 or not str2:\n",
        "        return 0.0 # One is empty, the other is not\n",
        "\n",
        "    # Normalize strings for case-insensitive and whitespace-agnostic comparison\n",
        "    norm_str1 = str1.strip().upper().replace('\\n', ' ')\n",
        "    norm_str2 = str2.strip().upper().replace('\\n', ' ')\n",
        "\n",
        "    # Simple character matching\n",
        "    match_count = sum(c1 == c2 for c1, c2 in zip(norm_str1, norm_str2))\n",
        "    max_len = max(len(norm_str1), len(norm_str2))\n",
        "\n",
        "    return match_count / max_len if max_len > 0 else 1.0\n",
        "\n",
        "\n",
        "def compare_values_recursive(ref_value, resp_value):\n",
        "    \"\"\"Compares two values recursively, handling nested structures and basic types. Returns a tuple: (total_score, comparison_count).\"\"\"\n",
        "    # Handle None values\n",
        "    if ref_value is None and resp_value is None:\n",
        "        return (1.0, 1) # Treat two None values as a perfect match (score 1.0, 1 comparison)\n",
        "    if ref_value is None or resp_value is None:\n",
        "        return (0.0, 1) # One is None, the other is not (score 0.0, 1 comparison)\n",
        "\n",
        "    if isinstance(ref_value, dict) and isinstance(resp_value, dict):\n",
        "        # If both are dictionaries, compare recursively and aggregate scores\n",
        "        return compare_dicts_recursive(ref_dict=ref_value, resp_dict=resp_value)\n",
        "    elif isinstance(ref_value, list) and isinstance(resp_value, list):\n",
        "        # If both are lists, compare recursively and aggregate scores (order doesn't matter here)\n",
        "        return compare_lists_recursive(ref_list=ref_value, resp_list=resp_value)\n",
        "    elif isinstance(ref_value, str) and isinstance(resp_value, str):\n",
        "        # Attempt to parse as date first, return 1.0 for match, 0.0 for mismatch\n",
        "        try:\n",
        "            ref_date = datetime.strptime(ref_value, \"%Y-%m-%d\") # Assuming reference is always YYYY-MM-DD\n",
        "            # Attempt to parse response date in common formats\n",
        "            resp_date = None\n",
        "            try:\n",
        "                resp_date = datetime.strptime(resp_value, \"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                pass # Not a recognized date format\n",
        "\n",
        "            if resp_date and ref_date == resp_date:\n",
        "                return (1.0, 1) # Dates match\n",
        "            else:\n",
        "                 return (0.0, 1) # Dates don't match or response date not recognized\n",
        "\n",
        "        except ValueError:\n",
        "            # If not a date, perform character-based string similarity\n",
        "            return (string_similarity(ref_value, resp_value), 1)\n",
        "\n",
        "    elif isinstance(ref_value, (int, float)) and isinstance(resp_value, (int, float)):\n",
        "        # Numeric comparison with tolerance for floating point differences\n",
        "        return (1.0, 1) if abs(ref_value - resp_value) < 1e-9 else (0.0, 1) # Return 1.0 for match, 0.0 for mismatch, 1 comparison\n",
        "    else:\n",
        "        # Direct comparison for other types (booleans, etc.), return 1.0 for match, 0.0 for mismatch\n",
        "        return (1.0, 1) if ref_value == resp_value else (0.0, 1)\n",
        "\n",
        "def compare_dicts_recursive(ref_dict, resp_dict):\n",
        "    \"\"\"Compares two dictionaries recursively, allowing extra keys in response. Returns a tuple: (total_score, comparison_count).\"\"\"\n",
        "    total_score = 0\n",
        "    comparison_count = 0\n",
        "\n",
        "    # Compare keys in reference\n",
        "    for key in ref_dict.keys():\n",
        "        if key in resp_dict:\n",
        "            score, count = compare_values_recursive(ref_dict[key], resp_dict[key])\n",
        "            total_score += score\n",
        "            comparison_count += count\n",
        "        else:\n",
        "            # Reference key missing in response - treat as mismatch for all nested elements\n",
        "            score, count = compare_values_recursive(ref_dict[key], None) # Compare against None to count all elements\n",
        "            total_score += score\n",
        "            comparison_count += count\n",
        "\n",
        "\n",
        "    # Keys only in response are ignored for accuracy based on reference\n",
        "    # They are not added to comparison_count as we are measuring accuracy against the reference structure\n",
        "\n",
        "\n",
        "    return (total_score, comparison_count)\n",
        "\n",
        "\n",
        "def compare_lists_recursive(ref_list, resp_list):\n",
        "    \"\"\"Compares two lists recursively, allowing elements to be in any order. Returns a tuple: (total_score, comparison_count).\"\"\"\n",
        "    if not ref_list and not resp_list:\n",
        "        return (1.0, 1) # Both empty lists are a perfect match\n",
        "\n",
        "    # If list lengths differ, penalize\n",
        "    if len(ref_list) != len(resp_list):\n",
        "         # Simple penalty: every extra/missing item is a full mismatch\n",
        "         # Calculate potential maximum comparisons based on the larger list\n",
        "         max_possible_comparisons = max(len(ref_list), len(resp_list)) * get_nested_element_count(ref_list if len(ref_list) > len(resp_list) else resp_list)\n",
        "         return (0.0, max_possible_comparisons if max_possible_comparisons > 0 else 1)\n",
        "\n",
        "\n",
        "    total_score = 0\n",
        "    comparison_count = 0\n",
        "    resp_copy = [(item, False) for item in resp_list] # Store item and a flag indicating if it's matched\n",
        "\n",
        "    for i, ref_item in enumerate(ref_list):\n",
        "        best_match_score = -1 # Initialize with a score less than any possible similarity\n",
        "        best_match_index = -1\n",
        "        best_match_count_for_item = 0 # Initialize here\n",
        "\n",
        "        for j, (resp_item, matched) in enumerate(resp_copy):\n",
        "            if not matched:\n",
        "                # Use the compare_values_recursive function to get a similarity score for the items\n",
        "                try:\n",
        "                    score, count = compare_values_recursive(ref_item, resp_item) # Use the scoring version\n",
        "                except Exception as e:\n",
        "                    # Fallback if scoring fails, which shouldn't happen with compare_values_recursive\n",
        "                    print(f\"Error during recursive comparison: {e}\", file=sys.stderr)\n",
        "                    score = 0.0 # Treat as no match if recursive comparison fails\n",
        "                    count = get_nested_element_count(ref_item) # Count elements for penalty\n",
        "\n",
        "\n",
        "                if score > best_match_score:\n",
        "                    best_match_score = score\n",
        "                    best_match_index = j\n",
        "                    best_match_count_for_item = count\n",
        "\n",
        "\n",
        "        if best_match_index != -1:\n",
        "            total_score += best_match_score\n",
        "            comparison_count += best_match_count_for_item\n",
        "            resp_copy[best_match_index] = (resp_copy[best_match_index][0], True) # Mark as matched\n",
        "        else:\n",
        "             # If no match is found, report the missing reference item\n",
        "             # Compare the reference item against None to count its nested elements as mismatches\n",
        "             score, count = compare_values_recursive(ref_item, None)\n",
        "             total_score += score # This should be 0.0\n",
        "             comparison_count += count # Count all nested elements as mismatches\n",
        "\n",
        "\n",
        "    # Ensure all items in the response list are accounted for in comparison_count, even if unmatched\n",
        "    # This prevents inflated accuracy when response list has extra items\n",
        "    for resp_item, matched in resp_copy:\n",
        "        if not matched:\n",
        "             # Compare unmatched response item against None to count its nested elements\n",
        "             score, count = compare_values_recursive(None, resp_item)\n",
        "             # Do not add to total_score as these are extra items not in reference\n",
        "             comparison_count += count\n",
        "\n",
        "\n",
        "    return (total_score, comparison_count)\n",
        "\n",
        "def get_nested_element_count(data):\n",
        "    \"\"\"Recursively counts the number of comparable elements (non-dict/list) in the structure.\"\"\"\n",
        "    count = 0\n",
        "    if isinstance(data, dict):\n",
        "        for value in data.values():\n",
        "            count += get_nested_element_count(value)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            count += get_nested_element_count(item)\n",
        "    else:\n",
        "        count = 1 # Count this element\n",
        "    return count\n",
        "\n",
        "\n",
        "def calculate_accuracy(instance: dict[str,str]) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    This calculates accuracy by recursively comparing the JSON structures,\n",
        "    allowing for elements to be in any order in lists and ignoring extra fields in the response.\n",
        "    Returns an average similarity score.\n",
        "    \"\"\"\n",
        "\n",
        "    ref_json_string = instance[\"reference\"]\n",
        "    resp_json_string = instance[\"response\"]\n",
        "\n",
        "    try:\n",
        "        reference_data = json.loads(ref_json_string)\n",
        "        response_data = json.loads(resp_json_string)\n",
        "    except json.JSONDecodeError:\n",
        "        # If JSON is invalid or parsing fails, return 0 accuracy\n",
        "        return {\"accuracy\": 0.0}\n",
        "\n",
        "    # Perform a recursive comparison\n",
        "    total_score, comparison_count = compare_values_recursive(reference_data, response_data)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    overall_accuracy = total_score / comparison_count if comparison_count > 0 else 1.0\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": overall_accuracy\n",
        "    }\n",
        "\n",
        "\n",
        "accuracy = CustomMetric(name=\"accuracy\", metric_function=calculate_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXdy-2UVjxy1",
        "tags": []
      },
      "source": [
        "### Define EvalTask & Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRoVmM2gjxy1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the evaluation task\n",
        "\n",
        "extraction_eval_task = EvalTask(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=[\n",
        "        \"exact_match\", # Exact match will only be 1 if the response is perfectly accurate, with no allowance for inconsistent JSON formatting. Hence, the custom `accuracy` metric is the better metric.\n",
        "        valid_schema,\n",
        "        accuracy\n",
        "    ],\n",
        "    experiment=EXPERIMENT_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj69SXdqNdLC"
      },
      "source": [
        "### Run the Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6KhR-uGjxy1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the experiment & experiment run\n",
        "\n",
        "experiment_run_name = f\"eval-{run_id}\"\n",
        "\n",
        "eval_result = extraction_eval_task.evaluate(\n",
        "    # experiment_run_name = experiment_run_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXmjQ4vkNikd"
      },
      "source": [
        "### Display the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQvVqjfrjxy1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_eval_result(eval_result=eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQMjObguZA-I"
      },
      "source": [
        "## Analyse the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX-N1yypNt1H"
      },
      "source": [
        "### Do a field-wise comparison\n",
        "\n",
        "It is helpful to do a deeper comparison to investigate where each Gemini model is not extracting the information accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7fHoNB4FmPO"
      },
      "outputs": [],
      "source": [
        "def deep_compare_and_print(ref, resp, path=\"\", output_buffer=None):\n",
        "    \"\"\"Recursively compares and prints differences to the provided buffer.\"\"\"\n",
        "    if output_buffer is None:\n",
        "      output_buffer = sys.stdout # Default to stdout if no buffer is provided\n",
        "\n",
        "    if isinstance(ref, dict) and isinstance(resp, dict):\n",
        "        all_keys = set(ref.keys()).union(set(resp.keys()))\n",
        "        for key in all_keys:\n",
        "            new_path = f\"{path}.{key}\" if path else key\n",
        "            if key in ref and key in resp:\n",
        "                deep_compare_and_print(ref[key], resp[key], new_path, output_buffer)\n",
        "            elif key in ref:\n",
        "                print(f\"  Mismatch at {new_path}: Reference has value '{ref[key]}', Response is missing key.\", file=output_buffer)\n",
        "            else: # key in resp\n",
        "                 print(f\"  Mismatch at {new_path}: Response has value '{resp[key]}', Reference is missing key.\", file=output_buffer)\n",
        "\n",
        "    elif isinstance(ref, list) and isinstance(resp, list):\n",
        "        # Compare lists by trying to find the best match for each reference item in the response list\n",
        "        ref_copy = list(ref)\n",
        "        resp_copy = [(item, False) for item in resp] # Store item and a flag indicating if it's matched\n",
        "\n",
        "        for i, ref_item in enumerate(ref_copy):\n",
        "            new_path = f\"{path}[{i}]\"\n",
        "            best_match_score = -1 # Initialize with a score less than any possible similarity\n",
        "            best_match_index = -1\n",
        "\n",
        "            for j, (resp_item, matched) in enumerate(resp_copy):\n",
        "                if not matched:\n",
        "                    # Use the compare_values_recursive function to get a similarity score for the items\n",
        "                    try:\n",
        "                        score, _ = compare_values_recursive(ref_item, resp_item) # Use the scoring version\n",
        "                    except Exception as e:\n",
        "                        # Fallback if scoring fails, which shouldn't happen with compare_values_recursive\n",
        "                        print(f\"Error during recursive comparison: {e}\", file=sys.stderr)\n",
        "                        score = 0.0 # Treat as no match if recursive comparison fails\n",
        "\n",
        "\n",
        "                    if score > best_match_score:\n",
        "                        best_match_score = score\n",
        "                        best_match_index = j\n",
        "\n",
        "\n",
        "            if best_match_index != -1:\n",
        "                # If a match is found, recursively compare the matched items\n",
        "                deep_compare_and_print(ref_item, resp_copy[best_match_index][0], new_path, output_buffer)\n",
        "                resp_copy[best_match_index] = (resp_copy[best_match_index][0], True) # Mark as matched\n",
        "            else:\n",
        "                 # If no match is found, report the missing reference item\n",
        "                 print(f\"  Mismatch in list at {new_path}: Reference item '{ref_item}' has no close match in response.\", file=output_buffer)\n",
        "\n",
        "\n",
        "        # Report any extra items in the response list\n",
        "        for j, (resp_item, matched) in enumerate(resp_copy):\n",
        "             if not matched:\n",
        "                 print(f\"  Mismatch in list at {path}[extra_item_{j}]: Response has extra item '{resp_item}'.\", file=output_buffer)\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Use the compare_values_recursive from the other cell for the actual comparison\n",
        "        # If not a perfect match (score < 1.0), print the mismatch\n",
        "        try:\n",
        "            score, _ = compare_values_recursive(ref, resp) # Use the scoring version\n",
        "            if score < 1.0:\n",
        "                 print(f\"  Mismatch at {path}: Reference='{ref}', Response='{resp}' (Similarity: {score:.2f})\", file=output_buffer)\n",
        "        except Exception as e:\n",
        "            # Fallback to boolean comparison if scoring fails\n",
        "            print(f\"Error during recursive comparison: {e}\", file=sys.stderr)\n",
        "            if ref != resp:\n",
        "                 print(f\"  Mismatch at {path}: Reference='{ref}', Response='{resp}' (Direct equality check)\", file=output_buffer)\n",
        "\n",
        "\n",
        "\n",
        "# Create a string buffer to capture the output\n",
        "comparison_output_buffer = io.StringIO()\n",
        "\n",
        "print(\"Deep comparison for all models and images:\", file=comparison_output_buffer)\n",
        "\n",
        "for index, row in eval_dataset.iterrows():\n",
        "    model_name = row['model']\n",
        "    image_name = row['image']\n",
        "    reference_string = row['reference']\n",
        "    response_string = row['response']\n",
        "\n",
        "    print(f\"\\n--- Comparing {model_name} on {image_name} ---\", file=comparison_output_buffer)\n",
        "\n",
        "    try:\n",
        "        reference_data = json.loads(reference_string)\n",
        "        response_data = json.loads(response_string)\n",
        "\n",
        "        deep_compare_and_print(reference_data, response_data, output_buffer=comparison_output_buffer)\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Could not decode JSON for comparison: {e}\", file=comparison_output_buffer)\n",
        "\n",
        "print(\"\\n--- End of Deep Comparison ---\", file=comparison_output_buffer)\n",
        "\n",
        "# Get the captured output from the buffer and store it in a variable\n",
        "deep_comparison_output = comparison_output_buffer.getvalue()\n",
        "\n",
        "# You can now use the 'deep_comparison_output' variable as needed.\n",
        "# For example, you could print it or pass it to another function.\n",
        "print(deep_comparison_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Lj96MvOqW4"
      },
      "source": [
        "### Use Gemini to analyse the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS2LVREz1zOF"
      },
      "outputs": [],
      "source": [
        "# Let's get Gemini to analyse the results.\n",
        "\n",
        "# prompt: Write code that calls Gemini 2.5 Flash to summarise and analyze the output of this experiment. What conclusions can be drawn?\n",
        "\n",
        "# Prepare the prompt for Gemini 2.5 Flash to summarize and analyze the results\n",
        "summary_prompt = \"\"\"\n",
        "Analyze the following experiment results comparing Gemini models for extracted data from a scanned form.\n",
        "The results include a summary table with overall metrics and row-based metrics, as well as a detailed field-wise comparison highlighting mismatches between the extracted data and the reference (ground truth).\n",
        "\n",
        "Summarize the performance of each model based on the metrics provided (valid_schema, accuracy) from the summary table.\n",
        "Analyze the detailed field-wise comparison to understand the *types* of errors and mismatches occurring for each model.\n",
        "Identify which models performed best and worst for each metric and based on the detailed error analysis.\n",
        "Draw conclusions about the strengths and weaknesses of Gemini models for this specific tabular data extraction task, considering both the overall accuracy and the nature of the errors.\n",
        "Consider the different versions of Gemini and how their performance varies.\n",
        "Provide a clear and concise summary of the overall results, followed by key conclusions supported by observations from the detailed comparison.\n",
        "\n",
        "Experiment Results Summary Table:\n",
        "\"\"\"\n",
        "\n",
        "# Convert the evaluation results summary and row-based metrics to a string format\n",
        "# Assuming eval_result has a structure that can be converted to a readable string\n",
        "try:\n",
        "    # This will likely involve converting the DataFrames within eval_result to string\n",
        "    eval_result_string = str(eval_result)\n",
        "except Exception as e:\n",
        "    eval_result_string = f\"Could not convert evaluation results to string: {e}\"\n",
        "    print(eval_result_string)\n",
        "\n",
        "\n",
        "# Concatenate the prompt, summary table results, and detailed comparison output\n",
        "full_prompt = summary_prompt + eval_result_string + \"\\n\\nDetailed Field-wise Comparison:\\n\" + deep_comparison_output\n",
        "\n",
        "# Use Gemini 2.5 Flash to analyze the results\n",
        "try:\n",
        "\n",
        "    # Generate the response\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=full_prompt\n",
        "    )\n",
        "\n",
        "    # Display the summary and analysis from Gemini\n",
        "    display(Markdown(response.text))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while calling Gemini: {e}\")\n",
        "    print(\"Please ensure you have access to Gemini 2.5 Flash and your project/location settings are correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xRAqAOCpk3C"
      },
      "source": [
        "## Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXVk6VeKpolc"
      },
      "source": [
        "This notebook has shown how to use the Gen AI Evaluation Service to evaluate Gemini's Structured Output, for a document processing task.\n",
        "\n",
        "It uses a \"bring your own response\" approach and uses custom `valid_schema` and `accuracy` metrics as well as the `exact_match` metric.\n",
        "\n",
        "It also does a deep \"field-wise\" comparison of the responses to understand inaccuracies, and uses Gemini to summarise and analyse the results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m127",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
