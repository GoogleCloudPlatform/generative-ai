{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZUGfIJwT-H2"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NulM87jyTWJG"
      },
      "source": [
        "# Placeholder of Gen AI Agent Eval SDK (Colab 1 for UI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLGMy4WJF_7o"
      },
      "source": [
        "**WARNING:**\n",
        "\n",
        "This colab contains features under development. Please don't use it for production work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWiijMdFxAon"
      },
      "source": [
        "**Goals:**\n",
        "\n",
        "This Colab notebook demonstrates how to use the Gen AI Eval SDK to evaluate your agents. It covers one primary use cases:\n",
        "\n",
        "1. Run Agent + Create Evaluation Run: The SDK will first run the agent and then create evaluation run to perform the evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTK2Y3bRUMce"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqNKLziY0mr3"
      },
      "source": [
        "## Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nax0k_afTVKc"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4_NugC0UFCU"
      },
      "source": [
        "## Install Vertex AI SDK for Gen AI Evaluation Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWaDe8WN0sKm"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --force-reinstall -q google-cloud-aiplatform[evaluation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1r_ha0J69yQ"
      },
      "source": [
        "## Initialize Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8-HbNKnUQD3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import vertexai\n",
        "from google.genai.types import HttpOptions\n",
        "\n",
        "# fmt: off\n",
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "LOCATION = \"\"  # @param {type: \"string\", placeholder: \"us-central1\", isTemplate: true}\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", LOCATION)\n",
        "# e.g. gs://my-bucket/my-folder\n",
        "GCS_DEST = \"\"  # @param {type: \"string\", placeholder: \"[your-gcs-bucket]\", isTemplate: true}\n",
        "# fmt: on\n",
        "# e.g. projects/{project_id}/locations/us-central1/reasoningEngines/{reasoning_engine_id}\n",
        "AGENT = \"\"  # @param {type: \"string\", placeholder: \"[your-agent]\", isTemplate: true}\n",
        "\n",
        "\n",
        "from vertexai import Client, types\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "client = Client(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    http_options=HttpOptions(\n",
        "        api_version=\"v1beta1\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpGx1vx_GisH"
      },
      "source": [
        "## Define Dataset and Agent Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5_PPZEyGjMV"
      },
      "outputs": [],
      "source": [
        "session_inputs1 = types.SessionInput(\n",
        "    user_id=\"user_123\",\n",
        "    state={\"my_key\": \"my_value\"},\n",
        ")\n",
        "session_inputs2 = session_inputs1.copy()\n",
        "session_inputs3 = session_inputs1.copy()\n",
        "session_inputs4 = session_inputs1.copy()\n",
        "\n",
        "agent_eval_dataset = pd.DataFrame(\n",
        "    {\n",
        "        \"prompt\": [\n",
        "            \"Write a four-line poem about a lonely robot, where every line must be a question and the word 'and' cannot be used.\",\n",
        "            \"Write a Python function to find the nth Fibonacci number using recursion with memoization, but without using any imports.\",\n",
        "            \"Check if 20 is prime or not\",\n",
        "            \"You are an agent, Take 2 steps \\n 1. Check if 20 is prime or not \\n 2. Check if 10 is prime or not\",\n",
        "        ],\n",
        "        \"response\": [\n",
        "            \"response 1\",\n",
        "            \"response 2\",\n",
        "            \"response 3\",\n",
        "            \"response 4\",\n",
        "        ],\n",
        "        \"session_inputs\": [\n",
        "            session_inputs1,\n",
        "            session_inputs2,\n",
        "            session_inputs3,\n",
        "            session_inputs4,\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "agent_info = {\n",
        "    \"agent\": AGENT,\n",
        "    \"name\": \"example-agent\",\n",
        "    \"instruction\": \"example agent developer instruction\",\n",
        "    \"description\": \"example agent description\",\n",
        "    \"tool_declarations\": [\n",
        "        {\n",
        "            \"function_declarations\": [\n",
        "                {\n",
        "                    \"name\": \"check_chime\",\n",
        "                    \"description\": \"Check chime.\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"nums\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"List of numbers to be verified.\",\n",
        "                            },\n",
        "                        },\n",
        "                        \"required\": [\"nums\"],\n",
        "                    },\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCWypwtGG6JG"
      },
      "source": [
        "# Run Gen AI Agent Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR91EJlxG9aI"
      },
      "source": [
        "## Option 1: Run Gen AI Evaluation with Evaluation Management Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfjPgdSEG5ZI"
      },
      "outputs": [],
      "source": [
        "# Run Gen AI Agent Evaluation using the Evaluation Management Service.\n",
        "# This will persist your dataset and evaluation results.\n",
        "evaluation_run = client.evals.create_evaluation_run(\n",
        "    dataset=types.EvaluationDataset(\n",
        "        eval_dataset_df=agent_eval_dataset,\n",
        "        candidate_name=agent_info[\"name\"],\n",
        "    ),\n",
        "    agent_info=agent_info,\n",
        "    metrics=[\n",
        "        types.RubricMetric.SAFETY,\n",
        "    ],\n",
        "    dest=GCS_DEST,\n",
        ")\n",
        "evaluation_run.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdwLb05_HD35"
      },
      "source": [
        "### Poll Evaluation Run for Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dUt_MRJHEWB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "completed_states = set(\n",
        "    [\n",
        "        \"SUCCEEDED\",\n",
        "        \"FAILED\",\n",
        "        \"CANCELLED\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "while evaluation_run.state not in completed_states:\n",
        "    evaluation_run.show()\n",
        "    evaluation_run = client.evals.get_evaluation_run(name=evaluation_run.name)\n",
        "    time.sleep(5)\n",
        "evaluation_run = client.evals.get_evaluation_run(\n",
        "    name=evaluation_run.name, include_evaluation_items=True\n",
        ")\n",
        "\n",
        "evaluation_run.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0tsjuRmHJXP"
      },
      "source": [
        "## [Optional] Option 2: Run Gen AI Evaluation Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU_Ojem5HJsS"
      },
      "outputs": [],
      "source": [
        "eval_result = client.evals.evaluate(\n",
        "    dataset=types.EvaluationDataset(\n",
        "        eval_dataset_df=agent_eval_dataset,\n",
        "        candidate_name=agent_info[\"name\"],\n",
        "    ),\n",
        "    agent_info=agent_info,\n",
        "    metrics=[\n",
        "        types.RubricMetric.SAFETY,\n",
        "    ],\n",
        ")\n",
        "\n",
        "eval_result.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab_1_for_agent_eval_UI_placeholder.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
