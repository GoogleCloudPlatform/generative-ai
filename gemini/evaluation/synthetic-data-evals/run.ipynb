{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Evaluation for Customer Support: Comparing Models and Parameters\n",
    "This notebook demonstrates how to generate synthetic evaluation data and compare different models and parameters for a customer support agent. We'll explore three main types of evaluations:\n",
    "Final Response Evaluation: Assessing the agent's final answer\n",
    "Single Step Evaluation: Evaluating individual tool selections\n",
    "Trajectory Evaluation: Analyzing the complete path of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "First, let's import the necessary libraries and initialize our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import weave\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Import our modules\n",
    "from evaluator import AgentEvaluator, load_dataset\n",
    "from dataset_generator import DatasetGenerator, create_customer_support_agent_evaluation_dataset\n",
    "from customer_support_agent import create_customer_support_agent\n",
    "from config import WEAVE_PROJECT_NAME\n",
    "\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "console.rule(\"[bold magenta]Agent Evaluation Framework\")\n",
    "\n",
    "# Initialize Weave for experiment tracking\n",
    "weave.init(WEAVE_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations to test\n",
    "model_configs = [\n",
    "    {\"model_id\": \"google/gemini-1.5-pro\", \"temperature\": 0.2, \"name\": \"Gemini Pro (Low Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-1.5-pro\", \"temperature\": 0.7, \"name\": \"Gemini Pro (High Temp)\"},\n",
    "    #Add not deepseak but OSS model from vertex{}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Synthetic Evaluation Dataset\n",
    "Let's create a synthetic dataset for evaluating our customer support agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"[bold blue]Generating synthetic evaluation dataset...[/bold blue]\")\n",
    "\n",
    "# Create base customer support agent for dataset generation\n",
    "base_agent = create_customer_support_agent(\n",
    "    use_weave=True,\n",
    "    model_id=\"google/gemini-1.5-pro\",\n",
    "    temperature=0.2,\n",
    "    planning_interval=2,\n",
    "    max_steps=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_agent.run(\"What is the best item in the category of book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset generator\n",
    "thresholds={\n",
    "            \"final_response\": 0.7,\n",
    "            \"single_step\": 0.7,\n",
    "            \"trajectory\": 0.7\n",
    "        }\n",
    "generator = DatasetGenerator(agent=base_agent, thresholds=thresholds, debug=True)\n",
    "\n",
    "# Generate comprehensive dataset with different scenarios\n",
    "console.print(\"[bold blue]Generating customer support evaluation dataset...[/bold blue]\")\n",
    "\n",
    "dataset = create_customer_support_agent_evaluation_dataset(generator, base_agent, num_prompts=5)  # Adjust number as needed\n",
    "\n",
    "# Save generated dataset\n",
    "dataset_path = \"customer_support_eval.json\"\n",
    "generator.save_dataset(dataset, dataset_path)\n",
    "\n",
    "console.print(f\"[green]✓[/green] Dataset generation complete! Saved to {dataset_path}\")\n",
    "\n",
    "# # Display dataset statistics\n",
    "# dataset_stats = {}\n",
    "# for category, examples in dataset.items():\n",
    "#     dataset_stats[category] = len(examples)\n",
    "\n",
    "# stats_table = Table(title=\"Generated Dataset Statistics\")\n",
    "# stats_table.add_column(\"Category\", style=\"cyan\")\n",
    "# stats_table.add_column(\"Count\", style=\"green\")\n",
    "\n",
    "# for category, count in dataset_stats.items():\n",
    "#     stats_table.add_row(category, str(count))\n",
    "\n",
    "# console.print(stats_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Evaluator\n",
    "Now let's set up our evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "console.print(\"[bold blue]Initializing evaluator...[/bold blue]\")\n",
    "evaluator = AgentEvaluator(debug=True)\n",
    "console.print(f\"[green]✓[/green] Evaluator initialized\")\n",
    "\n",
    "all_examples = load_dataset(\"customer_support_eval.json\")\n",
    "console.print(f\"[bold blue]Formatting dataset with {len(all_examples)} examples for evaluation...[/bold blue]\")\n",
    "eval_dataset = evaluator.format_dataset_for_eval(all_examples)\n",
    "console.print(f\"[green]✓[/green] Dataset formatted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Evaluations for Different Model Configurations\n",
    "Now let's evaluate different model configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def evaluate_model_config(config: Dict[str, Any], eval_dataset: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a specific model configuration and return results\"\"\"\n",
    "    console.print(f\"\\n[bold blue]Evaluating {config['name']}...[/bold blue]\")\n",
    "    \n",
    "    # Create agent with this configuration\n",
    "    agent = create_customer_support_agent(\n",
    "        model_id=config[\"model_id\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "        planning_interval=1,\n",
    "        max_steps=5\n",
    "    )\n",
    "    \n",
    "    # Define a function that runs the agent and returns the result\n",
    "    def run_agent(prompt):\n",
    "        return agent.run(prompt)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.run_evaluation(\n",
    "        run_agent, \n",
    "        eval_dataset, \n",
    "        output_dir=f\"evaluation_results/{config['name'].replace(' ', '_').lower()}\"\n",
    "    )\n",
    "    \n",
    "    # Add configuration details to results\n",
    "    results[\"config\"] = config\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluations for all configurations\n",
    "all_results = []\n",
    "for config in model_configs:\n",
    "    results = evaluate_model_config(config, eval_dataset)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Display summary results\n",
    "    console.print(f\"\\n[bold green]Results for {config['name']}:[/bold green]\")\n",
    "    if \"summary_metrics\" in results and results[\"summary_metrics\"]:\n",
    "        for metric, value in results[\"summary_metrics\"].items():\n",
    "            console.print(f\"[cyan]{metric}:[/cyan] {value:.2f}\")\n",
    "    else:\n",
    "        console.print(\"[yellow]No summary metrics available[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Model Performance\n",
    "Let's visualize and compare the performance of different model configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for result in all_results:\n",
    "    config = result[\"config\"]\n",
    "    metrics = result[\"summary_metrics\"]\n",
    "    \n",
    "    row = {\n",
    "        \"Model\": config[\"name\"],\n",
    "        \"Model ID\": config[\"model_id\"],\n",
    "        \"Temperature\": config[\"temperature\"]\n",
    "    }\n",
    "    \n",
    "    # Add all metrics\n",
    "    for metric, value in metrics.items():\n",
    "        row[metric] = value\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison to CSV\n",
    "comparison_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "console.print(f\"[green]✓[/green] Comparison results saved to model_comparison_results.csv\")\n",
    "\n",
    "# Display comparison table\n",
    "console.rule(\"[bold magenta]Model Comparison\")\n",
    "console.print(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get metric columns\n",
    "metric_columns = [col for col in comparison_df.columns \n",
    "                 if col not in [\"Model\", \"Model ID\", \"Temperature\"]]\n",
    "\n",
    "# Create bar chart for each metric\n",
    "for i, metric in enumerate(metric_columns):\n",
    "    plt.subplot(len(metric_columns), 1, i+1)\n",
    "    sns.barplot(x=\"Model\", y=metric, data=comparison_df)\n",
    "    plt.title(f\"Comparison of {metric}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"model_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed Analysis of Model Differences\n",
    "Let's analyze the differences between models in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare trajectory vs response scores across models\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Create scatter plots for each model\n",
    "# for result in all_results:\n",
    "#     config = result[\"config\"]\n",
    "#     detailed_metrics = result[\"detailed_metrics\"]\n",
    "    \n",
    "#     if \"trajectory_match_score\" in detailed_metrics.columns and \"response_correctness_score\" in detailed_metrics.columns:\n",
    "#         plt.scatter(\n",
    "#             detailed_metrics[\"trajectory_match_score\"],\n",
    "#             detailed_metrics[\"response_correctness_score\"],\n",
    "#             alpha=0.7,\n",
    "#             label=config[\"name\"]\n",
    "#         )\n",
    "\n",
    "# plt.xlabel(\"Trajectory Match Score\")\n",
    "# plt.ylabel(\"Response Correctness Score\")\n",
    "# plt.title(\"Relationship Between Trajectory and Response Quality Across Models\")\n",
    "# plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "# plt.legend()\n",
    "# plt.savefig(\"trajectory_vs_response_by_model.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # Analyze performance by query complexity\n",
    "# if \"difficulty\" in eval_dataset[0]:\n",
    "#     # Create a dataframe with all detailed metrics and model info\n",
    "#     all_detailed_metrics = []\n",
    "    \n",
    "#     for result in all_results:\n",
    "#         config = result[\"config\"]\n",
    "#         detailed_metrics = result[\"detailed_metrics\"].copy()\n",
    "#         detailed_metrics[\"Model\"] = config[\"name\"]\n",
    "#         all_detailed_metrics.append(detailed_metrics)\n",
    "    \n",
    "#     all_metrics_df = pd.concat(all_detailed_metrics)\n",
    "    \n",
    "#     # Plot performance by difficulty\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     sns.boxplot(x=\"difficulty\", y=\"response_correctness_score\", hue=\"Model\", data=all_metrics_df)\n",
    "#     plt.title(\"Response Correctness by Query Difficulty\")\n",
    "#     plt.xlabel(\"Query Difficulty\")\n",
    "#     plt.ylabel(\"Response Correctness Score\")\n",
    "#     plt.xticks(rotation=0)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"performance_by_difficulty.png\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Study: Comparing Models on a Specific Example\n",
    "Let's examine one specific example to see how different models handle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select a specific example to analyze\n",
    "# if len(eval_dataset) > 0:\n",
    "#     example = eval_dataset[0]  # Choose the first example\n",
    "    \n",
    "#     console.rule(\"[bold magenta]Case Study: Model Comparison on Specific Example\")\n",
    "    \n",
    "#     console.print(f\"[bold cyan]User Query:[/bold cyan] {example['context']}\")\n",
    "#     console.print(f\"[bold cyan]Expected Response:[/bold cyan] {example['reference']}\")\n",
    "    \n",
    "#     # Display expected trajectory if available\n",
    "#     if \"expected_trajectory\" in example:\n",
    "#         console.print(\"\\n[bold cyan]Expected Trajectory:[/bold cyan]\")\n",
    "#         for i, step in enumerate(example['expected_trajectory']):\n",
    "#             console.print(f\"[bold]Step {i+1}:[/bold]\")\n",
    "#             console.print(f\"  Tool: {step['tool_name']}\")\n",
    "#             console.print(f\"  Input: {step.get('tool_input', 'N/A')}\")\n",
    "#             if \"reasoning\" in step:\n",
    "#                 console.print(f\"  Reasoning: {step['reasoning'][:100]}...\")\n",
    "    \n",
    "#     # Compare model responses\n",
    "#     console.print(\"\\n[bold cyan]Model Responses:[/bold cyan]\")\n",
    "    \n",
    "#     response_table = Table(title=\"Model Responses\")\n",
    "#     response_table.add_column(\"Model\", style=\"cyan\")\n",
    "#     response_table.add_column(\"Response\", style=\"green\")\n",
    "#     response_table.add_column(\"Response Score\", style=\"yellow\")\n",
    "#     response_table.add_column(\"Trajectory Score\", style=\"yellow\")\n",
    "    \n",
    "#     for result in all_results:\n",
    "#         config = result[\"config\"]\n",
    "#         detailed_metrics = result[\"detailed_metrics\"]\n",
    "        \n",
    "#         # Find this example in the detailed metrics\n",
    "#         example_metrics = detailed_metrics[detailed_metrics['context'] == example['context']]\n",
    "        \n",
    "#         if not example_metrics.empty:\n",
    "#             model_name = config[\"name\"]\n",
    "#             response = example_metrics[\"candidate\"].values[0]\n",
    "#             response_score = example_metrics[\"response_correctness_score\"].values[0] if \"response_correctness_score\" in example_metrics.columns else \"N/A\"\n",
    "#             trajectory_score = example_metrics[\"trajectory_match_score\"].values[0] if \"trajectory_match_score\" in example_metrics.columns else \"N/A\"\n",
    "            \n",
    "#             response_table.add_row(\n",
    "#                 model_name, \n",
    "#                 response[:100] + \"...\" if len(response) > 100 else response,\n",
    "#                 str(response_score),\n",
    "#                 str(trajectory_score)\n",
    "#             )\n",
    "    \n",
    "#     console.print(response_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Parameter Impact\n",
    "Let's analyze how temperature affects model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group models by type and analyze temperature impact\n",
    "gemini_results = [r for r in all_results if \"gemini\" in r[\"config\"][\"model_id\"].lower()]\n",
    "\n",
    "if len(gemini_results) > 1:\n",
    "    console.rule(\"[bold magenta]Temperature Impact Analysis (Gemini)\")\n",
    "    \n",
    "    # Create dataframe for temperature analysis\n",
    "    temp_data = []\n",
    "    \n",
    "    for result in gemini_results:\n",
    "        config = result[\"config\"]\n",
    "        metrics = result[\"summary_metrics\"]\n",
    "        \n",
    "        row = {\n",
    "            \"Temperature\": config[\"temperature\"]\n",
    "        }\n",
    "        \n",
    "        # Add all metrics\n",
    "        for metric, value in metrics.items():\n",
    "            row[metric] = value\n",
    "        \n",
    "        temp_data.append(row)\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp_data)\n",
    "    \n",
    "    # Display temperature comparison\n",
    "    console.print(temp_df)\n",
    "    \n",
    "    # Visualize temperature impact\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get metric columns\n",
    "    metric_columns = [col for col in temp_df.columns if col != \"Temperature\"]\n",
    "    \n",
    "    # Create line chart for each metric\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        plt.subplot(len(metric_columns), 1, i+1)\n",
    "        sns.lineplot(x=\"Temperature\", y=metric, data=temp_df, marker='o')\n",
    "        plt.title(f\"Impact of Temperature on {metric}\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"temperature_impact.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Let's summarize what we've learned from comparing different models and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.rule(\"[bold magenta]Evaluation Summary\")\n",
    "\n",
    "# Calculate best model for each metric\n",
    "best_models = {}\n",
    "for metric in metric_columns:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_models[metric] = comparison_df.loc[best_idx, \"Model\"]\n",
    "\n",
    "# Create summary table\n",
    "summary_table = Table(title=\"Evaluation Summary\")\n",
    "summary_table.add_column(\"Metric\", style=\"cyan\")\n",
    "summary_table.add_column(\"Best Model\", style=\"green\")\n",
    "summary_table.add_column(\"Score\", style=\"yellow\")\n",
    "\n",
    "for metric, model in best_models.items():\n",
    "    score = comparison_df.loc[comparison_df[\"Model\"] == model, metric].values[0]\n",
    "    summary_table.add_row(metric, model, f\"{score:.2f}\")\n",
    "\n",
    "console.print(summary_table)\n",
    "\n",
    "console.print(\"\"\"\n",
    "[bold]Key Findings:[/bold]\n",
    "\n",
    "1. [bold cyan]Model Performance:[/bold cyan] We compared different models (Gemini Pro and Claude Sonnet) with varying parameters to identify the best configuration for our customer support agent.\n",
    "\n",
    "2. [bold cyan]Temperature Impact:[/bold cyan] We analyzed how temperature affects model performance, finding that lower temperatures generally lead to more consistent and accurate responses.\n",
    "\n",
    "3. [bold cyan]Evaluation Types:[/bold cyan] We used three evaluation approaches:\n",
    "   - Final Response Evaluation: Assessing overall task completion\n",
    "   - Single Step Evaluation: Evaluating tool selection accuracy\n",
    "   - Trajectory Evaluation: Analyzing the complete reasoning path\n",
    "\n",
    "4. [bold cyan]Performance by Difficulty:[/bold cyan] We examined how models perform across different query complexities, identifying strengths and weaknesses.\n",
    "\n",
    "[bold]Next Steps:[/bold]\n",
    "\n",
    "1. Fine-tune the best-performing model configuration for production deployment\n",
    "2. Expand the evaluation dataset with more diverse customer support scenarios\n",
    "3. Implement continuous evaluation to monitor agent performance over time\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
