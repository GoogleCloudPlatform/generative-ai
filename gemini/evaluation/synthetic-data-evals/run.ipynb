{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation for Customer Support: Comparing Models and Parameters\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Authors | [Anish Shah](https://github.com/ash0ts) |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate synthetic evaluation data and compare different models and parameters for a customer support agent. We'll explore three main types of evaluations:\n",
    "\n",
    "1. Final Response Evaluation: Assessing the agent's final answer\n",
    "2. Single Step Evaluation: Evaluating individual tool selections\n",
    "3. Trajectory Evaluation: Analyzing the complete path of actions\n",
    "\n",
    "The tutorial uses the following Google Cloud services and resources:\n",
    "* [Vertex AI Gen AI Evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)\n",
    "* [Weights and Biases Weave](https://wandb.me/tryweave)\n",
    "\n",
    "The steps performed include:\n",
    "* Generating synthetic evaluation data\n",
    "* Setting up evaluation metrics\n",
    "* Comparing model performance\n",
    "* Analyzing evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Please run the `setup.py` script in this folder before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !git clone https://github.com/ash0ts/generative-ai.git\n",
    "    %cd generative-ai/gemini/evaluation/synthetic-data-evals\n",
    "    !pip install -qqq uv \n",
    "    !uv pip install --system --requirements pyproject.toml\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from set_env import set_env\n",
    "set_env(\"GEMINI_API_KEY\")\n",
    "# set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "set_env(\"VERTEX_PROJECT_ID\")\n",
    "set_env(\"VERTEX_LOCATION\")\n",
    "set_env(\"VERTEX_MODEL_ID\")\n",
    "set_env(\"VERTEX_ENDPOINT_ID\")\n",
    "# set_env(\"DEEPSEEK_ENDPOINT_ID\")\n",
    "print(\"Set API Keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import weave\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "\n",
    "# Import our modules\n",
    "from evaluator import AgentEvaluator, load_dataset\n",
    "from dataset_generator import DatasetGenerator, create_customer_support_agent_evaluation_dataset\n",
    "from customer_support_agent import create_customer_support_agent\n",
    "from render_evals import render_model_comparison, render_difficulty_analysis, render_temperature_analysis, render_conclusion\n",
    "from config import WEAVE_PROJECT_NAME\n",
    "\n",
    "try:\n",
    "    in_jupyter = True\n",
    "except ImportError:\n",
    "    in_jupyter = False\n",
    "if in_jupyter:\n",
    "    import nest_asyncio\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "console.rule(\"[bold magenta]Agent Evaluation Framework\")\n",
    "\n",
    "# Initialize Weave for experiment tracking\n",
    "weave.init(WEAVE_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations to test\n",
    "model_configs = [\n",
    "    {\"model_id\": \"google/gemini-1.5-pro\", \"temperature\": 0.2, \"name\": \"Gemini Pro (Low Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-1.5-pro\", \"temperature\": 0.7, \"name\": \"Gemini Pro (High Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-2.0-flash-lite\", \"temperature\": 0.2, \"name\": \"Gemini Flash Lite (Low Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-2.0-flash-lite\", \"temperature\": 0.7, \"name\": \"Gemini Flash Lite (High Temp)\"},\n",
    "    #Any other Gemini or OSS model from vertex\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Support Agent Architecture\n",
    "\n",
    "This section describes the architecture and implementation of our customer support agent system, which is built using a combination of LLM-powered components and specialized tools.\n",
    "\n",
    "## Agent Overview\n",
    "\n",
    "The customer support agent is designed to handle a variety of e-commerce related queries by leveraging:\n",
    "\n",
    "1. **Foundation Models**: The agent can be powered by different models including Gemini (1.5 Pro, 2.0 Flash, etc.) through Vertex AI.\n",
    "\n",
    "2. **Specialized Tools**: A collection of purpose-built tools that provide domain-specific functionality:\n",
    "   - `ProductSearchTool`: Searches the product catalog by name, category, or description\n",
    "   - `OrderStatusTool`: Checks the status of customer orders\n",
    "   - `CategoryBrowseTool`: Allows browsing products by category\n",
    "   - `PriceCheckTool`: Retrieves pricing information for specific products\n",
    "   - `CustomerOrderHistoryTool`: Retrieves order history for customers\n",
    "\n",
    "3. **Realistic Data**: The system uses realistic product and order data derived from Amazon reviews to provide a realistic customer support experience.\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "The implementation consists of two main components:\n",
    "\n",
    "1. **`customer_support_agent.py`**: Contains the tool implementations and agent creation logic. The `create_customer_support_agent()` function configures the agent with the specified model, temperature, planning capabilities, and tools.\n",
    "\n",
    "2. **`vertex_model.py`**: Provides the model implementations that connect to Vertex AI:\n",
    "   - `VertexAIServerModel`: Base implementation for connecting to Vertex AI endpoints\n",
    "   - `WeaveVertexAIServerModel`: Extends the base model with Weights & Biases Weave tracking for experiment monitoring\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "The agent can be configured with various parameters:\n",
    "- Model selection (Gemini 1.5 Pro, 2.0 Flash, etc.)\n",
    "- Temperature settings for controlling response randomness\n",
    "- Planning interval to determine how often the agent should plan its actions\n",
    "- Maximum steps to limit the complexity of interactions\n",
    "\n",
    "This architecture enables comprehensive evaluation of different model configurations and parameters to determine the optimal setup for customer support scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"[bold blue]Creating customer support agent[/bold blue]\")\n",
    "\n",
    "# Initialize a customer support agent for generating high-quality evaluation data\n",
    "base_agent = create_customer_support_agent(\n",
    "    use_weave=True,                    # Enable Weave for experiment tracking\n",
    "    model_id=\"google/gemini-2.0-flash\", # Use Gemini 2.0 for fast and accurate generation\n",
    "    temperature=0.1,                   # Low temperature for consistent, deterministic outputs\n",
    "    planning_interval=2,               # Plan every 2 steps for better reasoning\n",
    "    max_steps=4                        # Allow up to 4 steps to handle medium-complex queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_agent.run(\"What is the best item in the category of book?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation for Agent Evaluation\n",
    "\n",
    "This section explains how we generate realistic test data to evaluate our customer support agent across different configurations and scenarios.\n",
    "\n",
    "## Why We Need Synthetic Evaluation Data\n",
    "\n",
    "Testing with synthetic data helps us:\n",
    "\n",
    "1. **Compare Models Fairly**: We can test different models (Gemini Pro vs Flash) and settings (temperature values) on the exact same customer queries.\n",
    "\n",
    "2. **Save Time and Resources**: Creating test data is faster and cheaper than collecting real customer conversations.\n",
    "\n",
    "3. **Cover Edge Cases**: We can include challenging scenarios that might be rare in real data but important to test.\n",
    "\n",
    "4. **Ensure Consistent Quality**: By filtering examples that meet quality thresholds, we build a reliable benchmark dataset.\n",
    "\n",
    "## How the Dataset Generator Works\n",
    "\n",
    "The `DatasetGenerator` class creates evaluation data through several steps:\n",
    "\n",
    "1. **Creating Realistic Queries**: Generates e-commerce questions using real product IDs, categories, and customer information:\n",
    "   - \"I'm looking for products in the Books category. What do you have?\"\n",
    "   - \"Can you check the status of my order OD123456?\"\n",
    "   - \"What's your best Electronics product? I need something reliable.\"\n",
    "\n",
    "2. **Recording Agent Behavior**: Runs the agent on these queries and captures:\n",
    "   - Which tools the agent used (ProductSearch, OrderStatus, etc.)\n",
    "   - The arguments passed to each tool\n",
    "   - The agent's reasoning at each step\n",
    "   - The final response to the user\n",
    "\n",
    "3. **Evaluating Quality**: Uses a judge model to score:\n",
    "   - Final response quality (0-1 score)\n",
    "   - Individual step effectiveness (0-1 score per step)\n",
    "   - Overall trajectory coherence (0-1 score)\n",
    "\n",
    "4. **Filtering Results**: Only keeps examples that meet quality thresholds (typically 0.7 for each dimension).\n",
    "\n",
    "## Dataset Structure and Applications\n",
    "\n",
    "Each example in the final dataset includes:\n",
    "- The original user prompt\n",
    "- Expected tool usage sequence\n",
    "- Validation criteria\n",
    "- Difficulty rating (easy, medium, hard)\n",
    "- Metadata about model configuration\n",
    "\n",
    "This dataset lets us:\n",
    "1. Determine which model performs best for customer support\n",
    "2. Find the right temperature settings for different query types\n",
    "3. Measure whether planning capabilities improve results\n",
    "4. Identify specific areas where the agent needs improvement\n",
    "\n",
    "By testing systematically, we can build a more effective customer support agent that handles user queries accurately while using computational resources efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset generator\n",
    "thresholds={\n",
    "            \"final_response\": 0.7,\n",
    "            \"single_step\": 0.7,\n",
    "            \"trajectory\": 0.7\n",
    "        }\n",
    "generator = DatasetGenerator(agent=base_agent, thresholds=thresholds, debug=True)\n",
    "\n",
    "# Generate comprehensive dataset with different scenarios\n",
    "console.print(\"[bold blue]Generating customer support evaluation dataset...[/bold blue]\")\n",
    "\n",
    "dataset = create_customer_support_agent_evaluation_dataset(generator, base_agent, num_prompts=5)  # Adjust number as needed\n",
    "\n",
    "# Save generated dataset\n",
    "dataset_path = \"customer_support_eval.json\"\n",
    "generator.save_dataset(dataset, dataset_path)\n",
    "\n",
    "console.print(f\"[green]✓[/green] Dataset generation complete! Saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation Framework\n",
    "\n",
    "This section explains how we evaluate different model configurations using a comprehensive evaluation framework to identify the best-performing agent setup.\n",
    "\n",
    "## Setting Up the Evaluation Pipeline\n",
    "\n",
    "The evaluation process begins with initializing our evaluation framework defined in `evaluator.py`.\n",
    "The below code loads our previously generated synthetic dataset and formats it for evaluation with Vertex AI.\n",
    "\n",
    "## How the Evaluation Process Works\n",
    "\n",
    "The `AgentEvaluator` class provides a systematic approach to testing agent performance across multiple dimensions:\n",
    "\n",
    "1. **Multi-dimensional Metrics**: We evaluate the agent on several key aspects:\n",
    "   - **Tool Selection Accuracy**: How well the agent chooses appropriate tools\n",
    "   - **Reasoning Quality**: The logical coherence of the agent's thinking process\n",
    "   - **Response Correctness**: Accuracy and completeness of final answers\n",
    "   - **Trajectory Match**: How well the agent's path aligns with expected solutions\n",
    "   - **Coherence**: Overall clarity and consistency of responses\n",
    "\n",
    "2. **Vertex AI Integration**: The evaluator uses Vertex AI's evaluation capabilities to score agent responses objectively, reducing human bias in the assessment process.\n",
    "\n",
    "3. **Weights & Biases Weave Integration**: Results are automatically logged to Weave, enabling:\n",
    "   - Interactive visualization of agent performance\n",
    "   - Comparison between different model configurations\n",
    "   - Tracking of experiments over time\n",
    "   - Sharing results with team members\n",
    "\n",
    "4. **Visualization Tools**: The framework generates charts and tables to help identify patterns:\n",
    "   - Score distribution plots showing performance across metrics\n",
    "   - Difficulty heatmaps revealing how the agent handles easy vs. hard queries\n",
    "   - Correlation analysis between trajectory quality and response accuracy\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "This evaluation framework helps us answer key questions:\n",
    "\n",
    "1. **Which model performs best?** Compare Gemini Pro vs. Flash models on the same test cases.\n",
    "\n",
    "2. **What temperature setting works better?** Test whether low temperature (0.2) or high temperature (0.7) produces better results.\n",
    "\n",
    "3. **Where are the weaknesses?** Identify specific query types or metrics where the agent underperforms.\n",
    "\n",
    "4. **Is planning helpful?** Measure whether enabling planning capabilities improves overall performance.\n",
    "\n",
    "By running all model configurations through this standardized evaluation process, we can make data-driven decisions about which agent setup to deploy for customer support scenarios, balancing performance and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "console.print(\"[bold blue]Initializing evaluator...[/bold blue]\")\n",
    "evaluator = AgentEvaluator(verbosity=1, project=os.getenv(\"VERTEX_PROJECT_ID\"), location=os.getenv(\"VERTEX_LOCATION\"))\n",
    "console.print(f\"[green]✓[/green] Evaluator initialized\")\n",
    "\n",
    "all_examples = load_dataset(\"customer_support_eval.json\")\n",
    "console.print(f\"[bold blue]Formatting dataset with {len(all_examples)} examples for evaluation...[/bold blue]\")\n",
    "eval_dataset = evaluator.format_dataset_for_eval(all_examples)\n",
    "console.print(f\"[green]✓[/green] Dataset formatted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model Evaluations\n",
    "\n",
    "This section shows how we test different model configurations against our evaluation dataset to find the best setup for customer support.\n",
    "\n",
    "## Testing Multiple Configurations\n",
    "\n",
    "We evaluate four different configurations:\n",
    "- Gemini Pro with low temperature (0.2)\n",
    "- Gemini Pro with high temperature (0.7)\n",
    "- Gemini Flash Lite with low temperature (0.2)\n",
    "- Gemini Flash Lite with high temperature (0.7)\n",
    "\n",
    "Each configuration is tested on the same set of customer queries, ensuring a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def evaluate_model_config(config: Dict[str, Any], eval_dataset: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a specific model configuration and return results\"\"\"\n",
    "    console.print(f\"\\n[bold blue]Evaluating {config['name']}...[/bold blue]\")\n",
    "    \n",
    "    # Create agent with this configuration\n",
    "    agent = create_customer_support_agent(\n",
    "        use_weave=True,\n",
    "        model_id=config[\"model_id\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "        planning_interval=config.get(\"planning_interval\", 1),\n",
    "        max_steps=config.get(\"max_steps\", 5)\n",
    "    )\n",
    "    \n",
    "    # Run evaluation with the agent object directly\n",
    "    results = evaluator.run_evaluation(\n",
    "        agent=agent, \n",
    "        eval_dataset=eval_dataset, \n",
    "        output_dir=f\"evaluation_results/{config['name'].replace(' ', '_').lower()}\",\n",
    "        weave_project=WEAVE_PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    # Add configuration details to results\n",
    "    results[\"config\"] = config\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluations for all configurations\n",
    "all_results = []\n",
    "for config in model_configs:\n",
    "    results = evaluate_model_config(config, eval_dataset)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Display summary results\n",
    "    console.print(f\"\\n[bold green]Results for {config['name']}:[/bold green]\")\n",
    "    if \"summary_metrics\" in results and results[\"summary_metrics\"]:\n",
    "        table = evaluator._render_summary_table(results[\"summary_metrics\"])\n",
    "        console.print(table)\n",
    "    else:\n",
    "        console.print(\"[yellow]No summary metrics available[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigate to Weave to compare results!!\n",
    "\n",
    "Go to the project set in `WEAVE_PROJECT_NAME` in `config.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Model Performance Patterns\n",
    "\n",
    "After running evaluations on different model configurations, we analyze the results to uncover key patterns and insights.\n",
    "\n",
    "## Response Quality by Model Type\n",
    "\n",
    "We compare how Gemini Pro and Gemini Flash Lite perform on response quality metrics. The analysis reveals which model provides more accurate and helpful customer support responses across our test cases.\n",
    "\n",
    "## Impact of Temperature Settings\n",
    "\n",
    "By examining how temperature affects performance, we can determine whether lower temperatures (0.2) produce more consistent, reliable responses or if higher temperatures (0.7) generate more helpful, creative solutions for customer queries.\n",
    "\n",
    "## Performance Across Query Difficulty\n",
    "\n",
    "We analyze how different models handle queries of varying complexity:\n",
    "- Which model excels at simple, straightforward questions?\n",
    "- Which configuration best handles complex, multi-part customer issues?\n",
    "- Are there specific difficulty levels where one model significantly outperforms others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_model_comparison(all_results, console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_difficulty_analysis(all_results, console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_temperature_analysis(all_results, console=console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Our systematic evaluation of different model configurations provides a framework for making data-driven decisions about customer support agent implementation.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "With the evaluation results in hand, we can now:\n",
    "\n",
    "1. **Select the optimal configuration** based on actual performance metrics rather than assumptions or theoretical capabilities.\n",
    "\n",
    "2. **Understand trade-offs** between different models (Gemini Pro vs. Flash Lite) and temperature settings (0.2 vs. 0.7) for our specific customer support scenarios.\n",
    "\n",
    "3. **Identify improvement areas** by focusing on the metrics where even our best configuration underperformed.\n",
    "\n",
    "4. **Expand our evaluation dataset** to include more diverse customer queries and edge cases.\n",
    "\n",
    "This evaluation framework allows us to continually refine our customer support agent as new models become available or as customer needs evolve. By measuring performance objectively across multiple dimensions, we can ensure we're deploying the most effective solution for our specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_conclusion(all_results, console=console)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
