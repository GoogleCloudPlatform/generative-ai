{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation for Customer Support: Comparing Models and Parameters\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"#\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"#\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Authors | [Anish Shah](https://github.com/ash0ts) |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate synthetic evaluation data and compare different models and parameters for a customer support agent. We'll explore three main types of evaluations:\n",
    "\n",
    "1. Final Response Evaluation: Assessing the agent's final answer\n",
    "2. Single Step Evaluation: Evaluating individual tool selections\n",
    "3. Trajectory Evaluation: Analyzing the complete path of actions\n",
    "\n",
    "The tutorial uses the following Google Cloud services and resources:\n",
    "* [Vertex AI Gen AI Evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)\n",
    "* [Weights and Biases Weave](https://wandb.me/tryweave)\n",
    "\n",
    "The steps performed include:\n",
    "* Generating synthetic evaluation data\n",
    "* Setting up evaluation metrics\n",
    "* Comparing model performance\n",
    "* Analyzing evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Please run the `setup.py` script in this folder before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !git clone https://github.com/ash0ts/generative-ai.git\n",
    "    %cd generative-ai/gemini/evaluation/synthetic-data-evals\n",
    "    !pip install -qqq uv \n",
    "    !uv pip install --system --requirements pyproject.toml\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from set_env import set_env\n",
    "set_env(\"GEMINI_API_KEY\")\n",
    "# set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "set_env(\"VERTEX_PROJECT_ID\")\n",
    "set_env(\"VERTEX_LOCATION\")\n",
    "set_env(\"VERTEX_MODEL_ID\")\n",
    "set_env(\"VERTEX_ENDPOINT_ID\")\n",
    "# set_env(\"DEEPSEEK_ENDPOINT_ID\")\n",
    "print(\"Set API Keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import weave\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "\n",
    "# Import our modules\n",
    "from evaluator import AgentEvaluator, load_dataset\n",
    "from dataset_generator import DatasetGenerator, create_customer_support_agent_evaluation_dataset\n",
    "from customer_support_agent import create_customer_support_agent\n",
    "from config import WEAVE_PROJECT_NAME\n",
    "\n",
    "try:\n",
    "    in_jupyter = True\n",
    "except ImportError:\n",
    "    in_jupyter = False\n",
    "if in_jupyter:\n",
    "    import nest_asyncio\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "console.rule(\"[bold magenta]Agent Evaluation Framework\")\n",
    "\n",
    "# Initialize Weave for experiment tracking\n",
    "weave.init(WEAVE_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations to test\n",
    "model_configs = [\n",
    "    {\"model_id\": \"google/gemini-1.5-pro\", \"temperature\": 0.2, \"name\": \"Gemini Pro (Low Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-1.5-pro\", \"temperature\": 0.7, \"name\": \"Gemini Pro (High Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-2.0-flash-lite\", \"temperature\": 0.2, \"name\": \"Gemini Flash Lite (Low Temp)\"},\n",
    "    {\"model_id\": \"google/gemini-2.0-flash-lite\", \"temperature\": 0.7, \"name\": \"Gemini Flash Lite (High Temp)\"},\n",
    "    #Any other Gemini or OSS model from vertex\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Support Agent Architecture\n",
    "\n",
    "This section describes the architecture and implementation of our customer support agent system, which is built using a combination of LLM-powered components and specialized tools.\n",
    "\n",
    "## Agent Overview\n",
    "\n",
    "The customer support agent is designed to handle a variety of e-commerce related queries by leveraging:\n",
    "\n",
    "1. **Foundation Models**: The agent can be powered by different models including Gemini (1.5 Pro, 2.0 Flash, etc.) through Vertex AI.\n",
    "\n",
    "2. **Specialized Tools**: A collection of purpose-built tools that provide domain-specific functionality:\n",
    "   - `ProductSearchTool`: Searches the product catalog by name, category, or description\n",
    "   - `OrderStatusTool`: Checks the status of customer orders\n",
    "   - `CategoryBrowseTool`: Allows browsing products by category\n",
    "   - `PriceCheckTool`: Retrieves pricing information for specific products\n",
    "   - `CustomerOrderHistoryTool`: Retrieves order history for customers\n",
    "\n",
    "3. **Realistic Data**: The system uses realistic product and order data derived from Amazon reviews to provide a realistic customer support experience.\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "The implementation consists of two main components:\n",
    "\n",
    "1. **`customer_support_agent.py`**: Contains the tool implementations and agent creation logic. The `create_customer_support_agent()` function configures the agent with the specified model, temperature, planning capabilities, and tools.\n",
    "\n",
    "2. **`vertex_model.py`**: Provides the model implementations that connect to Vertex AI:\n",
    "   - `VertexAIServerModel`: Base implementation for connecting to Vertex AI endpoints\n",
    "   - `WeaveVertexAIServerModel`: Extends the base model with Weights & Biases Weave tracking for experiment monitoring\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "The agent can be configured with various parameters:\n",
    "- Model selection (Gemini 1.5 Pro, 2.0 Flash, etc.)\n",
    "- Temperature settings for controlling response randomness\n",
    "- Planning interval to determine how often the agent should plan its actions\n",
    "- Maximum steps to limit the complexity of interactions\n",
    "\n",
    "This architecture enables comprehensive evaluation of different model configurations and parameters to determine the optimal setup for customer support scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"[bold blue]Creating customer support agent[/bold blue]\")\n",
    "\n",
    "# Initialize a customer support agent for generating high-quality evaluation data\n",
    "base_agent = create_customer_support_agent(\n",
    "    use_weave=True,                    # Enable Weave for experiment tracking\n",
    "    model_id=\"google/gemini-2.0-flash\", # Use Gemini 2.0 for fast and accurate generation\n",
    "    temperature=0.1,                   # Low temperature for consistent, deterministic outputs\n",
    "    planning_interval=2,               # Plan every 2 steps for better reasoning\n",
    "    max_steps=4                        # Allow up to 4 steps to handle medium-complex queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_agent.run(\"What is the best item in the category of book?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation for Agent Evaluation\n",
    "\n",
    "This section explains how we generate realistic test data to evaluate our customer support agent across different configurations and scenarios.\n",
    "\n",
    "## Why We Need Synthetic Evaluation Data\n",
    "\n",
    "Testing with synthetic data helps us:\n",
    "\n",
    "1. **Compare Models Fairly**: We can test different models (Gemini Pro vs Flash) and settings (temperature values) on the exact same customer queries.\n",
    "\n",
    "2. **Save Time and Resources**: Creating test data is faster and cheaper than collecting real customer conversations.\n",
    "\n",
    "3. **Cover Edge Cases**: We can include challenging scenarios that might be rare in real data but important to test.\n",
    "\n",
    "4. **Ensure Consistent Quality**: By filtering examples that meet quality thresholds, we build a reliable benchmark dataset.\n",
    "\n",
    "## How the Dataset Generator Works\n",
    "\n",
    "The `DatasetGenerator` class creates evaluation data through several steps:\n",
    "\n",
    "1. **Creating Realistic Queries**: Generates e-commerce questions using real product IDs, categories, and customer information:\n",
    "   - \"I'm looking for products in the Books category. What do you have?\"\n",
    "   - \"Can you check the status of my order OD123456?\"\n",
    "   - \"What's your best Electronics product? I need something reliable.\"\n",
    "\n",
    "2. **Recording Agent Behavior**: Runs the agent on these queries and captures:\n",
    "   - Which tools the agent used (ProductSearch, OrderStatus, etc.)\n",
    "   - The arguments passed to each tool\n",
    "   - The agent's reasoning at each step\n",
    "   - The final response to the user\n",
    "\n",
    "3. **Evaluating Quality**: Uses a judge model to score:\n",
    "   - Final response quality (0-1 score)\n",
    "   - Individual step effectiveness (0-1 score per step)\n",
    "   - Overall trajectory coherence (0-1 score)\n",
    "\n",
    "4. **Filtering Results**: Only keeps examples that meet quality thresholds (typically 0.7 for each dimension).\n",
    "\n",
    "## Dataset Structure and Applications\n",
    "\n",
    "Each example in the final dataset includes:\n",
    "- The original user prompt\n",
    "- Expected tool usage sequence\n",
    "- Validation criteria\n",
    "- Difficulty rating (easy, medium, hard)\n",
    "- Metadata about model configuration\n",
    "\n",
    "This dataset lets us:\n",
    "1. Determine which model performs best for customer support\n",
    "2. Find the right temperature settings for different query types\n",
    "3. Measure whether planning capabilities improve results\n",
    "4. Identify specific areas where the agent needs improvement\n",
    "\n",
    "By testing systematically, we can build a more effective customer support agent that handles user queries accurately while using computational resources efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset generator\n",
    "thresholds={\n",
    "            \"final_response\": 0.7,\n",
    "            \"single_step\": 0.7,\n",
    "            \"trajectory\": 0.7\n",
    "        }\n",
    "generator = DatasetGenerator(agent=base_agent, thresholds=thresholds, debug=True)\n",
    "\n",
    "# Generate comprehensive dataset with different scenarios\n",
    "console.print(\"[bold blue]Generating customer support evaluation dataset...[/bold blue]\")\n",
    "\n",
    "dataset = create_customer_support_agent_evaluation_dataset(generator, base_agent, num_prompts=5)  # Adjust number as needed\n",
    "\n",
    "# Save generated dataset\n",
    "dataset_path = \"customer_support_eval.json\"\n",
    "generator.save_dataset(dataset, dataset_path)\n",
    "\n",
    "console.print(f\"[green]✓[/green] Dataset generation complete! Saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation Framework\n",
    "\n",
    "This section explains how we evaluate different model configurations using a comprehensive evaluation framework to identify the best-performing agent setup.\n",
    "\n",
    "## Setting Up the Evaluation Pipeline\n",
    "\n",
    "The evaluation process begins with initializing our evaluation framework defined in `evaluator.py`.\n",
    "The below code loads our previously generated synthetic dataset and formats it for evaluation with Vertex AI.\n",
    "\n",
    "## How the Evaluation Process Works\n",
    "\n",
    "The `AgentEvaluator` class provides a systematic approach to testing agent performance across multiple dimensions:\n",
    "\n",
    "1. **Multi-dimensional Metrics**: We evaluate the agent on several key aspects:\n",
    "   - **Tool Selection Accuracy**: How well the agent chooses appropriate tools\n",
    "   - **Reasoning Quality**: The logical coherence of the agent's thinking process\n",
    "   - **Response Correctness**: Accuracy and completeness of final answers\n",
    "   - **Trajectory Match**: How well the agent's path aligns with expected solutions\n",
    "   - **Coherence**: Overall clarity and consistency of responses\n",
    "\n",
    "2. **Vertex AI Integration**: The evaluator uses Vertex AI's evaluation capabilities to score agent responses objectively, reducing human bias in the assessment process.\n",
    "\n",
    "3. **Weights & Biases Weave Integration**: Results are automatically logged to Weave, enabling:\n",
    "   - Interactive visualization of agent performance\n",
    "   - Comparison between different model configurations\n",
    "   - Tracking of experiments over time\n",
    "   - Sharing results with team members\n",
    "\n",
    "4. **Visualization Tools**: The framework generates charts and tables to help identify patterns:\n",
    "   - Score distribution plots showing performance across metrics\n",
    "   - Difficulty heatmaps revealing how the agent handles easy vs. hard queries\n",
    "   - Correlation analysis between trajectory quality and response accuracy\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "This evaluation framework helps us answer key questions:\n",
    "\n",
    "1. **Which model performs best?** Compare Gemini Pro vs. Flash models on the same test cases.\n",
    "\n",
    "2. **What temperature setting works better?** Test whether low temperature (0.2) or high temperature (0.7) produces better results.\n",
    "\n",
    "3. **Where are the weaknesses?** Identify specific query types or metrics where the agent underperforms.\n",
    "\n",
    "4. **Is planning helpful?** Measure whether enabling planning capabilities improves overall performance.\n",
    "\n",
    "By running all model configurations through this standardized evaluation process, we can make data-driven decisions about which agent setup to deploy for customer support scenarios, balancing performance and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "console.print(\"[bold blue]Initializing evaluator...[/bold blue]\")\n",
    "evaluator = AgentEvaluator(verbosity=1, project=os.getenv(\"VERTEX_PROJECT_ID\"), location=os.getenv(\"VERTEX_LOCATION\"))\n",
    "console.print(f\"[green]✓[/green] Evaluator initialized\")\n",
    "\n",
    "all_examples = load_dataset(\"customer_support_eval.json\")\n",
    "console.print(f\"[bold blue]Formatting dataset with {len(all_examples)} examples for evaluation...[/bold blue]\")\n",
    "eval_dataset = evaluator.format_dataset_for_eval(all_examples)\n",
    "console.print(f\"[green]✓[/green] Dataset formatted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model Evaluations\n",
    "\n",
    "This section shows how we test different model configurations against our evaluation dataset to find the best setup for customer support.\n",
    "\n",
    "## Testing Multiple Configurations\n",
    "\n",
    "We evaluate four different configurations:\n",
    "- Gemini Pro with low temperature (0.2)\n",
    "- Gemini Pro with high temperature (0.7)\n",
    "- Gemini Flash Lite with low temperature (0.2)\n",
    "- Gemini Flash Lite with high temperature (0.7)\n",
    "\n",
    "Each configuration is tested on the same set of customer queries, ensuring a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def evaluate_model_config(config: Dict[str, Any], eval_dataset: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a specific model configuration and return results\"\"\"\n",
    "    console.print(f\"\\n[bold blue]Evaluating {config['name']}...[/bold blue]\")\n",
    "    \n",
    "    # Create agent with this configuration\n",
    "    agent = create_customer_support_agent(\n",
    "        use_weave=True,\n",
    "        model_id=config[\"model_id\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "        planning_interval=config.get(\"planning_interval\", 1),\n",
    "        max_steps=config.get(\"max_steps\", 5)\n",
    "    )\n",
    "    \n",
    "    # Run evaluation with the agent object directly\n",
    "    results = evaluator.run_evaluation(\n",
    "        agent=agent, \n",
    "        eval_dataset=eval_dataset, \n",
    "        output_dir=f\"evaluation_results/{config['name'].replace(' ', '_').lower()}\",\n",
    "        weave_project=WEAVE_PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    # Add configuration details to results\n",
    "    results[\"config\"] = config\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluations for all configurations\n",
    "all_results = []\n",
    "for config in model_configs:\n",
    "    results = evaluate_model_config(config, eval_dataset)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Display summary results\n",
    "    console.print(f\"\\n[bold green]Results for {config['name']}:[/bold green]\")\n",
    "    if \"summary_metrics\" in results and results[\"summary_metrics\"]:\n",
    "        table = evaluator._render_summary_table(results[\"summary_metrics\"])\n",
    "        console.print(table)\n",
    "    else:\n",
    "        console.print(\"[yellow]No summary metrics available[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigate to Weave to compare results!!\n",
    "\n",
    "Go to the project set in `WEAVE_PROJECT_NAME` in `config.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Model Performance Patterns\n",
    "\n",
    "After running evaluations on different model configurations, we analyze the results to uncover key patterns and insights.\n",
    "\n",
    "## Response Quality by Model Type\n",
    "\n",
    "We compare how Gemini Pro and Gemini Flash Lite perform on response quality metrics. The analysis reveals which model provides more accurate and helpful customer support responses across our test cases.\n",
    "\n",
    "## Impact of Temperature Settings\n",
    "\n",
    "By examining how temperature affects performance, we can determine whether lower temperatures (0.2) produce more consistent, reliable responses or if higher temperatures (0.7) generate more helpful, creative solutions for customer queries.\n",
    "\n",
    "## Performance Across Query Difficulty\n",
    "\n",
    "We analyze how different models handle queries of varying complexity:\n",
    "- Which model excels at simple, straightforward questions?\n",
    "- Which configuration best handles complex, multi-part customer issues?\n",
    "- Are there specific difficulty levels where one model significantly outperforms others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models and their impact on response quality\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create a function to extract the base model name (ignoring parentheses)\n",
    "def get_base_model_name(full_name):\n",
    "    # Remove anything in parentheses and trim whitespace\n",
    "    base_name = re.sub(r'\\s*\\([^)]*\\)', '', full_name).strip()\n",
    "    return base_name\n",
    "\n",
    "# Prepare response data\n",
    "response_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    config = result[\"config\"]\n",
    "    detailed_metrics = result[\"detailed_metrics\"]\n",
    "    \n",
    "    # Get base model name\n",
    "    base_model = get_base_model_name(config[\"name\"])\n",
    "    \n",
    "    # Find response quality column\n",
    "    response_col = next((col for col in detailed_metrics.columns if \"response\" in col.lower() and \"score\" in col.lower()), None)\n",
    "    \n",
    "    if response_col and not detailed_metrics.empty:\n",
    "        # Add model info to each row\n",
    "        model_data = detailed_metrics.copy()\n",
    "        model_data[\"Base Model\"] = base_model\n",
    "        model_data[\"Temperature\"] = config[\"temperature\"]\n",
    "        \n",
    "        # Select only needed columns\n",
    "        model_data = model_data[[\"Base Model\", \"Temperature\", response_col]].rename(columns={response_col: \"Response Quality\"})\n",
    "        response_data.append(model_data)\n",
    "\n",
    "# Combine all data\n",
    "if response_data:\n",
    "    combined_data = pd.concat(response_data, ignore_index=True)\n",
    "    \n",
    "    # Create a simple, clean figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a basic boxplot\n",
    "    ax = sns.boxplot(x=\"Base Model\", y=\"Response Quality\", data=combined_data, \n",
    "                    palette=\"pastel\", width=0.6)\n",
    "    \n",
    "    # Simple, clean styling\n",
    "    plt.title(\"Response Quality by Model Type\", fontsize=16)\n",
    "    plt.xlabel(\"Model\", fontsize=14)\n",
    "    plt.ylabel(\"Response Quality Score\", fontsize=14)\n",
    "    \n",
    "    # Add the overall average line\n",
    "    overall_mean = combined_data[\"Response Quality\"].mean()\n",
    "    plt.axhline(y=overall_mean, color='red', linestyle='--', linewidth=1.5)\n",
    "    plt.text(len(combined_data[\"Base Model\"].unique()) - 0.5, overall_mean + 0.05, \n",
    "             f\"Overall Mean: {overall_mean:.2f}\", \n",
    "             ha='right', color='red', fontweight='bold')\n",
    "    \n",
    "    # Add mean values as annotations - now below the boxes\n",
    "    for i, model in enumerate(combined_data[\"Base Model\"].unique()):\n",
    "        model_data = combined_data[combined_data[\"Base Model\"] == model]\n",
    "        mean_score = model_data[\"Response Quality\"].mean()\n",
    "        \n",
    "        # Calculate the bottom of the box\n",
    "        q1 = model_data[\"Response Quality\"].quantile(0.25)\n",
    "        q3 = model_data[\"Response Quality\"].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        bottom = max(model_data[\"Response Quality\"].min(), q1 - 1.5 * iqr)\n",
    "        \n",
    "        # Place text below the box\n",
    "        plt.text(i, bottom - 0.2, f\"Mean: {mean_score:.2f}\", \n",
    "                ha='center', va='top', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Save and show\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_response_quality.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a comprehensive summary table with all metrics by model\n",
    "    console.rule(\"[bold magenta]Model Performance Summary\")\n",
    "    \n",
    "    # Get all metrics from all results\n",
    "    all_metrics = {}\n",
    "    for result in all_results:\n",
    "        config = result[\"config\"]\n",
    "        metrics = result[\"summary_metrics\"]\n",
    "        base_model = get_base_model_name(config[\"name\"])\n",
    "        \n",
    "        if base_model not in all_metrics:\n",
    "            all_metrics[base_model] = {}\n",
    "        \n",
    "        # Add all mean metrics\n",
    "        for metric_key, value in metrics.items():\n",
    "            if \"/Mean\" in metric_key:\n",
    "                # Clean up metric name for display\n",
    "                clean_metric = metric_key.replace(\"/Mean\", \"\").replace(\"_score\", \"\").replace(\"_\", \" \").title()\n",
    "                \n",
    "                if clean_metric not in all_metrics[base_model]:\n",
    "                    all_metrics[base_model][clean_metric] = []\n",
    "                \n",
    "                all_metrics[base_model][clean_metric].append(value)\n",
    "    \n",
    "    # Create a table with all metrics\n",
    "    summary_table = Table(title=\"All Metrics by Model Type\")\n",
    "    \n",
    "    # Add model column\n",
    "    summary_table.add_column(\"Model\", style=\"cyan\")\n",
    "    \n",
    "    # Find all unique metrics\n",
    "    all_metric_names = set()\n",
    "    for model_metrics in all_metrics.values():\n",
    "        all_metric_names.update(model_metrics.keys())\n",
    "    \n",
    "    # Add columns for each metric\n",
    "    for metric in sorted(all_metric_names):\n",
    "        summary_table.add_column(metric, style=\"green\")\n",
    "    \n",
    "    # Add rows for each model\n",
    "    for model, metrics in all_metrics.items():\n",
    "        row_values = [model]\n",
    "        \n",
    "        for metric in sorted(all_metric_names):\n",
    "            if metric in metrics:\n",
    "                # Calculate average if there are multiple values (from different temperatures)\n",
    "                avg_value = sum(metrics[metric]) / len(metrics[metric])\n",
    "                row_values.append(f\"{avg_value:.2f}\")\n",
    "            else:\n",
    "                row_values.append(\"N/A\")\n",
    "        \n",
    "        summary_table.add_row(*row_values)\n",
    "    \n",
    "    console.print(summary_table)\n",
    "else:\n",
    "    console.print(\"[yellow]Warning: No response quality data available for visualization[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by query difficulty\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create a function to extract the base model name (ignoring parentheses)\n",
    "def get_base_model_name(full_name):\n",
    "    # Remove anything in parentheses and trim whitespace\n",
    "    base_name = re.sub(r'\\s*\\([^)]*\\)', '', full_name).strip()\n",
    "    return base_name\n",
    "\n",
    "# Create a dataframe with all detailed metrics and model info\n",
    "all_detailed_metrics = []\n",
    "\n",
    "for result in all_results:\n",
    "    config = result[\"config\"]\n",
    "    detailed_metrics = result[\"detailed_metrics\"].copy()\n",
    "    \n",
    "    # Skip if detailed_metrics is empty\n",
    "    if detailed_metrics.empty:\n",
    "        continue\n",
    "    \n",
    "    # Get base model name and temperature\n",
    "    base_model = get_base_model_name(config[\"name\"])\n",
    "    temperature = config[\"temperature\"]\n",
    "    \n",
    "    # Add model info to detailed metrics\n",
    "    detailed_metrics[\"Base Model\"] = base_model\n",
    "    detailed_metrics[\"Temperature\"] = temperature\n",
    "    detailed_metrics[\"Model Config\"] = f\"{base_model} (T={temperature})\"\n",
    "    \n",
    "    all_detailed_metrics.append(detailed_metrics)\n",
    "\n",
    "# Only proceed if we have data\n",
    "if all_detailed_metrics:\n",
    "    all_metrics_df = pd.concat(all_detailed_metrics, ignore_index=True)\n",
    "    \n",
    "    # Check if difficulty column exists\n",
    "    difficulty_col = next((col for col in all_metrics_df.columns if col.lower() == \"difficulty\"), None)\n",
    "    \n",
    "    # Find response correctness column\n",
    "    response_col = next((col for col in all_metrics_df.columns if \"response\" in col.lower() and \"score\" in col.lower()), None)\n",
    "    \n",
    "    if difficulty_col and response_col:\n",
    "        # Convert difficulty to string to ensure proper categorical plotting\n",
    "        all_metrics_df[difficulty_col] = all_metrics_df[difficulty_col].astype(str)\n",
    "        \n",
    "        # Create a simple, clean figure\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        \n",
    "        # Plot performance by difficulty\n",
    "        ax = sns.boxplot(x=difficulty_col, y=response_col, data=all_metrics_df, \n",
    "                        palette=\"pastel\", width=0.6)\n",
    "        \n",
    "        # Simple, clean styling\n",
    "        plt.title(\"Response Quality by Query Difficulty\", fontsize=16)\n",
    "        plt.xlabel(\"Query Difficulty\", fontsize=14)\n",
    "        plt.ylabel(\"Response Quality Score\", fontsize=14)\n",
    "        \n",
    "        # Add the overall average line\n",
    "        overall_mean = all_metrics_df[response_col].mean()\n",
    "        plt.axhline(y=overall_mean, color='red', linestyle='--', linewidth=1.5)\n",
    "        plt.text(len(all_metrics_df[difficulty_col].unique()) - 0.5, overall_mean + 0.05, \n",
    "                f\"Overall Mean: {overall_mean:.2f}\", \n",
    "                ha='right', color='red', fontweight='bold')\n",
    "        \n",
    "        # Add mean values as annotations below the boxes\n",
    "        for i, difficulty in enumerate(sorted(all_metrics_df[difficulty_col].unique())):\n",
    "            difficulty_data = all_metrics_df[all_metrics_df[difficulty_col] == difficulty]\n",
    "            mean_score = difficulty_data[response_col].mean()\n",
    "            \n",
    "            # Calculate the bottom of the box\n",
    "            q1 = difficulty_data[response_col].quantile(0.25)\n",
    "            q3 = difficulty_data[response_col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            bottom = max(difficulty_data[response_col].min(), q1 - 1.5 * iqr)\n",
    "            \n",
    "            # Place text below the box\n",
    "            plt.text(i, bottom - 0.2, f\"Mean: {mean_score:.2f}\", \n",
    "                    ha='center', va='top', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # Save and show\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"response_quality_by_difficulty.png\", dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a detailed table showing best model for each difficulty level\n",
    "        console.rule(\"[bold magenta]Best Model Configuration by Difficulty\")\n",
    "        \n",
    "        detailed_table = Table(title=\"Best Model Configuration for Each Difficulty Level\")\n",
    "        detailed_table.add_column(\"Difficulty\", style=\"cyan\")\n",
    "        detailed_table.add_column(\"Best Model\", style=\"green\")\n",
    "        detailed_table.add_column(\"Temperature\", style=\"yellow\")\n",
    "        detailed_table.add_column(\"Score\", style=\"magenta\")\n",
    "        detailed_table.add_column(\"Improvement Over Avg\", style=\"blue\")\n",
    "        \n",
    "        # Add rows for each difficulty level\n",
    "        for difficulty in sorted(all_metrics_df[difficulty_col].unique()):\n",
    "            difficulty_data = all_metrics_df[all_metrics_df[difficulty_col] == difficulty]\n",
    "            avg_score = difficulty_data[response_col].mean()\n",
    "            \n",
    "            # Group by model and temperature to find the best configuration\n",
    "            grouped = difficulty_data.groupby([\"Base Model\", \"Temperature\"])[response_col].mean().reset_index()\n",
    "            best_idx = grouped[response_col].idxmax()\n",
    "            best_model = grouped.loc[best_idx, \"Base Model\"]\n",
    "            best_temp = grouped.loc[best_idx, \"Temperature\"]\n",
    "            best_score = grouped.loc[best_idx, response_col]\n",
    "            \n",
    "            # Calculate improvement over average\n",
    "            improvement = best_score - avg_score\n",
    "            improvement_pct = (improvement / avg_score) * 100 if avg_score > 0 else 0\n",
    "            \n",
    "            detailed_table.add_row(\n",
    "                difficulty,\n",
    "                best_model,\n",
    "                f\"{best_temp}\",\n",
    "                f\"{best_score:.2f}\",\n",
    "                f\"+{improvement:.2f} ({improvement_pct:.1f}%)\"\n",
    "            )\n",
    "        \n",
    "        console.print(detailed_table)\n",
    "        \n",
    "        # Create a table showing all metrics by difficulty\n",
    "        console.rule(\"[bold magenta]All Metrics by Difficulty\")\n",
    "        \n",
    "        # Find all score columns\n",
    "        score_columns = [col for col in all_metrics_df.columns \n",
    "                        if \"score\" in col.lower() and col != response_col]\n",
    "        \n",
    "        # Add response column to the list if not already included\n",
    "        if response_col and response_col not in score_columns:\n",
    "            score_columns.append(response_col)\n",
    "        \n",
    "        # Create the table\n",
    "        metrics_by_difficulty_table = Table(title=\"All Metrics by Difficulty Level\")\n",
    "        metrics_by_difficulty_table.add_column(\"Difficulty\", style=\"cyan\")\n",
    "        \n",
    "        # Add columns for each metric\n",
    "        for col in sorted(score_columns):\n",
    "            # Clean up column name for display\n",
    "            clean_name = col.replace(\"_score\", \"\").replace(\"_\", \" \").title()\n",
    "            metrics_by_difficulty_table.add_column(clean_name, style=\"green\")\n",
    "        \n",
    "        # Add rows for each difficulty level\n",
    "        for difficulty in sorted(all_metrics_df[difficulty_col].unique()):\n",
    "            difficulty_data = all_metrics_df[all_metrics_df[difficulty_col] == difficulty]\n",
    "            \n",
    "            row_values = [difficulty]\n",
    "            \n",
    "            for col in sorted(score_columns):\n",
    "                if col in difficulty_data.columns:\n",
    "                    avg_score = difficulty_data[col].mean()\n",
    "                    row_values.append(f\"{avg_score:.2f}\")\n",
    "                else:\n",
    "                    row_values.append(\"N/A\")\n",
    "            \n",
    "            metrics_by_difficulty_table.add_row(*row_values)\n",
    "        \n",
    "        console.print(metrics_by_difficulty_table)\n",
    "    else:\n",
    "        console.print(\"[yellow]Warning: Missing difficulty or response correctness columns in metrics data[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[yellow]Warning: No detailed metrics available for difficulty analysis[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by temperature\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create a function to extract the base model name (ignoring parentheses)\n",
    "def get_base_model_name(full_name):\n",
    "    # Remove anything in parentheses and trim whitespace\n",
    "    base_name = re.sub(r'\\s*\\([^)]*\\)', '', full_name).strip()\n",
    "    return base_name\n",
    "\n",
    "# Create a dataframe with all detailed metrics and model info\n",
    "all_detailed_metrics = []\n",
    "\n",
    "for result in all_results:\n",
    "    config = result[\"config\"]\n",
    "    detailed_metrics = result[\"detailed_metrics\"].copy()\n",
    "    \n",
    "    # Skip if detailed_metrics is empty\n",
    "    if detailed_metrics.empty:\n",
    "        continue\n",
    "    \n",
    "    # Get base model name and temperature\n",
    "    base_model = get_base_model_name(config[\"name\"])\n",
    "    temperature = config[\"temperature\"]\n",
    "    \n",
    "    # Add model info to detailed metrics\n",
    "    detailed_metrics[\"Base Model\"] = base_model\n",
    "    detailed_metrics[\"Temperature\"] = temperature\n",
    "    detailed_metrics[\"Model Config\"] = f\"{base_model} (T={temperature})\"\n",
    "    \n",
    "    all_detailed_metrics.append(detailed_metrics)\n",
    "\n",
    "# Only proceed if we have data\n",
    "if all_detailed_metrics:\n",
    "    all_metrics_df = pd.concat(all_detailed_metrics, ignore_index=True)\n",
    "    \n",
    "    # Find response correctness column\n",
    "    response_col = next((col for col in all_metrics_df.columns if \"response\" in col.lower() and \"score\" in col.lower()), None)\n",
    "    \n",
    "    if \"Temperature\" in all_metrics_df.columns and response_col:\n",
    "        # Convert temperature to string to ensure proper categorical plotting\n",
    "        all_metrics_df[\"Temperature\"] = all_metrics_df[\"Temperature\"].astype(str)\n",
    "        \n",
    "        # Create a simple, clean figure\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        \n",
    "        # Plot performance by temperature\n",
    "        ax = sns.boxplot(x=\"Temperature\", y=response_col, data=all_metrics_df, \n",
    "                        palette=\"pastel\", width=0.6)\n",
    "        \n",
    "        # Simple, clean styling\n",
    "        plt.title(\"Response Quality by Temperature\", fontsize=16)\n",
    "        plt.xlabel(\"Temperature\", fontsize=14)\n",
    "        plt.ylabel(\"Response Quality Score\", fontsize=14)\n",
    "        \n",
    "        # Add the overall average line\n",
    "        overall_mean = all_metrics_df[response_col].mean()\n",
    "        plt.axhline(y=overall_mean, color='red', linestyle='--', linewidth=1.5)\n",
    "        plt.text(len(all_metrics_df[\"Temperature\"].unique()) - 0.5, overall_mean + 0.05, \n",
    "                f\"Overall Mean: {overall_mean:.2f}\", \n",
    "                ha='right', color='red', fontweight='bold')\n",
    "        \n",
    "        # Add mean values as annotations below the boxes\n",
    "        for i, temp in enumerate(sorted(all_metrics_df[\"Temperature\"].unique())):\n",
    "            temp_data = all_metrics_df[all_metrics_df[\"Temperature\"] == temp]\n",
    "            mean_score = temp_data[response_col].mean()\n",
    "            \n",
    "            # Calculate the bottom of the box\n",
    "            q1 = temp_data[response_col].quantile(0.25)\n",
    "            q3 = temp_data[response_col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            bottom = max(temp_data[response_col].min(), q1 - 1.5 * iqr)\n",
    "            \n",
    "            # Place text below the box\n",
    "            plt.text(i, bottom - 0.2, f\"Mean: {mean_score:.2f}\", \n",
    "                    ha='center', va='top', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # Save and show\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"response_quality_by_temperature.png\", dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a detailed table showing best model for each temperature\n",
    "        console.rule(\"[bold magenta]Best Model Configuration by Temperature\")\n",
    "        \n",
    "        detailed_table = Table(title=\"Best Model Configuration for Each Temperature\")\n",
    "        detailed_table.add_column(\"Temperature\", style=\"cyan\")\n",
    "        detailed_table.add_column(\"Best Model\", style=\"green\")\n",
    "        detailed_table.add_column(\"Score\", style=\"magenta\")\n",
    "        detailed_table.add_column(\"Improvement Over Avg\", style=\"blue\")\n",
    "        \n",
    "        # Add rows for each temperature level\n",
    "        for temp in sorted(all_metrics_df[\"Temperature\"].unique()):\n",
    "            temp_data = all_metrics_df[all_metrics_df[\"Temperature\"] == temp]\n",
    "            avg_score = temp_data[response_col].mean()\n",
    "            \n",
    "            # Group by model to find the best configuration\n",
    "            grouped = temp_data.groupby([\"Base Model\"])[response_col].mean().reset_index()\n",
    "            best_idx = grouped[response_col].idxmax()\n",
    "            best_model = grouped.loc[best_idx, \"Base Model\"]\n",
    "            best_score = grouped.loc[best_idx, response_col]\n",
    "            \n",
    "            # Calculate improvement over average\n",
    "            improvement = best_score - avg_score\n",
    "            improvement_pct = (improvement / avg_score) * 100 if avg_score > 0 else 0\n",
    "            \n",
    "            detailed_table.add_row(\n",
    "                temp,\n",
    "                best_model,\n",
    "                f\"{best_score:.2f}\",\n",
    "                f\"+{improvement:.2f} ({improvement_pct:.1f}%)\"\n",
    "            )\n",
    "        \n",
    "        console.print(detailed_table)\n",
    "        \n",
    "        # Create a table showing all metrics by temperature\n",
    "        console.rule(\"[bold magenta]All Metrics by Temperature\")\n",
    "        \n",
    "        # Find all score columns\n",
    "        score_columns = [col for col in all_metrics_df.columns \n",
    "                        if \"score\" in col.lower() and col != response_col]\n",
    "        \n",
    "        # Add response column to the list if not already included\n",
    "        if response_col and response_col not in score_columns:\n",
    "            score_columns.append(response_col)\n",
    "        \n",
    "        # Create the table\n",
    "        metrics_by_temp_table = Table(title=\"All Metrics by Temperature\")\n",
    "        metrics_by_temp_table.add_column(\"Temperature\", style=\"cyan\")\n",
    "        \n",
    "        # Add columns for each metric\n",
    "        for col in sorted(score_columns):\n",
    "            # Clean up column name for display\n",
    "            clean_name = col.replace(\"_score\", \"\").replace(\"_\", \" \").title()\n",
    "            metrics_by_temp_table.add_column(clean_name, style=\"green\")\n",
    "        \n",
    "        # Add rows for each temperature level\n",
    "        for temp in sorted(all_metrics_df[\"Temperature\"].unique()):\n",
    "            temp_data = all_metrics_df[all_metrics_df[\"Temperature\"] == temp]\n",
    "            \n",
    "            row_values = [temp]\n",
    "            \n",
    "            for col in sorted(score_columns):\n",
    "                if col in temp_data.columns:\n",
    "                    avg_score = temp_data[col].mean()\n",
    "                    row_values.append(f\"{avg_score:.2f}\")\n",
    "                else:\n",
    "                    row_values.append(\"N/A\")\n",
    "            \n",
    "            metrics_by_temp_table.add_row(*row_values)\n",
    "        \n",
    "        console.print(metrics_by_temp_table)\n",
    "    else:\n",
    "        console.print(\"[yellow]Warning: Missing temperature or response correctness columns in metrics data[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[yellow]Warning: No detailed metrics available for temperature analysis[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Our systematic evaluation of different model configurations provides a framework for making data-driven decisions about customer support agent implementation.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "With the evaluation results in hand, we can now:\n",
    "\n",
    "1. **Select the optimal configuration** based on actual performance metrics rather than assumptions or theoretical capabilities.\n",
    "\n",
    "2. **Understand trade-offs** between different models (Gemini Pro vs. Flash Lite) and temperature settings (0.2 vs. 0.7) for our specific customer support scenarios.\n",
    "\n",
    "3. **Identify improvement areas** by focusing on the metrics where even our best configuration underperformed.\n",
    "\n",
    "4. **Expand our evaluation dataset** to include more diverse customer queries and edge cases.\n",
    "\n",
    "This evaluation framework allows us to continually refine our customer support agent as new models become available or as customer needs evolve. By measuring performance objectively across multiple dimensions, we can ensure we're deploying the most effective solution for our specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.rule(\"[bold magenta]Evaluation Summary\")\n",
    "\n",
    "# First create the comparison dataframe if it doesn't exist\n",
    "comparison_data = []\n",
    "for result in all_results:\n",
    "    config = result[\"config\"]\n",
    "    metrics = result[\"summary_metrics\"]\n",
    "    \n",
    "    row = {\n",
    "        \"Model\": config[\"name\"],\n",
    "        \"Model ID\": config[\"model_id\"],\n",
    "        \"Temperature\": config[\"temperature\"]\n",
    "    }\n",
    "    \n",
    "    # Add only mean values of metrics for cleaner comparison\n",
    "    for metric_key, value in metrics.items():\n",
    "        if \"/Mean\" in metric_key:\n",
    "            # Clean up metric name for display\n",
    "            clean_metric = metric_key.replace(\"/Mean\", \"\").replace(\"_score\", \"\").replace(\"_\", \" \").title()\n",
    "            row[clean_metric] = value\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Get metric columns (excluding metadata columns)\n",
    "metric_columns = [col for col in comparison_df.columns \n",
    "                 if col not in [\"Model\", \"Model ID\", \"Temperature\"]]\n",
    "\n",
    "# Calculate best model for each metric\n",
    "best_models = {}\n",
    "worst_models = {}\n",
    "for metric in metric_columns:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    worst_idx = comparison_df[metric].idxmin()\n",
    "    best_models[metric] = {\n",
    "        \"model\": comparison_df.loc[best_idx, \"Model\"],\n",
    "        \"score\": comparison_df.loc[best_idx, metric],\n",
    "        \"temperature\": comparison_df.loc[best_idx, \"Temperature\"]\n",
    "    }\n",
    "    worst_models[metric] = {\n",
    "        \"model\": comparison_df.loc[worst_idx, \"Model\"],\n",
    "        \"score\": comparison_df.loc[worst_idx, metric]\n",
    "    }\n",
    "\n",
    "# Create summary table\n",
    "summary_table = Table(title=\"Evaluation Summary\")\n",
    "summary_table.add_column(\"Metric\", style=\"cyan\")\n",
    "summary_table.add_column(\"Best Model\", style=\"green\")\n",
    "summary_table.add_column(\"Score\", style=\"yellow\")\n",
    "summary_table.add_column(\"Temperature\", style=\"magenta\")\n",
    "\n",
    "for metric, data in best_models.items():\n",
    "    summary_table.add_row(\n",
    "        metric, \n",
    "        data[\"model\"], \n",
    "        f\"{data['score']:.2f}\",\n",
    "        f\"{data['temperature']}\"\n",
    "    )\n",
    "\n",
    "console.print(summary_table)\n",
    "\n",
    "# Calculate average scores by model\n",
    "model_avg_scores = {}\n",
    "for _, row in comparison_df.iterrows():\n",
    "    model = row[\"Model\"]\n",
    "    scores = [row[m] for m in metric_columns]\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    model_avg_scores[model] = avg_score\n",
    "\n",
    "# Find overall best model\n",
    "overall_best_model = max(model_avg_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "# Find best temperature if we have temperature data\n",
    "temp_analysis = {}\n",
    "if \"Temperature\" in comparison_df.columns:\n",
    "    for temp in comparison_df[\"Temperature\"].unique():\n",
    "        temp_df = comparison_df[comparison_df[\"Temperature\"] == temp]\n",
    "        if not temp_df.empty:\n",
    "            avg_scores = [temp_df[m].mean() for m in metric_columns]\n",
    "            temp_analysis[temp] = sum(avg_scores) / len(avg_scores)\n",
    "    \n",
    "    if temp_analysis:  # Check if we have any temperature data\n",
    "        best_temp = max(temp_analysis.items(), key=lambda x: x[1])\n",
    "        worst_temp = min(temp_analysis.items(), key=lambda x: x[1])\n",
    "\n",
    "# Find strongest and weakest metrics\n",
    "metric_avgs = {metric: comparison_df[metric].mean() for metric in metric_columns}\n",
    "strongest_metric = max(metric_avgs.items(), key=lambda x: x[1])\n",
    "weakest_metric = min(metric_avgs.items(), key=lambda x: x[1])\n",
    "\n",
    "# Generate dynamic key findings\n",
    "key_findings = f\"\"\"\n",
    "[bold]Key Findings:[/bold]\n",
    "\n",
    "1. [bold cyan]Overall Performance:[/bold cyan] {overall_best_model[0]} is the best performing model with an average score of {overall_best_model[1]:.2f} across all metrics.\n",
    "\n",
    "2. [bold cyan]Metric-Specific Performance:[/bold cyan]\n",
    "   - Best metric: {strongest_metric[0]} (avg: {strongest_metric[1]:.2f})\n",
    "   - Most challenging metric: {weakest_metric[0]} (avg: {weakest_metric[1]:.2f})\n",
    "\"\"\"\n",
    "\n",
    "# Add temperature findings if available\n",
    "if \"Temperature\" in comparison_df.columns and temp_analysis:\n",
    "    key_findings += f\"\"\"\n",
    "3. [bold cyan]Temperature Impact:[/bold cyan] Temperature {best_temp[0]} yielded the best overall performance (avg: {best_temp[1]:.2f}), while temperature {worst_temp[0]} performed worst (avg: {worst_temp[1]:.2f}).\n",
    "\"\"\"\n",
    "\n",
    "# Add model-specific insights\n",
    "key_findings += \"\"\"\n",
    "4. [bold cyan]Model-Specific Insights:[/bold cyan]\n",
    "\"\"\"\n",
    "\n",
    "for metric in metric_columns:\n",
    "    best = best_models[metric]\n",
    "    worst = worst_models[metric]\n",
    "    key_findings += f\"   - {metric}: {best['model']} excels ({best['score']:.2f}), while {worst['model']} struggles ({worst['score']:.2f}).\\n\"\n",
    "\n",
    "# Add next steps\n",
    "key_findings += \"\"\"\n",
    "[bold]Next Steps:[/bold]\n",
    "\n",
    "1. Deploy the best-performing model configuration ({}) for production use\n",
    "2. Focus on improving performance in the weakest metric area: {}\n",
    "3. {}\n",
    "4. Expand the evaluation dataset with more diverse scenarios\n",
    "5. Implement continuous evaluation to monitor agent performance over time\n",
    "\"\"\".format(\n",
    "    overall_best_model[0],\n",
    "    weakest_metric[0],\n",
    "    f\"Conduct further temperature tuning around the optimal value of {best_temp[0]}\" if \"Temperature\" in comparison_df.columns and temp_analysis else \"Experiment with different temperature settings to optimize performance\"\n",
    ")\n",
    "\n",
    "console.print(key_findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
