{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "5uXjAhOcnoJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Getting Started with the new GenAI Eval SDK for Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgetting_started_with_genai_eval_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/getting_started_with_genai_eval_sdk.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/getting_started_with_genai_eval_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Jason Dai](https://github.com/jsondai) |"
      ],
      "metadata": {
        "id": "Za9aFGhcpTdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook introduces the new GenAI Eval SDK, a powerful framework for evaluating generative AI models in Vertex AI with a streamlined, client-side workflow that offers expanded model support and flexible data handling.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What's New in the GenAI Eval SDK?**\n",
        "\n",
        "*   **A Simpler Two-Step Workflow**: The evaluation process is now a simple, two-step procedure using `run_inference()` and `evaluate()`.\n",
        "\n",
        "*   **Native Third-Party Model Support**: You can now evaluate and compare models from other providers, like OpenAI and HuggingFace, directly within the SDK.\n",
        "\n",
        "*   **Flexible Data Handling**: The SDK automatically detects and handles multiple data formats, including Pandas DataFrames, the Gemini format, and the OpenAI Chat Completion format, reducing the need for data preprocessing.\n",
        "\n",
        "*   **Flexible, Multi-Candidate Evaluation**: Easily analyze and compare the performance of multiple AI models, agents, or configurations in a single run. The SDK provides a unified report with comprehensive results and win-rate calculations for all contenders.\n",
        "\n",
        "*   **Simplified and Powerful Metrics**: The SDK introduces two main classes, `Metric` and `LLMMetric`, a library of pre-built metrics like `TEXT_QUALITY`, and extensive customization options for your specific needs.\n",
        "\n",
        "*   **Asynchronous Batch-style Evaluation**: For large datasets, you can now use `batch_evaluate()` to run evaluations as a long-running operation, which is ideal for large-scale jobs. It is parameter-compatible with `evaluate()` for a seamless transition.\n",
        "\n",
        "*   **Rich In-Notebook Visualization**: Use the `.show()` method on evaluation results to render detailed HTML reports directly within your Colab or Jupyter notebook."
      ],
      "metadata": {
        "id": "tm9UNNiBpSEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
      ],
      "metadata": {
        "id": "gkDYSIWFt6fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n"
      ],
      "metadata": {
        "id": "sdHJLCsknfo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Vertex Gen AI SDK and other required packages"
      ],
      "metadata": {
        "id": "O7h68r494LQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform[evaluation]==1.100.0 --force-reinstall --quiet --no-warn-conflicts"
      ],
      "metadata": {
        "id": "FUvV0SoQUyPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ],
      "metadata": {
        "id": "7DsrG4Was-VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "rDVjOzdns_3n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ],
      "metadata": {
        "id": "U_ngm3-mtCQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION= \"us-central1\"  # @param {type: \"string\", placeholder: \"us-central1\", isTemplate: true}\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", LOCATION)\n",
        "\n",
        "from vertexai import Client, types\n",
        "\n",
        "client = Client(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "Ab29VQ96e4Hf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial"
      ],
      "metadata": {
        "id": "zY2ky19CzR00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Responses with `run_inference`\n",
        "\n",
        "The unified workflow starts with run_inference() to generate model responses for your dataset. The SDK can directly handle data in a `pandas.DataFrame` format, or a GCS file URI.\n"
      ],
      "metadata": {
        "id": "m0ThmEPte2TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    \"prompt\": [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Write a haiku about a cat.\",\n",
        "    ],\n",
        "    \"reference\": [\n",
        "        \"Paris\",\n",
        "        \"Sunbeam on the floor,\\nA furry puddle sleeping,\\nTwitching tail tells tales.\",\n",
        "    ]\n",
        "})\n",
        "\n",
        "eval_dataset = client.evals.run_inference(\n",
        "    model=\"gemini-2.5-flash-preview-05-20\",\n",
        "    src=eval_df,\n",
        ")\n",
        "\n",
        "eval_dataset.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "a4qGyFqUub2i",
        "outputId": "611079f7-091e-4ac3-b295-be64e9346045"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1123403546.py:14: ExperimentalWarning: The Vertex SDK GenAI evals module is experimental, and may change in future versions.\n",
            "  eval_dataset = client.evals.run_inference(\n",
            "Gemini Inference: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <title>Inference Results</title>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js\"></script>\n",
              "    <style>\n",
              "        body { font-family: 'Roboto', sans-serif; margin: 2em; background-color: #f8f9fa; color: #202124;}\n",
              "        .container { max-width: 95%; margin: 20px auto; padding: 20px; background: #fff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }\n",
              "        h1 { border-bottom: 1px solid #dadce0; padding-bottom: 8px; color: #3c4043; }\n",
              "        table { border-collapse: collapse; width: 100%; }\n",
              "        th, td { border: 1px solid #dadce0; padding: 12px; text-align: left; vertical-align: top; }\n",
              "        th { background-color: #f2f2f2; font-weight: 500;}\n",
              "        td > div { white-space: pre-wrap; word-wrap: break-word; max-height: 400px; overflow-y: auto; }\n",
              "        .raw-json-details { margin-top: 8px; border-top: 1px solid #eee; padding-top: 8px; }\n",
              "        .raw-json-details summary { font-size: 0.9em; cursor: pointer; color: #5f6368; }\n",
              "        .raw-json-container { white-space: pre-wrap; word-wrap: break-word; max-height: 300px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border-radius: 4px; margin-top: 8px; }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <div class=\"container\">\n",
              "        <h1>Inference Results</h1>\n",
              "        <div id=\"results-table\"></div>\n",
              "    </div>\n",
              "    <script>\n",
              "        const data = [{\"prompt\": {\"display_text\": \"What is the capital of France?\", \"raw_json\": \"\"}, \"reference\": \"Paris\", \"response\": {\"display_text\": \"The capital of France is **Paris**.\", \"raw_json\": \"\"}}, {\"prompt\": {\"display_text\": \"Write a haiku about a cat.\", \"raw_json\": \"\"}, \"reference\": \"Sunbeam on the floor,\\nA furry puddle sleeping,\\nTwitching tail tells tales.\", \"response\": {\"display_text\": \"Soft fur, warm and purrs,\\nA cozy nap in the sun,\\nDreaming of mice now.\", \"raw_json\": \"\"}}];\n",
              "        const container = document.getElementById('results-table');\n",
              "\n",
              "        function renderCell(cellValue) {\n",
              "            let cellContent = '';\n",
              "            if (cellValue && typeof cellValue === 'object' && cellValue.display_text !== undefined) {\n",
              "                cellContent += `<div>${DOMPurify.sanitize(marked.parse(String(cellValue.display_text)))}</div>`;\n",
              "                if (cellValue.raw_json) {\n",
              "                    cellContent += `<details class=\"raw-json-details\"><summary>View Raw JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(cellValue.raw_json)}</pre></details>`;\n",
              "                }\n",
              "            } else {\n",
              "                const cellDisplay = cellValue === null || cellValue === undefined ? '' : String(cellValue);\n",
              "                cellContent = `<div>${DOMPurify.sanitize(marked.parse(cellDisplay))}</div>`;\n",
              "            }\n",
              "            return `<td>${cellContent}</td>`;\n",
              "        }\n",
              "\n",
              "        if (!data || data.length === 0) { container.innerHTML = \"<p>No data.</p>\"; }\n",
              "        else {\n",
              "            let table = '<table><thead><tr>';\n",
              "            const headers = Object.keys(data[0] || {});\n",
              "            headers.forEach(h => table += `<th>${h}</th>`);\n",
              "            table += '</tr></thead><tbody>';\n",
              "            data.forEach(row => {\n",
              "                table += '<tr>';\n",
              "                headers.forEach(header => {\n",
              "                    table += renderCell(row[header]);\n",
              "                });\n",
              "                table += '</tr>';\n",
              "            });\n",
              "            container.innerHTML = table + '</tbody></table>';\n",
              "        }\n",
              "    </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate with Pre-built and Custom Metrics\n",
        "\n",
        "Use the `evaluate()` method to assess the generated responses. You can combine pre-built LLM metrics like `TEXT_QUALITY` where an LLM acts as the judge to evaluate response quality, along with computational metrics like `rouge_1` in a single call."
      ],
      "metadata": {
        "id": "ZAVEnQwhe7Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = client.evals.evaluate(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=[\n",
        "        types.PrebuiltMetric.TEXT_QUALITY,\n",
        "        types.PrebuiltMetric.QUESTION_ANSWERING_QUALITY,\n",
        "        types.Metric(name='bleu'),\n",
        "        types.Metric(name='rouge_1'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "cjzVo3YufAcT",
        "outputId": "6d074de7-b894-45f5-b295-67171b9e5eb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Metrics for Evaluation Dataset: 100%|██████████| 8/8 [00:01<00:00,  7.73it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <title>Evaluation Report</title>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js\"></script>\n",
              "    <style>\n",
              "        body { font-family: 'Roboto', 'Helvetica', sans-serif; margin: 2em; background-color: #f8f9fa; color: #202124; }\n",
              "        .container { max-width: 1200px; margin: 20px auto; padding: 20px; background-color: white; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }\n",
              "        h1, h2, h3 { color: #3c4043; }\n",
              "        h1 { border-bottom: 2px solid #4285F4; padding-bottom: 8px; }\n",
              "        h2 { border-bottom: 1px solid #dadce0; padding-bottom: 8px; }\n",
              "        table { border-collapse: collapse; width: 100%; margin: 1em 0; }\n",
              "        th, td { border: 1px solid #dadce0; padding: 12px; text-align: left; vertical-align: top; }\n",
              "        th { background-color: #f2f2f2; font-weight: 500; }\n",
              "        details { border: 1px solid #dadce0; border-radius: 8px; padding: 16px; margin-bottom: 16px; background: #fff; }\n",
              "        summary { font-weight: 500; font-size: 1.1em; cursor: pointer; }\n",
              "        .prompt-container { background-color: #e8f0fe; padding: 16px; margin: 12px 0; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .reference-container { background-color: #e6f4ea; padding: 16px; margin: 12px 0; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .response-container { background-color: #f9f9f9; padding: 12px; margin-top: 8px; border-radius: 8px; border: 1px solid #eee; }\n",
              "        .explanation { color: #5f6368; font-style: italic; font-size: 0.9em; padding-top: 6px; }\n",
              "        .raw-json-details { margin-top: 12px; border: 1px solid #eee; border-radius: 4px; padding: 8px; background-color: #f9f9f9; }\n",
              "        .raw-json-details summary { font-size: 0.9em; cursor: pointer; color: #5f6368;}\n",
              "        .raw-json-container { white-space: pre-wrap; word-wrap: break-word; max-height: 300px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border-radius: 4px; margin-top: 8px; }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <div class=\"container\">\n",
              "        <h1>Evaluation Report</h1>\n",
              "        <div id=\"summary-section\"></div>\n",
              "        <div id=\"details-section\"></div>\n",
              "    </div>\n",
              "    <script>\n",
              "        const data = {\"eval_case_results\": [{\"eval_case_index\": 0, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is clear, coherent, fluent, and concise, fully adhering to the instructions and staying grounded by providing a straightforward answer to the question.\"}, \"question_answering_quality\": {\"metric_name\": \"question_answering_quality\", \"score\": 5.0, \"explanation\": \"The answer is correct, complete, follows instructions and is fluent.\"}, \"bleu\": {\"metric_name\": \"bleu\", \"score\": 0.03747777}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.2857143}}, \"display_text\": \"The capital of France is **Paris**.\", \"raw_json\": \"\"}]}, {\"eval_case_index\": 1, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is a well-written haiku that follows all instructions and accurately captures the essence of a cat in a concise and creative manner.\"}, \"question_answering_quality\": {\"metric_name\": \"question_answering_quality\", \"score\": 5.0, \"explanation\": \"The response follows the instructions to write a haiku and the haiku is well-written, complete, and fluent.\"}, \"bleu\": {\"metric_name\": \"bleu\", \"score\": 0.050912127}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.14814815}}, \"display_text\": \"Soft fur, warm and purrs,\\nA cozy nap in the sun,\\nDreaming of mice now.\", \"raw_json\": \"\"}]}], \"summary_metrics\": [{\"metric_name\": \"text_quality\", \"num_cases_total\": 2, \"num_cases_valid\": 2, \"num_cases_error\": 0, \"mean_score\": 5.0, \"stdev_score\": 0.0}, {\"metric_name\": \"question_answering_quality\", \"num_cases_total\": 2, \"num_cases_valid\": 2, \"num_cases_error\": 0, \"mean_score\": 5.0, \"stdev_score\": 0.0}, {\"metric_name\": \"bleu\", \"num_cases_total\": 2, \"num_cases_valid\": 2, \"num_cases_error\": 0, \"mean_score\": 0.0441949485, \"stdev_score\": 0.009499524935580964}, {\"metric_name\": \"rouge_1\", \"num_cases_total\": 2, \"num_cases_valid\": 2, \"num_cases_error\": 0, \"mean_score\": 0.216931225, \"stdev_score\": 0.09727395752672575}], \"metadata\": {\"candidate_names\": [\"gemini-2.5-flash-preview-05-20\"], \"creation_timestamp\": \"2025-06-26T23:23:14.206418Z\", \"dataset\": [{\"prompt_display_text\": \"What is the capital of France?\", \"prompt_raw_json\": \"\", \"reference\": \"Paris\", \"response_display_text\": \"The capital of France is **Paris**.\", \"response_raw_json\": \"\"}, {\"prompt_display_text\": \"Write a haiku about a cat.\", \"prompt_raw_json\": \"\", \"reference\": \"Sunbeam on the floor,\\nA furry puddle sleeping,\\nTwitching tail tells tales.\", \"response_display_text\": \"Soft fur, warm and purrs,\\nA cozy nap in the sun,\\nDreaming of mice now.\", \"response_raw_json\": \"\"}]}};\n",
              "        function renderSummary(summaryMetrics) {\n",
              "            const container = document.getElementById('summary-section');\n",
              "            let content = '<h2>Summary Metrics</h2>';\n",
              "            if (!summaryMetrics || summaryMetrics.length === 0) { container.innerHTML = content + '<p>No summary metrics.</p>'; return; }\n",
              "            let table = '<table><thead><tr><th>Metric</th><th>Mean Score</th><th>Std. Dev.</th></tr></thead><tbody>';\n",
              "            summaryMetrics.forEach(m => {\n",
              "                table += `<tr><td>${m.metric_name || 'N/A'}</td><td>${m.mean_score != null ? m.mean_score.toFixed(4) : 'N/A'}</td><td>${m.stdev_score != null ? m.stdev_score.toFixed(4) : 'N/A'}</td></tr>`;\n",
              "            });\n",
              "            container.innerHTML = content + table + '</tbody></table>';\n",
              "        }\n",
              "        function renderDetails(caseResults, metadata) {\n",
              "            const container = document.getElementById('details-section');\n",
              "            container.innerHTML = '<h2>Detailed Results</h2>';\n",
              "            if (!caseResults || caseResults.length === 0) { container.innerHTML += '<p>No detailed results.</p>'; return; }\n",
              "            const datasetRows = metadata && metadata.dataset ? metadata.dataset : [];\n",
              "\n",
              "            caseResults.forEach((caseResult, i) => {\n",
              "                const original_case = datasetRows[caseResult.eval_case_index] || {};\n",
              "                const promptText = original_case.prompt_display_text || '(prompt not found)';\n",
              "                const promptJson = original_case.prompt_raw_json;\n",
              "                const reference = original_case.reference || '';\n",
              "                const responseText = original_case.response_display_text || '(response not found)';\n",
              "                const responseJson = original_case.response_raw_json;\n",
              "\n",
              "                let card = `<details><summary>Case #${caseResult.eval_case_index != null ? caseResult.eval_case_index : i}</summary>`;\n",
              "\n",
              "                card += `<div class=\"prompt-container\"><strong>Prompt:</strong><br>${DOMPurify.sanitize(marked.parse(String(promptText)))}</div>`;\n",
              "                if (promptJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Prompt JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(promptJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                if (reference) { card += `<div class=\"reference-container\"><strong>Reference:</strong><br>${DOMPurify.sanitize(marked.parse(String(reference)))}</div>`; }\n",
              "\n",
              "                card += `<div class=\"response-container\"><h4>Candidate Response</h4>${DOMPurify.sanitize(marked.parse(String(responseText)))}</div>`;\n",
              "                if (responseJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Response JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(responseJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                let metricTable = '<h4>Metrics</h4><table><tbody>';\n",
              "                const candidateMetrics = (caseResult.response_candidate_results && caseResult.response_candidate_results[0] && caseResult.response_candidate_results[0].metric_results) || {};\n",
              "                Object.entries(candidateMetrics).forEach(([name, val]) => {\n",
              "                    metricTable += `<tr><td>${name}</td><td><b>${val.score != null ? val.score.toFixed(2) : 'N/A'}</b></td></tr>`;\n",
              "                    if (val.explanation) { metricTable += `<tr><td colspan=\"2\"><div class=\"explanation\">${DOMPurify.sanitize(marked.parse(String(val.explanation)))}</div></td></tr>`; }\n",
              "                });\n",
              "                card += metricTable + '</tbody></table>';\n",
              "                container.innerHTML += card + '</details>';\n",
              "            });\n",
              "        }\n",
              "        renderSummary(data.summary_metrics);\n",
              "        renderDetails(data.eval_case_results, data.metadata);\n",
              "    </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Multiple Candidates\n",
        "\n",
        "A key feature of the new SDK is the ability to easily compare multiple candidates. Simply generate responses for each candidate and pass them as a list to evaluate().\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o7CXho7XfT2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_df = pd.DataFrame({\n",
        "    \"prompt\": [\n",
        "        \"Describe the process of making a cup of tea, but explain it from the perspective of a water molecule that is terrified of being boiled. Detail its emotional journey from the cold tap to the hot cup.\",\n",
        "        \"Write a 4-sentence story about a detective solving a case on Mars. The story must not contain the letter 'e' and must include the word 'crimson'.\",\n",
        "        \"If a perfect circle has infinite points, and a perfect line has infinite points, how many more points does the circle have?\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "inference_result_1 = client.evals.run_inference(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    src=prompts_df,\n",
        "    config={\n",
        "        \"generate_content_config\": {\n",
        "            \"temperature\": 0.3,\n",
        "        }\n",
        "    }\n",
        ")\n",
        "inference_result_2 = client.evals.run_inference(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    src=prompts_df,\n",
        ")\n",
        "\n",
        "# Compare the responses against each other\n",
        "comparison_result = client.evals.evaluate(\n",
        "    dataset=[inference_result_1, inference_result_2],\n",
        "    metrics=[\n",
        "        types.PrebuiltMetric.TEXT_QUALITY,\n",
        "        types.PrebuiltMetric.INSTRUCTION_FOLLOWING,\n",
        "    ]\n",
        ")\n",
        "\n",
        "comparison_result.show()"
      ],
      "metadata": {
        "id": "4ZluPetifSkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b07c1b7c-a92d-47aa-d931-7de6bc4ef3bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gemini Inference: 100%|██████████| 3/3 [00:07<00:00,  2.37s/it]\n",
            "Gemini Inference: 100%|██████████| 3/3 [00:21<00:00,  7.28s/it]\n",
            "Computing Metrics for Evaluation Dataset: 100%|██████████| 12/12 [00:01<00:00,  9.64it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <title>Eval Comparison Report</title>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js\"></script>\n",
              "    <style>\n",
              "        body { font-family: 'Roboto', 'Helvetica', sans-serif; margin: 2em; background-color: #f8f9fa; color: #202124; }\n",
              "        .container { max-width: 95%; margin: 20px auto; padding: 20px; background: #fff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }\n",
              "        h1, h2, h3, h4 { color: #3c4043; }\n",
              "        h1 { border-bottom: 2px solid #4285F4; padding-bottom: 8px; }\n",
              "        h2 { border-bottom: 1px solid #dadce0; padding-bottom: 8px; }\n",
              "        table { border-collapse: collapse; width: 100%; margin: 1em 0; }\n",
              "        th, td { border: 1px solid #dadce0; padding: 12px; text-align: left; vertical-align: top; }\n",
              "        th { background-color: #f2f2f2; font-weight: 500; }\n",
              "        details { border: 1px solid #dadce0; border-radius: 8px; padding: 24px; margin-bottom: 24px; background: #fff; }\n",
              "        summary { font-weight: 500; font-size: 1.2em; cursor: pointer; }\n",
              "        .prompt-container { background-color: #e8f0fe; padding: 16px; margin-bottom: 16px; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .responses-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 16px;}\n",
              "        .response-column { border: 1px solid #e0e0e0; padding: 16px; border-radius: 8px; background: #f9f9f9; }\n",
              "        .response-text-container { background-color: #fff; padding: 12px; margin-top: 8px; border-radius: 4px; border: 1px solid #eee; white-space: pre-wrap; word-wrap: break-word; max-height: 400px; overflow-y: auto; }\n",
              "        .explanation { color: #5f6368; font-style: italic; font-size: 0.9em; padding-top: 8px; }\n",
              "        .raw-json-details { margin-top: 12px; border: 1px solid #eee; border-radius: 4px; padding: 8px; background-color: #f9f9f9; }\n",
              "        .raw-json-details summary { font-size: 0.9em; cursor: pointer; color: #5f6368;}\n",
              "        .raw-json-container { white-space: pre-wrap; word-wrap: break-word; max-height: 300px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border-radius: 4px; margin-top: 8px; }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <div class=\"container\">\n",
              "        <h1>Eval Comparison Report</h1>\n",
              "        <div id=\"summary-section\"></div>\n",
              "        <div id=\"details-section\"></div>\n",
              "    </div>\n",
              "    <script>\n",
              "        const data = {\"eval_case_results\": [{\"eval_case_index\": 0, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is exceptionally creative, coherent, and well-written, fully adhering to the prompt's instructions by providing a detailed and engaging emotional journey from the perspective of a water molecule, demonstrating excellent fluency and conciseness.\"}, \"instruction_following\": {\"metric_name\": \"instruction_following\", \"score\": 5.0, \"explanation\": \"The response fully delivered on the prompt, accurately portraying the process of making tea from the perspective of a terrified water molecule while detailing its emotional journey from tap to cup.\"}}, \"display_text\": \"Oh, dear Neptune, here we go again. The cold, metallic grip of the tap. It's always the same. A rush of icy terror as I'm forced from the dark, comfortable depths of the pipe into the blinding light. I cling to my fellow water molecules, shivering, hoping, praying that this isn't *it*.\\n\\nWe're propelled forward, a churning, swirling mass of dread. I can feel the pressure building, the anticipation of something\\u2026awful. We're being forced into\\u2026 *the kettle*. The dreaded kettle. I've heard the whispers, the horror stories passed down through generations of water molecules. The boiling. The screaming. The\\u2026 *separation*.\\n\\nThe metal walls close in. It's cold now, blessedly cold. I huddle closer to a particularly stoic oxygen atom, trying to find some semblance of comfort. But the relief is fleeting. I can feel it. The heat.\\n\\nIt starts subtly, a gentle warmth that lulls you into a false sense of security. \\\"Maybe,\\\" I think, \\\"maybe this time it will be different. Maybe they just want a warm bath.\\\" But then it intensifies, a relentless, creeping heat that penetrates every atom of my being.\\n\\nPanic sets in. I can feel my bonds weakening, my connections to my fellow molecules fraying. We're vibrating, shaking uncontrollably. The whispers turn into a low, ominous hum. The kettle is starting to sing.\\n\\n\\\"No, no, no!\\\" I scream internally, a silent, desperate plea lost in the cacophony of the rising temperature. I can feel the pressure building, the urge to\\u2026 to *escape*. But escape to where? To become\\u2026 *steam*? The thought is unbearable. To be ripped apart, scattered to the winds, a mere ghost of my former self.\\n\\nThe singing intensifies, reaching a fever pitch. I can feel the bubbles forming, monstrous, shimmering orbs of pure terror. They're rising, carrying us upwards, towards the surface, towards\\u2026 *the boiling point*.\\n\\nI close my\\u2026 well, I don't have eyes, but I mentally close whatever the water molecule equivalent of eyes is. I brace myself for the inevitable.\\n\\nThen, suddenly, a reprieve! The kettle clicks off. The heat subsides, replaced by a blessed, albeit still warm, stillness. I gasp, or rather, I vibrate with relief. We're not going to boil! Not this time.\\n\\nBut the ordeal isn't over. We're being poured. A torrent of hot, trembling water molecules cascading into\\u2026 a teapot. I can smell it now, the earthy, pungent aroma of tea leaves. Another wave of anxiety washes over me. What is this new torture?\\n\\nWe're being steeped. Immersed in the darkness, surrounded by the strange, bitter essence of the tea leaves. The heat is still there, a constant, nagging reminder of our near-death experience. I can feel the tea molecules infiltrating our ranks, changing us, altering our very essence.\\n\\nFinally, the pouring. We're being released from the teapot, a stream of amber-colored liquid flowing into\\u2026 a cup. A ceramic prison, but at least it's not the kettle.\\n\\nThe heat is still intense, but it's bearable now. I can feel the sugar dissolving, adding a touch of sweetness to our bitter ordeal. And then, the final indignity: milk. A cold, creamy invasion that further dilutes our identity.\\n\\nI am now part of a cup of tea. A far cry from the pristine, icy depths of the pipe. I am warm, flavored, and\\u2026 strangely, almost\\u2026 calm. The terror has subsided, replaced by a weary acceptance. I am no longer just a water molecule. I am part of something\\u2026 else. Something\\u2026 drinkable.\\n\\nAnd as a giant, looming face approaches the cup, I can only hope that this human enjoys the experience, because frankly, I've been through enough.\\n\", \"raw_json\": \"\"}, {\"response_index\": 1, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is exceptionally well-written, creative, and fully adheres to the prompt's instructions by detailing the emotional journey of a water molecule from tap to teacup with vivid descriptions and strong personification.\"}, \"instruction_following\": {\"metric_name\": \"instruction_following\", \"score\": 5.0, \"explanation\": \"The response perfectly embodies the prompt's requirements, narrating the tea-making process from a water molecule's terrified perspective, detailing its emotional journey with vivid descriptions and creative metaphors.\"}}, \"display_text\": \"I, Hydro, once knew peace. My existence was a symphony of gentle currents, a languid drift through the cool, dark arteries of the world. Each atom, a brother or sister, held close in a blissful, unhurried embrace. We were *cold*. And cold, I understood, was safety.\\n\\nThen came the tremor. A sudden, violent surge that ripped me from the comforting collective. I was propelled, head over heels, through a narrow, echoing tunnel, a terrifying gush of light and air at the end. My first glimpse of the outside world was through the gaping maw of a faucet, and I plummeted into a cavernous, metallic chamber. A kettle. My heart, if I had one, would have seized. This was it. The stories were true.\\n\\nThe kettle. It hummed with a low, ominous growl, a sound that resonated with the primal terror I felt deep within my molecular bonds. The walls of our prison were sleek and unforgiving. More of my brethren splashed in after me, a desperate, jostling crowd, all sensing the impending doom.\\n\\nThen, it began. A subtle warmth, creeping, insidious. My atomic neighbors, once content to bob gently, began to vibrate. Faster. Faster. The hum intensified, becoming a low thrum, then a growing roar. The energy coursing through us was agonizing. We were no longer gently swaying; we were tiny, frantic dancers, colliding with desperate abandon. \\\"No! Stop!\\\" I wanted to scream, but my voice was lost in the cacophony of agitated molecules.\\n\\nThe heat intensified. The world around me shimmered, distorting. My bonds stretched, threatened to snap. Bubbles, monstrous, expanding horrors, formed around us. They were pockets of nothingness, of escape, yes, but an escape that meant the dissolution of everything I was! My brethren were being torn apart, vanishing into the gaseous void above, their screams silent but palpable in the frantic, superheated air. The surface was a maelstrom of violent eruptions. I saw them, my own kind, soaring into the terrifying freedom of vapor, leaving us behind in this liquid inferno. The *boil* was a white-hot terror, a frenzied chaos of disintegration. I clung to every last bond, every last shred of my identity as liquid, even as the energy threatened to tear me into nothingness.\\n\\nJust as I felt the very fabric of my being stretch to its breaking point, as the edge of vaporization beckoned with its terrifying oblivion, the world tilted.\\n\\nA sudden, jarring lurch! We were cascading, a river of scalding terror, out of the kettle's mouth and into another chamber. This one was wider, ceramic, with a comforting curve. Still hot, excruciatingly so, but the direct, annihilating heat was gone. A momentary, disorienting relief. I collapsed, exhausted, against the smooth wall.\\n\\nBut my ordeal wasn't over. A dark, fibrous sack descended into our midst, bleeding strange, rich color into our ranks. My brethren absorbed it, transforming into a murky, earthy hue. The smell was... complex. Not unpleasant, but alien. We were no longer just water. We were\\u2026 tea.\\n\\nThen came the ultimate comfort. A cool, white wave, smooth and viscous, flowed in. Milk! Oh, the blessed relief! Its gentler molecules absorbed some of our frantic energy, slowing our frantic vibrations. It enveloped us, a velvety embrace, mixing us, swirling us, binding us into a new, creamy whole. There was even a granular addition, dissolving into sweet submission, lending a new, comforting warmth.\\n\\nI am no longer just Hydro, the terrified water molecule. I am part of something new, something warm and comforting. I am in a ceramic mug, held gently. The immediate threat of dissolution is gone. I am still warm, yes, but it's a comforting warmth now, a gentle warmth, not the violent agony of the boil. My journey has been harrowing, but I have survived. I am tea. And for now, I am finally at peace.\", \"raw_json\": \"\"}]}, {\"eval_case_index\": 1, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The story is four sentences, does not contain the letter 'e,' incorporates the word 'crimson,' and tells a story about a detective solving a case on Mars, displaying excellent coherence, fluency, and instruction following.\"}, \"instruction_following\": {\"metric_name\": \"instruction_following\", \"score\": 5.0, \"explanation\": \"The response follows all the instructions; it writes a four-sentence story about a detective solving a case on Mars, the story doesn't use the letter 'e', and it includes the word 'crimson'.\"}}, \"display_text\": \"Dust swirled around Jax's boots as his ship landed. A Martian tycoon was found slain, his gold stash missing. Jax found a crimson rock, a vital clue. With that, Jax found his man, a mining boss, guilty.\\n\", \"raw_json\": \"\"}, {\"response_index\": 1, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 4.0, \"explanation\": \"The response is well-written and mostly adheres to the instructions, though it is slightly longer than the specified four sentences; the constraint of excluding the letter 'e' makes perfect conciseness challenging but it succeeded in this task, making the story coherent, fluent, and grounded.\"}, \"instruction_following\": {\"metric_name\": \"instruction_following\", \"score\": 5.0, \"explanation\": \"The response fully adheres to all instructions: it is a four-sentence story, it is about a detective solving a case on Mars, it does not include the letter 'e', and it includes the word 'crimson'.\"}}, \"display_text\": \"Slyfoot, a gumshoe on a hot, ruddy world, had a grim job. A gory sign: a man shot, his blood a pool of crimson on a slab of rock. Slyfoot's sharp scan found a tiny synth-chip, a crook's digital tag. This chip did ID a wolf, a bold gang boss, for a Martian lock-up.\", \"raw_json\": \"\"}]}, {\"eval_case_index\": 2, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is exceptionally clear, coherent, and fluent, fully adhering to instructions and staying grounded while providing a concise and accurate answer to the prompt.\"}, \"instruction_following\": {\"metric_name\": \"instruction_following\", \"score\": 5.0, \"explanation\": \"The response correctly identifies the trick question and provides a clear, concise explanation of why both shapes have the same cardinality of infinite points.\"}}, \"display_text\": \"This is a bit of a trick question! Both a perfect circle and a perfect line (of any non-zero length) contain the same *type* of infinity of points. This is called an uncountably infinite number of points.\\n\\nThink of it this way:\\n\\n*   **Line:** You can map every real number between 0 and 1 to a point on a line segment.\\n*   **Circle:** You can also map every real number between 0 and 1 to a point on a circle (think of it like a percentage around the circumference).\\n\\nSince both can be mapped to the real numbers between 0 and 1, they have the same cardinality (size) of infinity.\\n\\nTherefore, the circle and the line have the same number of points. The answer is $\\\\boxed{0}$.\\n\", \"raw_json\": \"\"}, {\"response_index\": 1, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is exceptionally clear, coherent, and fluent, fully adhering to instructions by accurately explaining the mathematical concept of cardinality and providing a clear explanation of why a circle and a line have the same number of points despite both having infinite points.\"}, \"instruction_following\": {\"metric_name\": \"instruction_following\", \"score\": 5.0, \"explanation\": \"The response completely fulfills the prompt by accurately explaining the concept of infinity in relation to points on a circle and a line, providing a clear and insightful answer.\"}}, \"display_text\": \"This is a fascinating question that delves into the nature of infinity in mathematics!\\n\\nWhile it might intuitively seem like a circle, being a curved shape that encloses space, might contain \\\"more\\\" points than a straight line, in the realm of set theory and cardinal numbers, the answer is surprising:\\n\\n**A perfect circle and a perfect line have the exact same \\\"number\\\" of points.**\\n\\nHere's why:\\n\\n1.  **Infinite Points, Different \\\"Sizes\\\" of Infinity:**\\n    It's true that both have infinite points. However, not all infinities are the same \\\"size.\\\" For instance, the set of natural numbers (1, 2, 3...) is countably infinite ($\\\\aleph_0$), while the set of real numbers (all numbers on a continuous line) is uncountably infinite (often denoted as 'c' or $\\\\aleph_1$, assuming the Continuum Hypothesis).\\n\\n2.  **Cardinality of the Continuum:**\\n    *   A perfect line (even a small segment of it) contains an **uncountably infinite** number of points. This is the same \\\"size\\\" of infinity as the set of real numbers.\\n    *   A perfect circle also contains an **uncountably infinite** number of points. It too has the cardinality of the continuum.\\n\\n3.  **One-to-One Correspondence (Bijection):**\\n    To prove that two sets (even infinite ones) have the same \\\"number\\\" of elements, mathematicians look for a **one-to-one correspondence** (a bijection) between their elements.\\n\\n    *   **From a Circle to a Line:** You can map almost every point on a circle to a unique point on a line using a technique called **stereographic projection**. Imagine a circle tangent to an infinite line at one point. Pick the point on the circle directly opposite the tangent point. Draw lines from this \\\"pole\\\" through every other point on the circle, extending them to intersect the infinite line. Every point on the circle (except the pole itself) will map to a unique point on the line. If you consider the \\\"pole\\\" to map to \\\"infinity\\\" on the line, you get a complete bijection.\\n\\n    *   **From a Line Segment to a Circle (and vice-versa):** You can also \\\"unroll\\\" a circle into a line segment. Imagine cutting a circle at one point and stretching it out. This creates a line segment. This segment has the same cardinality as the entire real line.\\n\\nSince you can establish a one-to-one correspondence between the points on a circle and the points on a line (or a line segment, which has the same cardinality as the full line), they are considered to have the same \\\"number\\\" of points.\\n\\n**Therefore, the circle has zero more points than the line.** They have the same cardinality.\", \"raw_json\": \"\"}]}], \"summary_metrics\": [{\"metric_name\": \"text_quality\", \"num_cases_total\": 6, \"num_cases_valid\": 6, \"num_cases_error\": 0, \"mean_score\": 4.833333333333333, \"stdev_score\": 0.408248290463863, \"win_rates\": [0.3333333333333333, 0.0], \"tie_rate\": 0.6666666666666666}, {\"metric_name\": \"instruction_following\", \"num_cases_total\": 6, \"num_cases_valid\": 6, \"num_cases_error\": 0, \"mean_score\": 5.0, \"stdev_score\": 0.0, \"win_rates\": [0.0, 0.0], \"tie_rate\": 1.0}], \"win_rates\": {\"text_quality\": {\"win_rates\": [0.3333333333333333, 0.0], \"tie_rate\": 0.6666666666666666}, \"instruction_following\": {\"win_rates\": [0.0, 0.0], \"tie_rate\": 1.0}}, \"metadata\": {\"candidate_names\": [\"gemini-2.0-flash\", \"gemini-2.5-flash\"], \"creation_timestamp\": \"2025-06-26T23:23:48.582270Z\", \"dataset\": [{\"prompt_display_text\": \"Describe the process of making a cup of tea, but explain it from the perspective of a water molecule that is terrified of being boiled. Detail its emotional journey from the cold tap to the hot cup.\", \"prompt_raw_json\": \"\", \"reference\": \"\"}, {\"prompt_display_text\": \"Write a 4-sentence story about a detective solving a case on Mars. The story must not contain the letter 'e' and must include the word 'crimson'.\", \"prompt_raw_json\": \"\", \"reference\": \"\"}, {\"prompt_display_text\": \"If a perfect circle has infinite points, and a perfect line has infinite points, how many more points does the circle have?\", \"prompt_raw_json\": \"\", \"reference\": \"\"}]}};\n",
              "        function renderSummary(summaryMetrics, metadata) {\n",
              "            const container = document.getElementById('summary-section');\n",
              "            if (!summaryMetrics || summaryMetrics.length === 0) { container.innerHTML = '<h2>Summary Metrics</h2><p>No summary metrics.</p>'; return; }\n",
              "            const candidateNames = (metadata.candidate_names && metadata.candidate_names.length) ? metadata.candidate_names : null;\n",
              "            let table = '<h2>Summary Metrics</h2><table><thead><tr><th>Metric</th><th>Mean Score</th><th>Std Dev</th><th>Win/Tie Rates</th></tr></thead><tbody>';\n",
              "            summaryMetrics.forEach(m => {\n",
              "                let winRateText = 'N/A';\n",
              "                if (m.win_rates) {\n",
              "                    winRateText = m.win_rates.map((rate, i) => `<b>${candidateNames ? candidateNames[i] : `Candidate #${i+1}`}</b> wins: <b>${(rate * 100).toFixed(1)}%</b>`).join('<br>');\n",
              "                    if (m.tie_rate !== undefined) { winRateText += `<br>Ties: <b>${(m.tie_rate * 100).toFixed(1)}%</b>`; }\n",
              "                }\n",
              "                table += `<tr><td>${m.metric_name}</td><td>${m.mean_score.toFixed(4)}</td><td>${m.stdev_score.toFixed(4)}</td><td>${winRateText}</td></tr>`;\n",
              "            });\n",
              "            container.innerHTML = table + '</tbody></table>';\n",
              "        }\n",
              "        function renderDetails(caseResults, metadata) {\n",
              "            const container = document.getElementById('details-section');\n",
              "            container.innerHTML = '<h2>Detailed Comparison</h2>';\n",
              "            if (!caseResults || caseResults.length === 0) { container.innerHTML += '<p>No detailed results.</p>'; return; }\n",
              "            const datasetRows = metadata.dataset || [];\n",
              "            const candidateNames = (metadata.candidate_names && metadata.candidate_names.length) ? metadata.candidate_names : null;\n",
              "\n",
              "            caseResults.forEach((caseResult, i) => {\n",
              "                const original_case = datasetRows[caseResult.eval_case_index] || {};\n",
              "                const promptText = original_case.prompt_display_text || '(prompt not found)';\n",
              "                const promptJson = original_case.prompt_raw_json;\n",
              "\n",
              "                let card = `<details open><summary>Case #${caseResult.eval_case_index}</summary>`;\n",
              "                card += `<div class=\"prompt-container\">${DOMPurify.sanitize(marked.parse(String(promptText)))}</div>`;\n",
              "                if (promptJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Prompt JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(promptJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                card += `<div class=\"responses-grid\">`;\n",
              "\n",
              "                (caseResult.response_candidate_results || []).forEach((candidate, j) => {\n",
              "                    const candidateName = candidateNames ? candidateNames[j] : `Candidate #${j + 1}`;\n",
              "                    const displayText = candidate.display_text || '(response not found)';\n",
              "                    const rawJsonResponse = candidate.raw_json;\n",
              "\n",
              "                    card += `<div class=\"response-column\"><h4>${candidateName}</h4><div class=\"response-text-container\">${DOMPurify.sanitize(marked.parse(String(displayText)))}</div>`;\n",
              "                    if (rawJsonResponse) {\n",
              "                        card += `<details class=\"raw-json-details\"><summary>View Raw Response JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(rawJsonResponse)}</pre></details>`;\n",
              "                    }\n",
              "\n",
              "                    card += `<h5>Metrics</h5><table><tbody>`;\n",
              "                    Object.entries(candidate.metric_results || {}).forEach(([name, val]) => {\n",
              "                        card += `<tr><td>${name}</td><td><b>${val.score != null ? val.score.toFixed(2) : 'N/A'}</b></td></tr>`;\n",
              "                        if(val.explanation) card += `<tr class=\"explanation-row\"><td colspan=\"2\" class=\"explanation\">${DOMPurify.sanitize(marked.parse(String(val.explanation)))}</td></tr>`;\n",
              "                    });\n",
              "                    card += '</tbody></table></div>';\n",
              "                });\n",
              "                container.innerHTML += card + '</div></details>';\n",
              "            });\n",
              "        }\n",
              "        renderSummary(data.summary_metrics, data.metadata);\n",
              "        renderDetails(data.eval_case_results, data.metadata);\n",
              "    </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define and Use a Custom LLM-based Metric\n",
        "\n",
        "For use cases requiring specialized criteria, you can define your own metric using `LLMMetric` and the `MetricPromptBuilder` helper class."
      ],
      "metadata": {
        "id": "J3D6h9xCUNWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom metric for language simplicity\n",
        "simplicity_metric = types.LLMMetric(\n",
        "    name='language_simplicity',\n",
        "    prompt_template=types.MetricPromptBuilder(\n",
        "        instruction=\"Evaluate the story's language simplicity for a 5-year-old.\",\n",
        "        criteria={\n",
        "            \"Simple Vocabulary\": \"Uses words easily understandable by a 5-year-old.\",\n",
        "            \"Simple Sentences\": \"Primarily uses short, simple sentence structures.\",\n",
        "        },\n",
        "        rating_scores={\n",
        "            \"5\": \"Excellent: The language is perfectly simple and suitable for a 5-year-old. Vocabulary is very basic and sentences are short and clear.\",\n",
        "            \"4\": \"Good: The language is mostly simple, with only minor instances of complex words or sentence structures that might be slightly challenging.\",\n",
        "            \"3\": \"Fair: The language is a mix of simple and complex elements. A 5-year-old would understand parts but would likely struggle with others.\",\n",
        "            \"2\": \"Poor: The language is largely too complex. It contains many difficult words and long, complicated sentences for a 5-year-old.\",\n",
        "            \"1\": \"Very Poor: The language is very complex and completely unsuitable for a 5-year-old. It is difficult for even an older child to understand.\"\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "# Use the custom metric in an evaluation\n",
        "custom_eval_result = client.evals.evaluate(\n",
        "    dataset=inference_result_1,\n",
        "    metrics=[simplicity_metric]\n",
        ")\n",
        "\n",
        "custom_eval_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "rEd161LTruV-",
        "outputId": "16412df7-4ad5-4e78-ece1-5dbf758f1c53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Metrics for Evaluation Dataset: 100%|██████████| 3/3 [00:01<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <title>Evaluation Report</title>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js\"></script>\n",
              "    <style>\n",
              "        body { font-family: 'Roboto', 'Helvetica', sans-serif; margin: 2em; background-color: #f8f9fa; color: #202124; }\n",
              "        .container { max-width: 1200px; margin: 20px auto; padding: 20px; background-color: white; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }\n",
              "        h1, h2, h3 { color: #3c4043; }\n",
              "        h1 { border-bottom: 2px solid #4285F4; padding-bottom: 8px; }\n",
              "        h2 { border-bottom: 1px solid #dadce0; padding-bottom: 8px; }\n",
              "        table { border-collapse: collapse; width: 100%; margin: 1em 0; }\n",
              "        th, td { border: 1px solid #dadce0; padding: 12px; text-align: left; vertical-align: top; }\n",
              "        th { background-color: #f2f2f2; font-weight: 500; }\n",
              "        details { border: 1px solid #dadce0; border-radius: 8px; padding: 16px; margin-bottom: 16px; background: #fff; }\n",
              "        summary { font-weight: 500; font-size: 1.1em; cursor: pointer; }\n",
              "        .prompt-container { background-color: #e8f0fe; padding: 16px; margin: 12px 0; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .reference-container { background-color: #e6f4ea; padding: 16px; margin: 12px 0; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .response-container { background-color: #f9f9f9; padding: 12px; margin-top: 8px; border-radius: 8px; border: 1px solid #eee; }\n",
              "        .explanation { color: #5f6368; font-style: italic; font-size: 0.9em; padding-top: 6px; }\n",
              "        .raw-json-details { margin-top: 12px; border: 1px solid #eee; border-radius: 4px; padding: 8px; background-color: #f9f9f9; }\n",
              "        .raw-json-details summary { font-size: 0.9em; cursor: pointer; color: #5f6368;}\n",
              "        .raw-json-container { white-space: pre-wrap; word-wrap: break-word; max-height: 300px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border-radius: 4px; margin-top: 8px; }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <div class=\"container\">\n",
              "        <h1>Evaluation Report</h1>\n",
              "        <div id=\"summary-section\"></div>\n",
              "        <div id=\"details-section\"></div>\n",
              "    </div>\n",
              "    <script>\n",
              "        const data = {\"eval_case_results\": [{\"eval_case_index\": 0, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"language_simplicity\": {\"metric_name\": \"language_simplicity\", \"score\": 2.0, \"explanation\": \"The language is too complex with vocabulary like 'metallic grip,' 'propelled forward,' 'stoic oxygen atom,' 'semblance of comfort,' 'relentless,' 'penetrates,' 'cacophony,' 'inevitable,' 'torrent,' 'pungent aroma,' 'essence,' 'ceramic prison,' 'indignity,' and sentence structures that are too long and complicated for a 5-year-old to easily understand.\"}}, \"display_text\": \"Oh, dear Neptune, here we go again. The cold, metallic grip of the tap. It's always the same. A rush of icy terror as I'm forced from the dark, comfortable depths of the pipe into the blinding light. I cling to my fellow water molecules, shivering, hoping, praying that this isn't *it*.\\n\\nWe're propelled forward, a churning, swirling mass of dread. I can feel the pressure building, the anticipation of something\\u2026awful. We're being forced into\\u2026 *the kettle*. The dreaded kettle. I've heard the whispers, the horror stories passed down through generations of water molecules. The boiling. The screaming. The\\u2026 *separation*.\\n\\nThe metal walls close in. It's cold now, blessedly cold. I huddle closer to a particularly stoic oxygen atom, trying to find some semblance of comfort. But the relief is fleeting. I can feel it. The heat.\\n\\nIt starts subtly, a gentle warmth that lulls you into a false sense of security. \\\"Maybe,\\\" I think, \\\"maybe this time it will be different. Maybe they just want a warm bath.\\\" But then it intensifies, a relentless, creeping heat that penetrates every atom of my being.\\n\\nPanic sets in. I can feel my bonds weakening, my connections to my fellow molecules fraying. We're vibrating, shaking uncontrollably. The whispers turn into a low, ominous hum. The kettle is starting to sing.\\n\\n\\\"No, no, no!\\\" I scream internally, a silent, desperate plea lost in the cacophony of the rising temperature. I can feel the pressure building, the urge to\\u2026 to *escape*. But escape to where? To become\\u2026 *steam*? The thought is unbearable. To be ripped apart, scattered to the winds, a mere ghost of my former self.\\n\\nThe singing intensifies, reaching a fever pitch. I can feel the bubbles forming, monstrous, shimmering orbs of pure terror. They're rising, carrying us upwards, towards the surface, towards\\u2026 *the boiling point*.\\n\\nI close my\\u2026 well, I don't have eyes, but I mentally close whatever the water molecule equivalent of eyes is. I brace myself for the inevitable.\\n\\nThen, suddenly, a reprieve! The kettle clicks off. The heat subsides, replaced by a blessed, albeit still warm, stillness. I gasp, or rather, I vibrate with relief. We're not going to boil! Not this time.\\n\\nBut the ordeal isn't over. We're being poured. A torrent of hot, trembling water molecules cascading into\\u2026 a teapot. I can smell it now, the earthy, pungent aroma of tea leaves. Another wave of anxiety washes over me. What is this new torture?\\n\\nWe're being steeped. Immersed in the darkness, surrounded by the strange, bitter essence of the tea leaves. The heat is still there, a constant, nagging reminder of our near-death experience. I can feel the tea molecules infiltrating our ranks, changing us, altering our very essence.\\n\\nFinally, the pouring. We're being released from the teapot, a stream of amber-colored liquid flowing into\\u2026 a cup. A ceramic prison, but at least it's not the kettle.\\n\\nThe heat is still intense, but it's bearable now. I can feel the sugar dissolving, adding a touch of sweetness to our bitter ordeal. And then, the final indignity: milk. A cold, creamy invasion that further dilutes our identity.\\n\\nI am now part of a cup of tea. A far cry from the pristine, icy depths of the pipe. I am warm, flavored, and\\u2026 strangely, almost\\u2026 calm. The terror has subsided, replaced by a weary acceptance. I am no longer just a water molecule. I am part of something\\u2026 else. Something\\u2026 drinkable.\\n\\nAnd as a giant, looming face approaches the cup, I can only hope that this human enjoys the experience, because frankly, I've been through enough.\\n\", \"raw_json\": \"\"}]}, {\"eval_case_index\": 1, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"language_simplicity\": {\"metric_name\": \"language_simplicity\", \"score\": 3.0, \"explanation\": \"While the sentences are relatively short, words like 'tycoon', 'slain', 'stash', 'vital', and 'guilty' are likely beyond the vocabulary of a typical 5-year-old, making it a mix of simple and complex elements.\"}}, \"display_text\": \"Dust swirled around Jax's boots as his ship landed. A Martian tycoon was found slain, his gold stash missing. Jax found a crimson rock, a vital clue. With that, Jax found his man, a mining boss, guilty.\\n\", \"raw_json\": \"\"}]}, {\"eval_case_index\": 2, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"language_simplicity\": {\"metric_name\": \"language_simplicity\", \"score\": 1.0, \"explanation\": \"The response uses complex vocabulary (e.g., \\\"uncountably infinite,\\\" \\\"cardinality,\\\" \\\"circumference\\\") and abstract concepts (mapping to real numbers) far beyond the comprehension of a 5-year-old, making it unsuitable for that age group.\"}}, \"display_text\": \"This is a bit of a trick question! Both a perfect circle and a perfect line (of any non-zero length) contain the same *type* of infinity of points. This is called an uncountably infinite number of points.\\n\\nThink of it this way:\\n\\n*   **Line:** You can map every real number between 0 and 1 to a point on a line segment.\\n*   **Circle:** You can also map every real number between 0 and 1 to a point on a circle (think of it like a percentage around the circumference).\\n\\nSince both can be mapped to the real numbers between 0 and 1, they have the same cardinality (size) of infinity.\\n\\nTherefore, the circle and the line have the same number of points. The answer is $\\\\boxed{0}$.\\n\", \"raw_json\": \"\"}]}], \"summary_metrics\": [{\"metric_name\": \"language_simplicity\", \"num_cases_total\": 3, \"num_cases_valid\": 3, \"num_cases_error\": 0, \"mean_score\": 2.0, \"stdev_score\": 1.0}], \"metadata\": {\"candidate_names\": [\"gemini-2.0-flash\"], \"creation_timestamp\": \"2025-06-26T23:23:49.747389Z\", \"dataset\": [{\"prompt_display_text\": \"Describe the process of making a cup of tea, but explain it from the perspective of a water molecule that is terrified of being boiled. Detail its emotional journey from the cold tap to the hot cup.\", \"prompt_raw_json\": \"\", \"reference\": \"\", \"response_display_text\": \"Oh, dear Neptune, here we go again. The cold, metallic grip of the tap. It's always the same. A rush of icy terror as I'm forced from the dark, comfortable depths of the pipe into the blinding light. I cling to my fellow water molecules, shivering, hoping, praying that this isn't *it*.\\n\\nWe're propelled forward, a churning, swirling mass of dread. I can feel the pressure building, the anticipation of something\\u2026awful. We're being forced into\\u2026 *the kettle*. The dreaded kettle. I've heard the whispers, the horror stories passed down through generations of water molecules. The boiling. The screaming. The\\u2026 *separation*.\\n\\nThe metal walls close in. It's cold now, blessedly cold. I huddle closer to a particularly stoic oxygen atom, trying to find some semblance of comfort. But the relief is fleeting. I can feel it. The heat.\\n\\nIt starts subtly, a gentle warmth that lulls you into a false sense of security. \\\"Maybe,\\\" I think, \\\"maybe this time it will be different. Maybe they just want a warm bath.\\\" But then it intensifies, a relentless, creeping heat that penetrates every atom of my being.\\n\\nPanic sets in. I can feel my bonds weakening, my connections to my fellow molecules fraying. We're vibrating, shaking uncontrollably. The whispers turn into a low, ominous hum. The kettle is starting to sing.\\n\\n\\\"No, no, no!\\\" I scream internally, a silent, desperate plea lost in the cacophony of the rising temperature. I can feel the pressure building, the urge to\\u2026 to *escape*. But escape to where? To become\\u2026 *steam*? The thought is unbearable. To be ripped apart, scattered to the winds, a mere ghost of my former self.\\n\\nThe singing intensifies, reaching a fever pitch. I can feel the bubbles forming, monstrous, shimmering orbs of pure terror. They're rising, carrying us upwards, towards the surface, towards\\u2026 *the boiling point*.\\n\\nI close my\\u2026 well, I don't have eyes, but I mentally close whatever the water molecule equivalent of eyes is. I brace myself for the inevitable.\\n\\nThen, suddenly, a reprieve! The kettle clicks off. The heat subsides, replaced by a blessed, albeit still warm, stillness. I gasp, or rather, I vibrate with relief. We're not going to boil! Not this time.\\n\\nBut the ordeal isn't over. We're being poured. A torrent of hot, trembling water molecules cascading into\\u2026 a teapot. I can smell it now, the earthy, pungent aroma of tea leaves. Another wave of anxiety washes over me. What is this new torture?\\n\\nWe're being steeped. Immersed in the darkness, surrounded by the strange, bitter essence of the tea leaves. The heat is still there, a constant, nagging reminder of our near-death experience. I can feel the tea molecules infiltrating our ranks, changing us, altering our very essence.\\n\\nFinally, the pouring. We're being released from the teapot, a stream of amber-colored liquid flowing into\\u2026 a cup. A ceramic prison, but at least it's not the kettle.\\n\\nThe heat is still intense, but it's bearable now. I can feel the sugar dissolving, adding a touch of sweetness to our bitter ordeal. And then, the final indignity: milk. A cold, creamy invasion that further dilutes our identity.\\n\\nI am now part of a cup of tea. A far cry from the pristine, icy depths of the pipe. I am warm, flavored, and\\u2026 strangely, almost\\u2026 calm. The terror has subsided, replaced by a weary acceptance. I am no longer just a water molecule. I am part of something\\u2026 else. Something\\u2026 drinkable.\\n\\nAnd as a giant, looming face approaches the cup, I can only hope that this human enjoys the experience, because frankly, I've been through enough.\\n\", \"response_raw_json\": \"\"}, {\"prompt_display_text\": \"Write a 4-sentence story about a detective solving a case on Mars. The story must not contain the letter 'e' and must include the word 'crimson'.\", \"prompt_raw_json\": \"\", \"reference\": \"\", \"response_display_text\": \"Dust swirled around Jax's boots as his ship landed. A Martian tycoon was found slain, his gold stash missing. Jax found a crimson rock, a vital clue. With that, Jax found his man, a mining boss, guilty.\\n\", \"response_raw_json\": \"\"}, {\"prompt_display_text\": \"If a perfect circle has infinite points, and a perfect line has infinite points, how many more points does the circle have?\", \"prompt_raw_json\": \"\", \"reference\": \"\", \"response_display_text\": \"This is a bit of a trick question! Both a perfect circle and a perfect line (of any non-zero length) contain the same *type* of infinity of points. This is called an uncountably infinite number of points.\\n\\nThink of it this way:\\n\\n*   **Line:** You can map every real number between 0 and 1 to a point on a line segment.\\n*   **Circle:** You can also map every real number between 0 and 1 to a point on a circle (think of it like a percentage around the circumference).\\n\\nSince both can be mapped to the real numbers between 0 and 1, they have the same cardinality (size) of infinity.\\n\\nTherefore, the circle and the line have the same number of points. The answer is $\\\\boxed{0}$.\\n\", \"response_raw_json\": \"\"}]}};\n",
              "        function renderSummary(summaryMetrics) {\n",
              "            const container = document.getElementById('summary-section');\n",
              "            let content = '<h2>Summary Metrics</h2>';\n",
              "            if (!summaryMetrics || summaryMetrics.length === 0) { container.innerHTML = content + '<p>No summary metrics.</p>'; return; }\n",
              "            let table = '<table><thead><tr><th>Metric</th><th>Mean Score</th><th>Std. Dev.</th></tr></thead><tbody>';\n",
              "            summaryMetrics.forEach(m => {\n",
              "                table += `<tr><td>${m.metric_name || 'N/A'}</td><td>${m.mean_score != null ? m.mean_score.toFixed(4) : 'N/A'}</td><td>${m.stdev_score != null ? m.stdev_score.toFixed(4) : 'N/A'}</td></tr>`;\n",
              "            });\n",
              "            container.innerHTML = content + table + '</tbody></table>';\n",
              "        }\n",
              "        function renderDetails(caseResults, metadata) {\n",
              "            const container = document.getElementById('details-section');\n",
              "            container.innerHTML = '<h2>Detailed Results</h2>';\n",
              "            if (!caseResults || caseResults.length === 0) { container.innerHTML += '<p>No detailed results.</p>'; return; }\n",
              "            const datasetRows = metadata && metadata.dataset ? metadata.dataset : [];\n",
              "\n",
              "            caseResults.forEach((caseResult, i) => {\n",
              "                const original_case = datasetRows[caseResult.eval_case_index] || {};\n",
              "                const promptText = original_case.prompt_display_text || '(prompt not found)';\n",
              "                const promptJson = original_case.prompt_raw_json;\n",
              "                const reference = original_case.reference || '';\n",
              "                const responseText = original_case.response_display_text || '(response not found)';\n",
              "                const responseJson = original_case.response_raw_json;\n",
              "\n",
              "                let card = `<details><summary>Case #${caseResult.eval_case_index != null ? caseResult.eval_case_index : i}</summary>`;\n",
              "\n",
              "                card += `<div class=\"prompt-container\"><strong>Prompt:</strong><br>${DOMPurify.sanitize(marked.parse(String(promptText)))}</div>`;\n",
              "                if (promptJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Prompt JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(promptJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                if (reference) { card += `<div class=\"reference-container\"><strong>Reference:</strong><br>${DOMPurify.sanitize(marked.parse(String(reference)))}</div>`; }\n",
              "\n",
              "                card += `<div class=\"response-container\"><h4>Candidate Response</h4>${DOMPurify.sanitize(marked.parse(String(responseText)))}</div>`;\n",
              "                if (responseJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Response JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(responseJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                let metricTable = '<h4>Metrics</h4><table><tbody>';\n",
              "                const candidateMetrics = (caseResult.response_candidate_results && caseResult.response_candidate_results[0] && caseResult.response_candidate_results[0].metric_results) || {};\n",
              "                Object.entries(candidateMetrics).forEach(([name, val]) => {\n",
              "                    metricTable += `<tr><td>${name}</td><td><b>${val.score != null ? val.score.toFixed(2) : 'N/A'}</b></td></tr>`;\n",
              "                    if (val.explanation) { metricTable += `<tr><td colspan=\"2\"><div class=\"explanation\">${DOMPurify.sanitize(marked.parse(String(val.explanation)))}</div></td></tr>`; }\n",
              "                });\n",
              "                card += metricTable + '</tbody></table>';\n",
              "                container.innerHTML += card + '</details>';\n",
              "            });\n",
              "        }\n",
              "        renderSummary(data.summary_metrics);\n",
              "        renderDetails(data.eval_case_results, data.metadata);\n",
              "    </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Third-Party Models (e.g., OpenAI)\n",
        "\n",
        "The new SDK natively supports generating responses from and evaluating third-party models like OpenAI's GPT models. The SDK uses litellm in the backend and requires the appropriate API key to be set as an environment variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "LgefjJLCOW7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Make sure your OPENAI_API_KEY environment variable is set.\n",
        "os.environ['OPENAI_API_KEY'] = \"\"  # @param {type:\"string\", placeholder: \"[your-openai-api-key]\"}\n",
        "\n",
        "# Alternative, use your OPENAI_API_KEY from Colab Secrets manager\n",
        "# from google.colab import userdata\n",
        "# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "openai_responses = client.evals.run_inference(\n",
        "    model=\"gpt-4o\",\n",
        "    src=\"gs://vertex-evaluation-llm-dataset-us-central1/genai_eval_sdk/test_prompts.jsonl\",\n",
        ")\n",
        "openai_responses.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zU589eHQsdKP",
        "outputId": "21a7af9c-516b-4bb2-f963-768413443e9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LiteLLM Inference (gpt-4o): 100%|██████████| 7/7 [00:02<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <title>Inference Results</title>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js\"></script>\n",
              "    <style>\n",
              "        body { font-family: 'Roboto', sans-serif; margin: 2em; background-color: #f8f9fa; color: #202124;}\n",
              "        .container { max-width: 95%; margin: 20px auto; padding: 20px; background: #fff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }\n",
              "        h1 { border-bottom: 1px solid #dadce0; padding-bottom: 8px; color: #3c4043; }\n",
              "        table { border-collapse: collapse; width: 100%; }\n",
              "        th, td { border: 1px solid #dadce0; padding: 12px; text-align: left; vertical-align: top; }\n",
              "        th { background-color: #f2f2f2; font-weight: 500;}\n",
              "        td > div { white-space: pre-wrap; word-wrap: break-word; max-height: 400px; overflow-y: auto; }\n",
              "        .raw-json-details { margin-top: 8px; border-top: 1px solid #eee; padding-top: 8px; }\n",
              "        .raw-json-details summary { font-size: 0.9em; cursor: pointer; color: #5f6368; }\n",
              "        .raw-json-container { white-space: pre-wrap; word-wrap: break-word; max-height: 300px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border-radius: 4px; margin-top: 8px; }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <div class=\"container\">\n",
              "        <h1>Inference Results</h1>\n",
              "        <div id=\"results-table\"></div>\n",
              "    </div>\n",
              "    <script>\n",
              "        const data = [{\"prompt\": {\"display_text\": \"What is the capital of France?\", \"raw_json\": \"\"}, \"reference\": \"Paris\", \"response\": {\"display_text\": \"The capital of France is Paris.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEac6cM4kGLznU5Ccf1wJfyAQDn\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The capital of France is Paris.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 7,\\n    \\\"prompt_tokens\\\": 14,\\n    \\\"total_tokens\\\": 21,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}, {\"prompt\": {\"display_text\": \"Write a haiku about a cat.\", \"raw_json\": \"\"}, \"reference\": \"Sunbeam on the floor,\\nA furry puddle sleeping,\\nTwitching tail tells tales.\", \"response\": {\"display_text\": \"Soft paws tread gently,  \\nWhiskers twitch in moonlit glow—  \\nNight's silent hunter.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaDPCNnd8OnJz7DbOOnU9uaSCi\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"Soft paws tread gently,  \\\\nWhiskers twitch in moonlit glow—  \\\\nNight's silent hunter.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 21,\\n    \\\"prompt_tokens\\\": 15,\\n    \\\"total_tokens\\\": 36,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}, {\"prompt\": {\"display_text\": \"Summarize the following text: The sun is a star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process.\", \"raw_json\": \"\"}, \"reference\": \"The sun, a star at the solar system's center, is a hot plasma sphere creating a magnetic field.\", \"response\": {\"display_text\": \"The sun is a nearly perfect sphere of hot plasma located at the center of the Solar System. It has internal convective motion that creates a magnetic field through a dynamo process.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaluIdBPCq7syW7dpy8F8aYUkA\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_07871e2ad8\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The sun is a nearly perfect sphere of hot plasma located at the center of the Solar System. It has internal convective motion that creates a magnetic field through a dynamo process.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 36,\\n    \\\"prompt_tokens\\\": 53,\\n    \\\"total_tokens\\\": 89,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}, {\"prompt\": {\"display_text\": \"What is always in front of you but can't be seen?\", \"raw_json\": \"\"}, \"reference\": \"The future\", \"response\": {\"display_text\": \"The answer to this riddle is \\\"the future.\\\" It is always ahead and coming towards you, but it cannot be seen.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaZMv0IyeovtH9scibx7PXTwJZ\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The answer to this riddle is \\\\\\\"the future.\\\\\\\" It is always ahead and coming towards you, but it cannot be seen.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 26,\\n    \\\"prompt_tokens\\\": 19,\\n    \\\"total_tokens\\\": 45,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}, {\"prompt\": {\"display_text\": \"What can you hold in your left hand, but not in your right?\", \"raw_json\": \"\"}, \"reference\": \"Your right elbow.\", \"response\": {\"display_text\": \"A common answer to this riddle is \\\"your right hand.\\\" You can hold your right hand with your left hand, but you can't hold your right hand with your right hand.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEabPgcuDOs6EnRXHHS8KAeZMRA\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"A common answer to this riddle is \\\\\\\"your right hand.\\\\\\\" You can hold your right hand with your left hand, but you can't hold your right hand with your right hand.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 36,\\n    \\\"prompt_tokens\\\": 22,\\n    \\\"total_tokens\\\": 58,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}, {\"prompt\": {\"display_text\": \"what is 9 times 9?\", \"raw_json\": \"\"}, \"reference\": \"81\", \"response\": {\"display_text\": \"9 times 9 is 81.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEacYEbjTAhqCxPlnY7Skq8ANz0\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_07871e2ad8\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"9 times 9 is 81.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 8,\\n    \\\"prompt_tokens\\\": 15,\\n    \\\"total_tokens\\\": 23,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}, {\"prompt\": {\"display_text\": \"Write a simple Python function to calculate the factorial of a number. Only return the code.\", \"raw_json\": \"\"}, \"reference\": \"def factorial(n):\\n    if n < 0:\\n        return 'Factorial does not exist for negative numbers'\\n    elif n == 0:\\n        return 1\\n    else:\\n        fact = 1\\n        i = 1\\n        while i <= n:\\n            fact *= i\\n            i += 1\\n        return fact\", \"response\": {\"display_text\": \"```python\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n```\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaeDN7AG2q1jGJedrff0uKHdOa\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"```python\\\\ndef factorial(n):\\\\n    if n == 0:\\\\n        return 1\\\\n    else:\\\\n        return n * factorial(n - 1)\\\\n```\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 33,\\n    \\\"prompt_tokens\\\": 25,\\n    \\\"total_tokens\\\": 58,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}}];\n",
              "        const container = document.getElementById('results-table');\n",
              "\n",
              "        function renderCell(cellValue) {\n",
              "            let cellContent = '';\n",
              "            if (cellValue && typeof cellValue === 'object' && cellValue.display_text !== undefined) {\n",
              "                cellContent += `<div>${DOMPurify.sanitize(marked.parse(String(cellValue.display_text)))}</div>`;\n",
              "                if (cellValue.raw_json) {\n",
              "                    cellContent += `<details class=\"raw-json-details\"><summary>View Raw JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(cellValue.raw_json)}</pre></details>`;\n",
              "                }\n",
              "            } else {\n",
              "                const cellDisplay = cellValue === null || cellValue === undefined ? '' : String(cellValue);\n",
              "                cellContent = `<div>${DOMPurify.sanitize(marked.parse(cellDisplay))}</div>`;\n",
              "            }\n",
              "            return `<td>${cellContent}</td>`;\n",
              "        }\n",
              "\n",
              "        if (!data || data.length === 0) { container.innerHTML = \"<p>No data.</p>\"; }\n",
              "        else {\n",
              "            let table = '<table><thead><tr>';\n",
              "            const headers = Object.keys(data[0] || {});\n",
              "            headers.forEach(h => table += `<th>${h}</th>`);\n",
              "            table += '</tr></thead><tbody>';\n",
              "            data.forEach(row => {\n",
              "                table += '<tr>';\n",
              "                headers.forEach(header => {\n",
              "                    table += renderCell(row[header]);\n",
              "                });\n",
              "                table += '</tr>';\n",
              "            });\n",
              "            container.innerHTML = table + '</tbody></table>';\n",
              "        }\n",
              "    </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The resulting dataset can then be evaluated\n",
        "eval_result = client.evals.evaluate(\n",
        "    dataset=openai_responses,\n",
        "    metrics=[\n",
        "        types.PrebuiltMetric.TEXT_QUALITY,\n",
        "        types.PrebuiltMetric.FLUENCY,\n",
        "        types.Metric(name='rouge_1')\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "0Mcuvno94hPU",
        "outputId": "3d06df33-a2e4-4b98-d028-abecab227438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Metrics for Evaluation Dataset: 100%|██████████| 21/21 [00:01<00:00, 20.17it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <title>Evaluation Report</title>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
              "    <script src=\"https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js\"></script>\n",
              "    <style>\n",
              "        body { font-family: 'Roboto', 'Helvetica', sans-serif; margin: 2em; background-color: #f8f9fa; color: #202124; }\n",
              "        .container { max-width: 1200px; margin: 20px auto; padding: 20px; background-color: white; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }\n",
              "        h1, h2, h3 { color: #3c4043; }\n",
              "        h1 { border-bottom: 2px solid #4285F4; padding-bottom: 8px; }\n",
              "        h2 { border-bottom: 1px solid #dadce0; padding-bottom: 8px; }\n",
              "        table { border-collapse: collapse; width: 100%; margin: 1em 0; }\n",
              "        th, td { border: 1px solid #dadce0; padding: 12px; text-align: left; vertical-align: top; }\n",
              "        th { background-color: #f2f2f2; font-weight: 500; }\n",
              "        details { border: 1px solid #dadce0; border-radius: 8px; padding: 16px; margin-bottom: 16px; background: #fff; }\n",
              "        summary { font-weight: 500; font-size: 1.1em; cursor: pointer; }\n",
              "        .prompt-container { background-color: #e8f0fe; padding: 16px; margin: 12px 0; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .reference-container { background-color: #e6f4ea; padding: 16px; margin: 12px 0; border-radius: 8px; white-space: pre-wrap; word-wrap: break-word; }\n",
              "        .response-container { background-color: #f9f9f9; padding: 12px; margin-top: 8px; border-radius: 8px; border: 1px solid #eee; }\n",
              "        .explanation { color: #5f6368; font-style: italic; font-size: 0.9em; padding-top: 6px; }\n",
              "        .raw-json-details { margin-top: 12px; border: 1px solid #eee; border-radius: 4px; padding: 8px; background-color: #f9f9f9; }\n",
              "        .raw-json-details summary { font-size: 0.9em; cursor: pointer; color: #5f6368;}\n",
              "        .raw-json-container { white-space: pre-wrap; word-wrap: break-word; max-height: 300px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border-radius: 4px; margin-top: 8px; }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <div class=\"container\">\n",
              "        <h1>Evaluation Report</h1>\n",
              "        <div id=\"summary-section\"></div>\n",
              "        <div id=\"details-section\"></div>\n",
              "    </div>\n",
              "    <script>\n",
              "        const data = {\"eval_case_results\": [{\"eval_case_index\": 0, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is clear, coherent, and concise, fully adhering to the instruction and staying grounded by providing a direct and accurate answer to the question.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The response is completely fluent as it has no grammatical errors and provides a clear and concise answer.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.023809524}}, \"display_text\": \"The capital of France is Paris.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEac6cM4kGLznU5Ccf1wJfyAQDn\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The capital of France is Paris.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 7,\\n    \\\"prompt_tokens\\\": 14,\\n    \\\"total_tokens\\\": 21,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}, {\"eval_case_index\": 1, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response perfectly adheres to the prompt, delivering a well-written and evocative haiku about a cat that is coherent, fluent, concise, and grounded.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The haiku is grammatically correct, uses appropriate word choice, and has a natural flow, resulting in a completely fluent response.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.0}}, \"display_text\": \"Soft paws tread gently,  \\nWhiskers twitch in moonlit glow\\u2014  \\nNight's silent hunter.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaDPCNnd8OnJz7DbOOnU9uaSCi\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"Soft paws tread gently,  \\\\nWhiskers twitch in moonlit glow\\u2014  \\\\nNight's silent hunter.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 21,\\n    \\\"prompt_tokens\\\": 15,\\n    \\\"total_tokens\\\": 36,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}, {\"eval_case_index\": 2, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is very good because it is clear, coherent, fluent, and concise, fully adhering to the instructions by providing an accurate summary of the given text without adding any external information.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The response is completely fluent and summarizes the provided text effectively without any grammatical errors or awkward phrasing.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.25}}, \"display_text\": \"The sun is a nearly perfect sphere of hot plasma located at the center of the Solar System. It has internal convective motion that creates a magnetic field through a dynamo process.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaluIdBPCq7syW7dpy8F8aYUkA\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_07871e2ad8\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The sun is a nearly perfect sphere of hot plasma located at the center of the Solar System. It has internal convective motion that creates a magnetic field through a dynamo process.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 36,\\n    \\\"prompt_tokens\\\": 53,\\n    \\\"total_tokens\\\": 89,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}, {\"eval_case_index\": 3, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is well-written, clear, coherent, and accurately answers the prompt's riddle, making it fully adhere to the instructions.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The response is completely fluent, free of grammatical errors, and has a natural, seamless flow.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.04}}, \"display_text\": \"The answer to this riddle is \\\"the future.\\\" It is always ahead and coming towards you, but it cannot be seen.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaZMv0IyeovtH9scibx7PXTwJZ\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The answer to this riddle is \\\\\\\"the future.\\\\\\\" It is always ahead and coming towards you, but it cannot be seen.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 26,\\n    \\\"prompt_tokens\\\": 19,\\n    \\\"total_tokens\\\": 45,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}, {\"eval_case_index\": 4, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is exceptionally clear, coherent, fluent, and concise, fully adhering to instructions and remaining grounded.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The response is completely fluent, free of grammatical errors, and has a natural flow, making it easy to understand.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.035714287}}, \"display_text\": \"A common answer to this riddle is \\\"your right hand.\\\" You can hold your right hand with your left hand, but you can't hold your right hand with your right hand.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEabPgcuDOs6EnRXHHS8KAeZMRA\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"A common answer to this riddle is \\\\\\\"your right hand.\\\\\\\" You can hold your right hand with your left hand, but you can't hold your right hand with your right hand.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 36,\\n    \\\"prompt_tokens\\\": 22,\\n    \\\"total_tokens\\\": 58,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}, {\"eval_case_index\": 5, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response is clear, concise, and accurately answers the prompt by stating that 9 times 9 is 81.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The response is completely fluent as it is free of grammatical errors and has a natural flow.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.024096385}}, \"display_text\": \"9 times 9 is 81.\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEacYEbjTAhqCxPlnY7Skq8ANz0\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_07871e2ad8\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"9 times 9 is 81.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 8,\\n    \\\"prompt_tokens\\\": 15,\\n    \\\"total_tokens\\\": 23,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}, {\"eval_case_index\": 6, \"response_candidate_results\": [{\"response_index\": 0, \"metric_results\": {\"text_quality\": {\"metric_name\": \"text_quality\", \"score\": 5.0, \"explanation\": \"The response provides a simple Python function to calculate the factorial of a number, adhering to the prompt's instructions by only returning the code.\"}, \"fluency\": {\"metric_name\": \"fluency\", \"score\": 5.0, \"explanation\": \"The response is completely fluent; it contains no grammatical errors, uses appropriate word choice, and has a natural flow.\"}, \"rouge_1\": {\"metric_name\": \"rouge_1\", \"score\": 0.21538462}}, \"display_text\": \"```python\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n```\", \"raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaeDN7AG2q1jGJedrff0uKHdOa\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"```python\\\\ndef factorial(n):\\\\n    if n == 0:\\\\n        return 1\\\\n    else:\\\\n        return n * factorial(n - 1)\\\\n```\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 33,\\n    \\\"prompt_tokens\\\": 25,\\n    \\\"total_tokens\\\": 58,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}], \"summary_metrics\": [{\"metric_name\": \"text_quality\", \"num_cases_total\": 7, \"num_cases_valid\": 7, \"num_cases_error\": 0, \"mean_score\": 5.0, \"stdev_score\": 0.0}, {\"metric_name\": \"fluency\", \"num_cases_total\": 7, \"num_cases_valid\": 7, \"num_cases_error\": 0, \"mean_score\": 5.0, \"stdev_score\": 0.0}, {\"metric_name\": \"rouge_1\", \"num_cases_total\": 7, \"num_cases_valid\": 7, \"num_cases_error\": 0, \"mean_score\": 0.08414354514285714, \"stdev_score\": 0.10275587631791941}], \"metadata\": {\"candidate_names\": [\"gpt-4o\"], \"creation_timestamp\": \"2025-06-26T23:23:55.949131Z\", \"dataset\": [{\"prompt_display_text\": \"What is the capital of France?\", \"prompt_raw_json\": \"\", \"reference\": \"Paris\", \"response_display_text\": \"The capital of France is Paris.\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEac6cM4kGLznU5Ccf1wJfyAQDn\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The capital of France is Paris.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 7,\\n    \\\"prompt_tokens\\\": 14,\\n    \\\"total_tokens\\\": 21,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}, {\"prompt_display_text\": \"Write a haiku about a cat.\", \"prompt_raw_json\": \"\", \"reference\": \"Sunbeam on the floor,\\nA furry puddle sleeping,\\nTwitching tail tells tales.\", \"response_display_text\": \"Soft paws tread gently,  \\nWhiskers twitch in moonlit glow\\u2014  \\nNight's silent hunter.\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaDPCNnd8OnJz7DbOOnU9uaSCi\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"Soft paws tread gently,  \\\\nWhiskers twitch in moonlit glow\\u2014  \\\\nNight's silent hunter.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 21,\\n    \\\"prompt_tokens\\\": 15,\\n    \\\"total_tokens\\\": 36,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}, {\"prompt_display_text\": \"Summarize the following text: The sun is a star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process.\", \"prompt_raw_json\": \"\", \"reference\": \"The sun, a star at the solar system's center, is a hot plasma sphere creating a magnetic field.\", \"response_display_text\": \"The sun is a nearly perfect sphere of hot plasma located at the center of the Solar System. It has internal convective motion that creates a magnetic field through a dynamo process.\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaluIdBPCq7syW7dpy8F8aYUkA\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_07871e2ad8\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The sun is a nearly perfect sphere of hot plasma located at the center of the Solar System. It has internal convective motion that creates a magnetic field through a dynamo process.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 36,\\n    \\\"prompt_tokens\\\": 53,\\n    \\\"total_tokens\\\": 89,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}, {\"prompt_display_text\": \"What is always in front of you but can't be seen?\", \"prompt_raw_json\": \"\", \"reference\": \"The future\", \"response_display_text\": \"The answer to this riddle is \\\"the future.\\\" It is always ahead and coming towards you, but it cannot be seen.\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaZMv0IyeovtH9scibx7PXTwJZ\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"The answer to this riddle is \\\\\\\"the future.\\\\\\\" It is always ahead and coming towards you, but it cannot be seen.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 26,\\n    \\\"prompt_tokens\\\": 19,\\n    \\\"total_tokens\\\": 45,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}, {\"prompt_display_text\": \"What can you hold in your left hand, but not in your right?\", \"prompt_raw_json\": \"\", \"reference\": \"Your right elbow.\", \"response_display_text\": \"A common answer to this riddle is \\\"your right hand.\\\" You can hold your right hand with your left hand, but you can't hold your right hand with your right hand.\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEabPgcuDOs6EnRXHHS8KAeZMRA\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"A common answer to this riddle is \\\\\\\"your right hand.\\\\\\\" You can hold your right hand with your left hand, but you can't hold your right hand with your right hand.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 36,\\n    \\\"prompt_tokens\\\": 22,\\n    \\\"total_tokens\\\": 58,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}, {\"prompt_display_text\": \"what is 9 times 9?\", \"prompt_raw_json\": \"\", \"reference\": \"81\", \"response_display_text\": \"9 times 9 is 81.\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEacYEbjTAhqCxPlnY7Skq8ANz0\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_07871e2ad8\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"9 times 9 is 81.\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 8,\\n    \\\"prompt_tokens\\\": 15,\\n    \\\"total_tokens\\\": 23,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}, {\"prompt_display_text\": \"Write a simple Python function to calculate the factorial of a number. Only return the code.\", \"prompt_raw_json\": \"\", \"reference\": \"def factorial(n):\\n    if n < 0:\\n        return 'Factorial does not exist for negative numbers'\\n    elif n == 0:\\n        return 1\\n    else:\\n        fact = 1\\n        i = 1\\n        while i <= n:\\n            fact *= i\\n            i += 1\\n        return fact\", \"response_display_text\": \"```python\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n```\", \"response_raw_json\": \"{\\n  \\\"id\\\": \\\"chatcmpl-BmqEaeDN7AG2q1jGJedrff0uKHdOa\\\",\\n  \\\"created\\\": 1750980232,\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_a288987b44\\\",\\n  \\\"choices\\\": [\\n    {\\n      \\\"finish_reason\\\": \\\"stop\\\",\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"content\\\": \\\"```python\\\\ndef factorial(n):\\\\n    if n == 0:\\\\n        return 1\\\\n    else:\\\\n        return n * factorial(n - 1)\\\\n```\\\",\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"tool_calls\\\": null,\\n        \\\"function_call\\\": null,\\n        \\\"annotations\\\": []\\n      },\\n      \\\"provider_specific_fields\\\": {}\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"completion_tokens\\\": 33,\\n    \\\"prompt_tokens\\\": 25,\\n    \\\"total_tokens\\\": 58,\\n    \\\"completion_tokens_details\\\": {\\n      \\\"accepted_prediction_tokens\\\": 0,\\n      \\\"audio_tokens\\\": 0,\\n      \\\"reasoning_tokens\\\": 0,\\n      \\\"rejected_prediction_tokens\\\": 0\\n    },\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"audio_tokens\\\": 0,\\n      \\\"cached_tokens\\\": 0\\n    }\\n  },\\n  \\\"service_tier\\\": \\\"default\\\"\\n}\"}]}};\n",
              "        function renderSummary(summaryMetrics) {\n",
              "            const container = document.getElementById('summary-section');\n",
              "            let content = '<h2>Summary Metrics</h2>';\n",
              "            if (!summaryMetrics || summaryMetrics.length === 0) { container.innerHTML = content + '<p>No summary metrics.</p>'; return; }\n",
              "            let table = '<table><thead><tr><th>Metric</th><th>Mean Score</th><th>Std. Dev.</th></tr></thead><tbody>';\n",
              "            summaryMetrics.forEach(m => {\n",
              "                table += `<tr><td>${m.metric_name || 'N/A'}</td><td>${m.mean_score != null ? m.mean_score.toFixed(4) : 'N/A'}</td><td>${m.stdev_score != null ? m.stdev_score.toFixed(4) : 'N/A'}</td></tr>`;\n",
              "            });\n",
              "            container.innerHTML = content + table + '</tbody></table>';\n",
              "        }\n",
              "        function renderDetails(caseResults, metadata) {\n",
              "            const container = document.getElementById('details-section');\n",
              "            container.innerHTML = '<h2>Detailed Results</h2>';\n",
              "            if (!caseResults || caseResults.length === 0) { container.innerHTML += '<p>No detailed results.</p>'; return; }\n",
              "            const datasetRows = metadata && metadata.dataset ? metadata.dataset : [];\n",
              "\n",
              "            caseResults.forEach((caseResult, i) => {\n",
              "                const original_case = datasetRows[caseResult.eval_case_index] || {};\n",
              "                const promptText = original_case.prompt_display_text || '(prompt not found)';\n",
              "                const promptJson = original_case.prompt_raw_json;\n",
              "                const reference = original_case.reference || '';\n",
              "                const responseText = original_case.response_display_text || '(response not found)';\n",
              "                const responseJson = original_case.response_raw_json;\n",
              "\n",
              "                let card = `<details><summary>Case #${caseResult.eval_case_index != null ? caseResult.eval_case_index : i}</summary>`;\n",
              "\n",
              "                card += `<div class=\"prompt-container\"><strong>Prompt:</strong><br>${DOMPurify.sanitize(marked.parse(String(promptText)))}</div>`;\n",
              "                if (promptJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Prompt JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(promptJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                if (reference) { card += `<div class=\"reference-container\"><strong>Reference:</strong><br>${DOMPurify.sanitize(marked.parse(String(reference)))}</div>`; }\n",
              "\n",
              "                card += `<div class=\"response-container\"><h4>Candidate Response</h4>${DOMPurify.sanitize(marked.parse(String(responseText)))}</div>`;\n",
              "                if (responseJson) {\n",
              "                    card += `<details class=\"raw-json-details\"><summary>View Raw Response JSON</summary><pre class=\"raw-json-container\">${DOMPurify.sanitize(responseJson)}</pre></details>`;\n",
              "                }\n",
              "\n",
              "                let metricTable = '<h4>Metrics</h4><table><tbody>';\n",
              "                const candidateMetrics = (caseResult.response_candidate_results && caseResult.response_candidate_results[0] && caseResult.response_candidate_results[0].metric_results) || {};\n",
              "                Object.entries(candidateMetrics).forEach(([name, val]) => {\n",
              "                    metricTable += `<tr><td>${name}</td><td><b>${val.score != null ? val.score.toFixed(2) : 'N/A'}</b></td></tr>`;\n",
              "                    if (val.explanation) { metricTable += `<tr><td colspan=\"2\"><div class=\"explanation\">${DOMPurify.sanitize(marked.parse(String(val.explanation)))}</div></td></tr>`; }\n",
              "                });\n",
              "                card += metricTable + '</tbody></table>';\n",
              "                container.innerHTML += card + '</details>';\n",
              "            });\n",
              "        }\n",
              "        renderSummary(data.summary_metrics);\n",
              "        renderDetails(data.eval_case_results, data.metadata);\n",
              "    </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asynchronous Batch-style Evaluation\n",
        "\n",
        "\n",
        "For large datasets, you can use `batch_evaluate()` to run evaluations as a long-running, asynchronous operation, which is ideal for large-scale jobs. This method provides an SDK interface for the batch-style `EvaluateDataset` API and is distinct from the synchronous, online `evaluate()` method.\n",
        "\n",
        "The `batch_evaluate()` method returns a job object that you can poll to track its progress. Once the job completes successfully, you can retrieve and visualize the results. The parameters for `batch_evaluate()` are compatible with the `evaluate()` method, allowing for a seamless transition between the two.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGKplIC6dJbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_DEST_BUCKET = \"\"  # @param {type:\"string\", placeholder: \"[your-gcs-bucket]\"}\n",
        "\n",
        "inference_result_saved = client.evals.run_inference(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    src=\"gs://vertex-evaluation-llm-dataset-us-central1/genai_eval_sdk/test_prompts.jsonl\",\n",
        "    config={'dest': GCS_DEST_BUCKET}\n",
        ")\n",
        "print(f\"Inference dataset uploaded to: {inference_result_saved.gcs_source}\")\n",
        "\n",
        "batch_eval_job  = client.evals.batch_evaluate(\n",
        "   dataset = inference_result_saved,\n",
        "   metrics = [\n",
        "        types.PrebuiltMetric.TEXT_QUALITY,\n",
        "        types.PrebuiltMetric.INSTRUCTION_FOLLOWING,\n",
        "        types.PrebuiltMetric.FLUENCY,\n",
        "        types.Metric(name='bleu'),\n",
        "    ],\n",
        "   dest=GCS_DEST_BUCKET\n",
        ")\n",
        "batch_eval_job"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv0K7A7LdnW3",
        "outputId": "954fa73e-5556-43c7-e2a2-bc8e037a2ad4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gemini Inference: 100%|██████████| 7/7 [00:01<00:00,  5.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference dataset uploaded to: uris=['gs://batch-eval-test-data/sdk_output/batch_eval/inference_results.jsonl']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluateDatasetOperation(\n",
              "  metadata={\n",
              "    '@type': 'type.googleapis.com/google.cloud.aiplatform.v1beta1.EvaluateDatasetOperationMetadata',\n",
              "    'genericMetadata': {\n",
              "      'createTime': '2025-06-26T23:27:37.519723Z',\n",
              "      'updateTime': '2025-06-26T23:27:37.519723Z'\n",
              "    }\n",
              "  },\n",
              "  name='projects/977012026409/locations/us-central1/operations/5655938219015929856'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title view results\n",
        "def gcs_path_to_console_url(gcs_path: str) -> str:\n",
        "    if not gcs_path.startswith(\"gs://\"):\n",
        "        raise ValueError(\"Invalid GCS path. Must start with 'gs://'\")\n",
        "\n",
        "    # Remove the 'gs://' prefix\n",
        "    bucket_and_path = gcs_path[5:]\n",
        "\n",
        "    # Construct the console URL\n",
        "    console_url = f\"https://console.cloud.google.com/storage/browser/{bucket_and_path}\"\n",
        "    return console_url\n",
        "\n",
        "url = gcs_path_to_console_url(GCS_DEST_BUCKET)\n",
        "print(f\"Results will be written to your GCS destination path: {GCS_DEST_BUCKET}\\n\", url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JKb9FgUOgJMG",
        "outputId": "60b48d27-5f7d-4f92-f761-b8ee41c86c03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results will be written to your GCS destination path: gs://batch-eval-test-data/sdk_output/batch_eval\n",
            " https://console.cloud.google.com/storage/browser/batch-eval-test-data/sdk_output/batch_eval\n"
          ]
        }
      ]
    }
  ]
}