- messages:
    - additional_kwargs: {}
      content:
        - text: Explain what's MLOps
          type: text
      example: false
      id: null
      name: null
      response_metadata: {}
      type: human
    - additional_kwargs: {}
      content: ""
      example: false
      id: null
      invalid_tool_calls: []
      name: null
      response_metadata: {}
      tool_calls:
        - args:
            query: What is MLOps
          id: retrieve_docs
          name: retrieve_docs
          type: tool_call
      type: ai
      usage_metadata: null
    - additional_kwargs: {}
      artifact:
        - id: null
          metadata:
            id: bb374a79-3a46-47be-8487-87864bc390fa
            relevance_score: 0.9900000095367432
          page_content:
            "• Avoiding training-serving skews that are due to inconsistencies\
            \ in data and in runtime dependencies between \ntraining environments and\
            \ serving environments.\n• Handling concerns about model fairness and adversarial\
            \ attacks.\nMLOps is a methodology for ML engineering that unifies ML system\
            \ development (the ML element) with ML system \noperations (the Ops element).\
            \ It advocates formalizing and (when beneficial) automating critical steps\
            \ of ML system \nconstruction. MLOps provides a set of standardized processes\
            \ and technology capabilities for building, deploying, \nand operationalizing\
            \ ML systems rapidly and reliably.\nMLOps supports ML development and deployment\
            \ in the way that DevOps and DataOps support application engi -\nneering and\
            \ data engineering (analytics). The difference is that when you deploy a web\
            \ service, you care about resil -\nience, queries per second, load balancing,\
            \ and so on. When you deploy an ML model, you also need to worry about"
          type: Document
        - id: null
          metadata:
            id: 21bf7fc0-8319-4bb2-927c-7810426dd705
            relevance_score: 0.9900000095367432
          page_content:
            "Executive summary\nAcross industries, DevOps and DataOps have\
            \ been widely adopted as methodologies to improve quality and re -\nduce the\
            \ time to market of software engineering and data engineering initiatives.\
            \ With the rapid growth in machine \nlearning (ML) systems, similar approaches\
            \ need to be developed in the context of ML engineering, which handle the\
            \ \nunique complexities of the practical applications of ML. This is the domain\
            \ of MLOps. MLOps is a set of standard -\nized processes and technology capabilities\
            \ for building, deploying, and operationalizing ML systems rapidly and \n\
            reliably.]\nWe previously published Google Cloud’s AI Adoption Framework \
            \ to provide guidance for technology leaders who \nwant to build an effective\
            \ artificial intelligence (AI) capability in order to transform their business.\
            \ That framework \ncovers AI challenges around people, data, technology, and\
            \ process, structured in six different themes: learn, lead, \naccess, secure,\
            \ scale, and automate ."
          type: Document
        - id: null
          metadata:
            id: 1b6b6734-cc97-4d59-ac43-aa8ccefe5f15
            relevance_score: 0.6800000071525574
          page_content:
            "ience, queries per second, load balancing, and so on. When you\
            \ deploy an ML model, you also need to worry about \nchanges in the data,\
            \ changes in the model, users trying to game the system, and so on. This is\
            \ what MLOps is about.\nMLOps practices can result in the following benefits\
            \ over systems that do not follow MLOps practices:\n• Shorter development\
            \ cycles, and as a result, shorter time to market.\n• Better collaboration\
            \ between teams.\n• Increased reliability, performance, scalability, and security\
            \ of ML systems. \n• Streamlined operational and governance processes. \n\
            • Increased return on investment of ML projects.\nIn this section, you learn\
            \ about the MLOps lifecycle and workflow, and about the individual capabilities\
            \ that are re -\n5 Towards ML Engineering , Google, 2020."
          type: Document
        - id: null
          metadata:
            id: eaeddc49-46b6-4ca7-910c-ce4ec362bb36
            relevance_score: 0.6700000166893005
          page_content:
            "8\n• Prediction serving  is about serving the model that is deployed\
            \ in production for inference.\n• Continuous monitoring  is about monitoring\
            \ the effectiveness and efficiency of a deployed model.\n• Data and model\
            \ management  is a central, cross-cutting function for governing ML artifacts\
            \ to support audit -\nability, traceability, and compliance. Data and model\
            \ management can also promote shareability, reusability, \nand discoverability\
            \ of ML assets.\nMLOps: An end-to-end workflow\nFigure 3 shows a simplified\
            \ but canonical flow for how the MLOps processes interact with each other,\
            \ focusing on \nhigh-level flow of control and on key inputs and outputs.\n\
            This is not a waterfall workflow that has to sequentially pass through all\
            \ the processes. The processes can be \nskipped, or the flow can repeat a\
            \ given phase or a subsequence of the processes. The diagram shows the following\
            \ \nflow:\n1. The core activity during this ML development phase is experimentation.\
            \ As data scientists and ML research -"
          type: Document
        - id: null
          metadata:
            id: 87d53996-add4-4110-9f50-4e3287c088b9
            relevance_score: 0.6399999856948853
          page_content:
            "10\nSome foundational capabilities are required in order to support\
            \ any IT workload, such as a reliable, scalable, and \nsecure compute infrastructure.\
            \ Most organizations already have investments in these capabilities and can\
            \ benefit by \ntaking advantage of them for ML workflows. Such capabilities\
            \ might span multiple clouds, or even operate partially \non-premises. Ideally,\
            \ this would include advanced capabilities such as specialized ML accelerators.\n\
            In addition, an organization needs standardized configuration management and\
            \ CI/CD capabilities to build, test, \nrelease, and operate software systems\
            \ rapidly and reliably, including ML systems.\nOn top of these foundational\
            \ capabilities is a set of core MLOps capabilities. These include experimentation,\
            \ data \nprocessing, model training, model evaluation, model serving, online\
            \ experimentation, model monitoring, ML pipeline, \nand model registry. Finally,\
            \ two cross-cutting capabilities that enable integration and interaction are\
            \ an ML metadata"
          type: Document
      content:
        "## Context provided:\n\n<Document 0>\n• Avoiding training-serving skews\
        \ that are due to inconsistencies in data and in runtime dependencies between\
        \ \ntraining environments and serving environments.\n• Handling concerns about\
        \ model fairness and adversarial attacks.\nMLOps is a methodology for ML engineering\
        \ that unifies ML system development (the ML element) with ML system \noperations\
        \ (the Ops element). It advocates formalizing and (when beneficial) automating\
        \ critical steps of ML system \nconstruction. MLOps provides a set of standardized\
        \ processes and technology capabilities for building, deploying, \nand operationalizing\
        \ ML systems rapidly and reliably.\nMLOps supports ML development and deployment\
        \ in the way that DevOps and DataOps support application engi -\nneering and\
        \ data engineering (analytics). The difference is that when you deploy a web\
        \ service, you care about resil -\nience, queries per second, load balancing,\
        \ and so on. When you deploy an ML model, you also need to worry about\n</Document\
        \ 0>\n\n<Document 1>\nExecutive summary\nAcross industries, DevOps and DataOps\
        \ have been widely adopted as methodologies to improve quality and re -\nduce\
        \ the time to market of software engineering and data engineering initiatives.\
        \ With the rapid growth in machine \nlearning (ML) systems, similar approaches\
        \ need to be developed in the context of ML engineering, which handle the \n\
        unique complexities of the practical applications of ML. This is the domain\
        \ of MLOps. MLOps is a set of standard -\nized processes and technology capabilities\
        \ for building, deploying, and operationalizing ML systems rapidly and \nreliably.]\n\
        We previously published Google Cloud’s AI Adoption Framework  to provide guidance\
        \ for technology leaders who \nwant to build an effective artificial intelligence\
        \ (AI) capability in order to transform their business. That framework \ncovers\
        \ AI challenges around people, data, technology, and process, structured in\
        \ six different themes: learn, lead, \naccess, secure, scale, and automate .\n\
        </Document 1>\n\n<Document 2>\nience, queries per second, load balancing, and\
        \ so on. When you deploy an ML model, you also need to worry about \nchanges\
        \ in the data, changes in the model, users trying to game the system, and so\
        \ on. This is what MLOps is about.\nMLOps practices can result in the following\
        \ benefits over systems that do not follow MLOps practices:\n• Shorter development\
        \ cycles, and as a result, shorter time to market.\n• Better collaboration between\
        \ teams.\n• Increased reliability, performance, scalability, and security of\
        \ ML systems. \n• Streamlined operational and governance processes. \n• Increased\
        \ return on investment of ML projects.\nIn this section, you learn about the\
        \ MLOps lifecycle and workflow, and about the individual capabilities that are\
        \ re -\n5 Towards ML Engineering , Google, 2020.\n</Document 2>\n\n<Document\
        \ 3>\n8\n• Prediction serving  is about serving the model that is deployed in\
        \ production for inference.\n• Continuous monitoring  is about monitoring the\
        \ effectiveness and efficiency of a deployed model.\n• Data and model management\
        \  is a central, cross-cutting function for governing ML artifacts to support\
        \ audit -\nability, traceability, and compliance. Data and model management\
        \ can also promote shareability, reusability, \nand discoverability of ML assets.\n\
        MLOps: An end-to-end workflow\nFigure 3 shows a simplified but canonical flow\
        \ for how the MLOps processes interact with each other, focusing on \nhigh-level\
        \ flow of control and on key inputs and outputs.\nThis is not a waterfall workflow\
        \ that has to sequentially pass through all the processes. The processes can\
        \ be \nskipped, or the flow can repeat a given phase or a subsequence of the\
        \ processes. The diagram shows the following \nflow:\n1. The core activity during\
        \ this ML development phase is experimentation. As data scientists and ML research\
        \ -\n</Document 3>\n\n<Document 4>\n10\nSome foundational capabilities are required\
        \ in order to support any IT workload, such as a reliable, scalable, and \n\
        secure compute infrastructure. Most organizations already have investments in\
        \ these capabilities and can benefit by \ntaking advantage of them for ML workflows.\
        \ Such capabilities might span multiple clouds, or even operate partially \n\
        on-premises. Ideally, this would include advanced capabilities such as specialized\
        \ ML accelerators.\nIn addition, an organization needs standardized configuration\
        \ management and CI/CD capabilities to build, test, \nrelease, and operate software\
        \ systems rapidly and reliably, including ML systems.\nOn top of these foundational\
        \ capabilities is a set of core MLOps capabilities. These include experimentation,\
        \ data \nprocessing, model training, model evaluation, model serving, online\
        \ experimentation, model monitoring, ML pipeline, \nand model registry. Finally,\
        \ two cross-cutting capabilities that enable integration and interaction are\
        \ an ML metadata\n</Document 4>"
      id: null
      name: retrieve_docs
      response_metadata: {}
      status: success
      tool_call_id: retrieve_docs
      type: tool
    - additional_kwargs: {}
      content:
        MLOps is a set of standardized processes and technology capabilities
        designed to build, deploy, and operationalize machine learning (ML) systems
        rapidly and reliably.  It combines ML system development ("ML") with ML system
        operations ("Ops"), similar to how DevOps and DataOps streamline software and
        data engineering.  MLOps addresses the unique complexities of ML applications,
        including concerns about data changes, model changes, and adversarial attacks,
        aiming for shorter development cycles, better team collaboration, increased
        system reliability and performance, and improved return on investment.  A key
        aspect is managing the entire ML lifecycle, from experimentation and data processing
        to model training, evaluation, serving, monitoring, and management of ML artifacts.
      example: false
      id: d20a662a-13f8-4101-821e-0366cba66b51
      invalid_tool_calls: []
      name: null
      response_metadata: {}
      tool_calls: []
      type: ai
      usage_metadata: null
    - additional_kwargs: {}
      content:
        - text: How can I evaluate my models?
          type: text
      example: false
      id: null
      name: null
      response_metadata: {}
      type: human
    - additional_kwargs: {}
      content: ""
      example: false
      id: null
      invalid_tool_calls: []
      name: null
      response_metadata: {}
      tool_calls:
        - args:
            query: How to evaluate machine learning models
          id: retrieve_docs
          name: retrieve_docs
          type: tool_call
      type: ai
      usage_metadata: null
    - additional_kwargs: {}
      artifact:
        - id: null
          metadata:
            id: ae3635dd-8736-44d9-b06a-ff0ba7ba2804
            relevance_score: 0.9599999785423279
          page_content:
            "34\nFigure 14 shows the tasks that are involved in model governance.\n\
            Model governance can use information in the ML metadata and the model registry\
            \ to do the following tasks:\n• Store:  Add or update model properties and\
            \ track model versions and property changes. The model registry can \nstore\
            \ many model versions from the experimentation and continuous training phases,\
            \ which lets data scientists \neasily reproduce significant models.\n• Evaluate:\
            \  Compare a new challenger model to the champion model by looking not only\
            \ at evaluation metrics \n(accuracy, precision, recall, specificity, and so\
            \ on) but also at business KPIs that are collected through online ex -\nperimentation.\
            \ Additionally, model owners need to be able to understand and explain the\
            \ model predictions—for \nexample, by using feature attribution methods. This\
            \ ensures the quality of the model that is deployed in produc -\ntion."
          type: Document
        - id: null
          metadata:
            id: ecb53ce1-e74e-4130-8f37-34471ce57e08
            relevance_score: 0.9200000166893005
          page_content:
            "12\nModel training should be able to scale with the size of both\
            \ the models and the datasets that are used for training. \nKey functionalities\
            \ in model training include the following:\n• Support common ML frameworks\
            \ and support custom runtime environments.\n• Support large-scale distributed\
            \ training with different strategies for multiple GPUs and multiple workers.\n\
            • Enable on-demand use of ML accelerators.\n• Allow efficient hyperparameter\
            \ tuning and target optimization at scale.\n• Ideally, provide built-in automated\
            \ ML (AutoML) functionality, including automated feature selection and engi\
            \ -\nneering as well as automated model architecture search and selection.\n\
            Model evaluation\nThe model evaluation capability lets you assess the effectiveness\
            \ of your model, interactively during experimentation \nand automatically\
            \ in production. Key functionalities in model evaluation include the following:\n\
            • Perform batch scoring of your models on evaluation datasets at scale."
          type: Document
        - id: null
          metadata:
            id: 2cd1f8d9-6e3d-437a-b73a-d92f6a819f11
            relevance_score: 0.9200000166893005
          page_content:
            "and automatically in production. Key functionalities in model\
            \ evaluation include the following:\n• Perform batch scoring of your models\
            \ on evaluation datasets at scale.\n• Compute pre-defined or custom evaluation\
            \ metrics for your model on different slices of the data.\n• Track trained-model\
            \ predictive performance across different continuous-training executions.\n\
            • Visualize and compare performances of different models.\n• Provide tools\
            \ for what-if analysis and for identifying bias and fairness issues.\n• Enable\
            \ model behavior interpretation using various explainable AI techniques.\n\
            Model serving\nThe model serving capability lets you deploy and serve your\
            \ models in production environments. Key functionalities \nin model serving\
            \ include the following:\n• Provide support for low-latency, near-real-time\
            \ (online) prediction and high-throughput batch (offline)  \nprediction.\n\
            • Provide built-in support for common ML serving frameworks (for example,\
            \ TensorFlow Serving , TorchServe ,"
          type: Document
        - id: null
          metadata:
            id: 795c184f-e5bd-45a9-8ec3-3baba0623672
            relevance_score: 0.8299999833106995
          page_content:
            "21\n5. Model evaluation. The model is evaluated against the test\
            \ data split to assess the performance of the model \nusing various evaluation\
            \ metrics on different partitions of the data.\n6. Model validation. The results\
            \ of model evaluations are validated to make sure that the model meets the\
            \ \nexpected performance criteria.\n7. Model registration. The validated model\
            \ is stored in a model registry along with its metadata.\nAs the diagram shows,\
            \ the continuous training pipeline runs based on a retraining trigger. When\
            \ the pipeline starts, it \nextracts a fresh training dataset from the dataset\
            \ and feature repository, executes the steps in the ML workflow, and \nsubmits\
            \ a trained model to the model registry. All the run information and the artifacts\
            \ that are produced throughout \nthe pipeline run are tracked in the metadata\
            \ and artifact repository.\nAn orchestrated and automated training pipeline\
            \ mirrors the steps of the typical data science process that runs in"
          type: Document
        - id: null
          metadata:
            id: 8e6fcd4e-0379-4999-8196-93b635ab726d
            relevance_score: 0.5899999737739563
          page_content:
            "22\nThe data validation step can detect when data anomalies start\
            \ occurring. \nThese anomalies can include new features, new feature domains,\
            \ fea -\ntures that have disappeared, and drastic changes in feature distributions.\
            \ \nThe data validation step works by comparing the new training data to the\
            \ \nexpected data schema and reference statistics. For details about how data\
            \ \nvalidation works, see Analyzing and validating data at scale for machine\
            \ \nlearning with TensorFlow Data Validation .\nThe model validation phase\
            \ detects anomalies when a lack of improve -\nment or even a degradation in\
            \ performance of new model candidates is \nobserved. The pipeline can apply\
            \ complex validation logic in this step, \nincluding several evaluation metrics,\
            \ sensitivity analysis based on specific \ninputs, calibration, and fairness\
            \ indicators.\nAn important aspect of continuous training, therefore, is tracking.\
            \ Pipeline \nruns must track generated metadata and artifacts in a way that\
            \ enables"
          type: Document
      content:
        "## Context provided:\n\n<Document 0>\n34\nFigure 14 shows the tasks\
        \ that are involved in model governance.\nModel governance can use information\
        \ in the ML metadata and the model registry to do the following tasks:\n• Store:\
        \  Add or update model properties and track model versions and property changes.\
        \ The model registry can \nstore many model versions from the experimentation\
        \ and continuous training phases, which lets data scientists \neasily reproduce\
        \ significant models.\n• Evaluate:  Compare a new challenger model to the champion\
        \ model by looking not only at evaluation metrics \n(accuracy, precision, recall,\
        \ specificity, and so on) but also at business KPIs that are collected through\
        \ online ex -\nperimentation. Additionally, model owners need to be able to\
        \ understand and explain the model predictions—for \nexample, by using feature\
        \ attribution methods. This ensures the quality of the model that is deployed\
        \ in produc -\ntion.\n</Document 0>\n\n<Document 1>\n12\nModel training should\
        \ be able to scale with the size of both the models and the datasets that are\
        \ used for training. \nKey functionalities in model training include the following:\n\
        • Support common ML frameworks and support custom runtime environments.\n• Support\
        \ large-scale distributed training with different strategies for multiple GPUs\
        \ and multiple workers.\n• Enable on-demand use of ML accelerators.\n• Allow\
        \ efficient hyperparameter tuning and target optimization at scale.\n• Ideally,\
        \ provide built-in automated ML (AutoML) functionality, including automated\
        \ feature selection and engi -\nneering as well as automated model architecture\
        \ search and selection.\nModel evaluation\nThe model evaluation capability lets\
        \ you assess the effectiveness of your model, interactively during experimentation\
        \ \nand automatically in production. Key functionalities in model evaluation\
        \ include the following:\n• Perform batch scoring of your models on evaluation\
        \ datasets at scale.\n</Document 1>\n\n<Document 2>\nand automatically in production.\
        \ Key functionalities in model evaluation include the following:\n• Perform\
        \ batch scoring of your models on evaluation datasets at scale.\n• Compute pre-defined\
        \ or custom evaluation metrics for your model on different slices of the data.\n\
        • Track trained-model predictive performance across different continuous-training\
        \ executions.\n• Visualize and compare performances of different models.\n•\
        \ Provide tools for what-if analysis and for identifying bias and fairness issues.\n\
        • Enable model behavior interpretation using various explainable AI techniques.\n\
        Model serving\nThe model serving capability lets you deploy and serve your models\
        \ in production environments. Key functionalities \nin model serving include\
        \ the following:\n• Provide support for low-latency, near-real-time (online)\
        \ prediction and high-throughput batch (offline)  \nprediction.\n• Provide built-in\
        \ support for common ML serving frameworks (for example, TensorFlow Serving\
        \ , TorchServe ,\n</Document 2>\n\n<Document 3>\n21\n5. Model evaluation. The\
        \ model is evaluated against the test data split to assess the performance of\
        \ the model \nusing various evaluation metrics on different partitions of the\
        \ data.\n6. Model validation. The results of model evaluations are validated\
        \ to make sure that the model meets the \nexpected performance criteria.\n7.\
        \ Model registration. The validated model is stored in a model registry along\
        \ with its metadata.\nAs the diagram shows, the continuous training pipeline\
        \ runs based on a retraining trigger. When the pipeline starts, it \nextracts\
        \ a fresh training dataset from the dataset and feature repository, executes\
        \ the steps in the ML workflow, and \nsubmits a trained model to the model registry.\
        \ All the run information and the artifacts that are produced throughout \n\
        the pipeline run are tracked in the metadata and artifact repository.\nAn orchestrated\
        \ and automated training pipeline mirrors the steps of the typical data science\
        \ process that runs in\n</Document 3>\n\n<Document 4>\n22\nThe data validation\
        \ step can detect when data anomalies start occurring. \nThese anomalies can\
        \ include new features, new feature domains, fea -\ntures that have disappeared,\
        \ and drastic changes in feature distributions. \nThe data validation step works\
        \ by comparing the new training data to the \nexpected data schema and reference\
        \ statistics. For details about how data \nvalidation works, see Analyzing and\
        \ validating data at scale for machine \nlearning with TensorFlow Data Validation\
        \ .\nThe model validation phase detects anomalies when a lack of improvement\
        \ or even a degradation in performance of new model candidates is \nobserved.\
        \ The pipeline can apply complex validation logic in this step, \nincluding\
        \ several evaluation metrics, sensitivity analysis based on specific \ninputs,\
        \ calibration, and fairness indicators.\nAn important aspect of continuous training,\
        \ therefore, is tracking. Pipeline \nruns must track generated metadata and\
        \ artifacts in a way that enables\n</Document 4>"
      id: null
      name: retrieve_docs
      response_metadata: {}
      status: success
      tool_call_id: retrieve_docs
      type: tool
    - additional_kwargs: {}
      content:
        "Model evaluation depends heavily on the type of model and the problem
        you're solving.  There's no single answer, but here are some key aspects:


        **1. Choosing the Right Metrics:**  The metrics you use depend on the type of
        problem:


        * **Classification:** Accuracy, precision, recall, F1-score, AUC-ROC, log loss
        are common.  Consider which type of error is more costly (false positive vs.
        false negative) when choosing metrics.

        * **Regression:** Mean Squared Error (MSE), Root Mean Squared Error (RMSE),
        Mean Absolute Error (MAE), R-squared are frequently used.  The choice depends
        on the sensitivity to outliers and the scale of your target variable.

        * **Clustering:** Silhouette score, Davies-Bouldin index, Calinski-Harabasz
        index measure the quality of clusters.

        * **Ranking:** Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision
        (MAP) are used to evaluate ranking models.


        **2. Data Splitting:**  Crucially, you need to split your data into at least
        three sets:


        * **Training set:** Used to train the model.

        * **Validation set:** Used to tune hyperparameters and select the best model
        during training.  This prevents overfitting to the training data.

        * **Test set:** Used for a final, unbiased evaluation of the model's performance
        *after* training and hyperparameter tuning are complete.  This gives a realistic
        estimate of how the model will perform on unseen data.


        **3. Cross-Validation:** For smaller datasets, k-fold cross-validation is a
        powerful technique.  It involves splitting the data into k folds, training the
        model k times (each time using a different fold as the validation set), and
        averaging the performance across all k folds. This provides a more robust estimate
        of performance than a single train-test split.


        **4. Error Analysis:**  Don't just look at the numbers.  Analyze the types
        of errors your model is making.  This can reveal weaknesses in the model or
        the data, and guide improvements.  Visualizations (e.g., confusion matrices
        for classification) are very helpful here.


        **5. Business Metrics:**  Ultimately, the best model is the one that best achieves
        your business goals.  Consider metrics relevant to your specific application,
        such as cost savings, increased revenue, or improved customer satisfaction.


        **6. Fairness and Bias:**  Assess your model for potential biases.  Does it
        perform differently across different demographic groups?  Addressing bias is
        crucial for ethical and responsible AI.


        **7. Explainability:**  For some applications, understanding *why* a model makes
        a particular prediction is important.  Techniques like SHAP values or LIME can
        help explain model predictions.


        **8. Monitoring:**  Even after deployment, continue to monitor your model's
        performance.  Data drift (changes in the input data distribution) can degrade
        performance over time.  Regular retraining or model updates may be necessary.



        Remember to document your evaluation process thoroughly, including the metrics
        used, the data splits, and the results.  This ensures reproducibility and allows
        for comparison of different models."
      example: false
      id: efd86751-f6e3-475e-9ccc-a747f012613d
      invalid_tool_calls: []
      name: null
      response_metadata: {}
      tool_calls: []
      type: ai
      usage_metadata: null
  title: Understanding MLOps
  update_time: "2024-10-22T13:32:44.209150"
