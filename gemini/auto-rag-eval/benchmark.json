[
    {
        "context": [
            "\u2022 Data Parallelism (DP): Models and optimizer states are\nreplicated across multiple devices with data evenly dis\ntributed to all. For LLMs training, the Zero Redundancy\nOptimizer (ZeRO) [19] further enhances this method by\nsharding these states on each data parallel process and\nusing allgather and reduce-scatter for parameter fetching\nand gradient calculation. \u2022 Tensor Parallelism (TP): This involves placing a model\nlayer on multiple GPUs that perform computations in\nparallel [13] [14]. It includes row-wise and column-wise\nparallelism, necessitating allgather and all2all for input\nsplitting and output merging. \u2022 Expert Parallelism (EP): MoE Models\u2019 different expert\nmodels are distributed on different GPUs during MoE\ntraining [15]\u2013[17]. The gate model selects tokens for al\nlocation during input, with corresponding tokens sent to\nexperts model via all2all communication. \u2022 Fully Sharded Data Parallel (FSDP) is an implemen\ntation based on the ZeRO Stage 3 algorithm [19]. FSDP\npartitions the model\u2019s parameters, optimizer states, and\ngradients, distributing them across different GPUs, with\neach GPU retaining only 1/n of the total. During for\nward propagation, FSDP performs an allgather operation\nto assemble the complete parameters, which are then re\nleased after the forward pass is completed. Similarly, dur\ning backward propagation, FSDP conducts an allgather\noperation to obtain the complete parameters, followed\nby backward computation to calculate gradients. It then\nperforms a reduce-scatter operation to synchronize gra\ndients across all GPUs, resulting in each GPU holding\n1/n of the reduced gradients. Finally, FSDP updates the\n1/n parameters using each GPU\u2019s 1/n gradients and opti\nmizer states. F S D P   r e d u c e s   G P U   m e m o r y   u s a g e   b y   m a i n   t a i n i n g   o n l y   1 / n   o f   t h e   p a r a m e t e r s ,   g r a d i e n t s ,   a n d   o p t i \n m i z e r   s t a t e s   o n   e a c h   G P U ,   e n a b l i n g   t r a i n i n g   o f   l a r g e r \n s c a l e   m o d e l s .",
            "Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm op\nerations and MLA up-projections during back-propagation, thereby eliminating the need to\npersistently store their output activations. With a minor overhead, this strategy significantly\nreduces memory requirements for storing activations. Exponential Moving Average in CPU. During training, we preserve the Exponential Mov\ning Average (EMA) of the model parameters for early estimation of the model performance\nafter learning rate decay. The EMA parameters are stored in CPU memory and are updated\nasynchronously after each training step. This method allows us to maintain EMA parameters\nwithout incurring additional memory or time overhead. Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy,\nwe deploy the shallowest layers (including the embedding layer) and deepest layers (including\nthe output head) of the model on the same PP rank. This arrangement enables the physical\nsharing of parameters and gradients, of the shared embedding and output head, between the\nMTP module and the main model. This physical sharing mechanism further enhances our\nmemory efficiency. 3.3. FP8 Training\nInspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022;\nPeng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8\ndata format for training DeepSeek-V3. While low-precision training holds great promise, it\nis often limited by the presence of outliers in activations, weights, and gradients (Fishman\net al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in in\nference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies\ndemonstrating successful application of low-precision techniques in large-scale language model 14 \u03a3 Fprop FP32 Input To FP8 BF16 Weight \u03a3 Dgrad FP32 Input\nGradient Output Output\nGradient\nBF16 To FP8 \u03a3 Wgrad FP32 To FP8 To FP8 Weight\nGradient Optimizer\nStates To\nBF16 Master\nWeight To FP8 To BF16 To BF16 To FP32 \u6216\u8005Input->Activation_L\nOutput->Activation_{L+1} FP32 Figure 6 | The overall mixed precision framework with FP8 data format. To address this challenge and effectively extend the dynamic\nrange of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping\nwith 1 \u00d7 elements or block-wise grouping with \u00d7 elements. The associated dequantiza\ntion overhead is largely mitigated under our increased-precision accumulation process, a critical\naspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further\nreduce memory and communication overhead in MoE training, we cache and dispatch activa\ntions in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8\nmixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeek\nV2, training for approximately 1 trillion tokens (see more details in Appendix B.1).",
            "[19] S. Rajbhandari, J. He, \u201cZero: Memory op\ntimizations toward training trillion parameter models,\u201d in SC20: In\nternational Conference for High Performance Computing, Networking,\nStorage and Analysis. IEEE, 2020, pp. 1\u201316. I, II-B1, II-B1, V-B3\n[20] HFAiLab, \u201cHai platform: A high-performance deep learning training\nplatform with task-level gpu compute time-sharing scheduling,\u201d 2023. [Online]. Available: https://github.com/HFAiLab/hai-platform I\n[21] A. Krizhevsky, I. Sutskever, and G. Vaswani, N. Shazeer, N. Jones, A. N. Polosukhin, \u201cAttention is all you need,\u201d Ad\nvances in neural information processing systems, vol. 30, 2017. W. Nelson, A. Bridgland et al., \u201cImproved protein\nstructure prediction using potentials from deep learning,\u201d Nature, vol. 577, no. 7792, pp. 706\u2013710, 2020. II-A\n[25] D. Silver, T. Hubert, J. Schrittwieser, I. 6419, pp. 1140\u20131144, 2018. II-A\n[26] L. Floridi and M. Chiriatti, \u201cGpt-3: Its nature, scope, limits, and con\nsequences,\u201d Minds and Machines, vol. 30, pp. 681\u2013694, 2020. II-A\n[27] A. Gehrmann et al., \u201cPalm: Scal\ning language modeling with pathways,\u201d Journal of Machine Learning\nResearch, vol. 24, no. 240, pp. 1\u2013113, 2023. Hinton, \u201cAdaptive\nmixtures of local experts,\u201d Neural computation, vol. 3, no. 2, pp. 181\u2013214,"
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Curious Onlooker",
            "Difficulty": "Mild"
        },
        "Question": "When training large language models, what are the different computational strategies employed to reduce GPU memory usage and how do these strategies achieve memory efficiency?",
        "Answer": "Several computational strategies are employed to reduce GPU memory usage during large language model training. Data Parallelism, enhanced by ZeRO, shards model states across devices. Tensor Parallelism distributes model layers across GPUs. Expert Parallelism distributes different expert models on different GPUs during MoE training. Fully Sharded Data Parallelism (FSDP) partitions model parameters, optimizer states, and gradients across GPUs, with each GPU holding only a fraction. Recomputation avoids storing activations by recalculating them during backpropagation. Exponential Moving Average (EMA) stores parameters in CPU memory. Sharing embedding and output head facilitates physical sharing of parameters and gradients. Finally, the FP8 mixed precision framework utilizes lower-precision FP8 data format and BF16 to reduce memory usage and communication overhead."
    },
    {
        "context": [
            "\u2022 Data Parallelism (DP): Models and optimizer states are\nreplicated across multiple devices with data evenly dis\ntributed to all. For LLMs training, the Zero Redundancy\nOptimizer (ZeRO) [19] further enhances this method by\nsharding these states on each data parallel process and\nusing allgather and reduce-scatter for parameter fetching\nand gradient calculation. \u2022 Tensor Parallelism (TP): This involves placing a model\nlayer on multiple GPUs that perform computations in\nparallel [13] [14]. It includes row-wise and column-wise\nparallelism, necessitating allgather and all2all for input\nsplitting and output merging. \u2022 Expert Parallelism (EP): MoE Models\u2019 different expert\nmodels are distributed on different GPUs during MoE\ntraining [15]\u2013[17]. The gate model selects tokens for al\nlocation during input, with corresponding tokens sent to\nexperts model via all2all communication. \u2022 Fully Sharded Data Parallel (FSDP) is an implemen\ntation based on the ZeRO Stage 3 algorithm [19]. FSDP\npartitions the model\u2019s parameters, optimizer states, and\ngradients, distributing them across different GPUs, with\neach GPU retaining only 1/n of the total. During for\nward propagation, FSDP performs an allgather operation\nto assemble the complete parameters, which are then re\nleased after the forward pass is completed. Similarly, dur\ning backward propagation, FSDP conducts an allgather\noperation to obtain the complete parameters, followed\nby backward computation to calculate gradients. It then\nperforms a reduce-scatter operation to synchronize gra\ndients across all GPUs, resulting in each GPU holding\n1/n of the reduced gradients. Finally, FSDP updates the\n1/n parameters using each GPU\u2019s 1/n gradients and opti\nmizer states. F S D P   r e d u c e s   G P U   m e m o r y   u s a g e   b y   m a i n   t a i n i n g   o n l y   1 / n   o f   t h e   p a r a m e t e r s ,   g r a d i e n t s ,   a n d   o p t i \n m i z e r   s t a t e s   o n   e a c h   G P U ,   e n a b l i n g   t r a i n i n g   o f   l a r g e r \n s c a l e   m o d e l s .",
            "Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm op\nerations and MLA up-projections during back-propagation, thereby eliminating the need to\npersistently store their output activations. With a minor overhead, this strategy significantly\nreduces memory requirements for storing activations. Exponential Moving Average in CPU. During training, we preserve the Exponential Mov\ning Average (EMA) of the model parameters for early estimation of the model performance\nafter learning rate decay. The EMA parameters are stored in CPU memory and are updated\nasynchronously after each training step. This method allows us to maintain EMA parameters\nwithout incurring additional memory or time overhead. Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy,\nwe deploy the shallowest layers (including the embedding layer) and deepest layers (including\nthe output head) of the model on the same PP rank. This arrangement enables the physical\nsharing of parameters and gradients, of the shared embedding and output head, between the\nMTP module and the main model. This physical sharing mechanism further enhances our\nmemory efficiency. 3.3. FP8 Training\nInspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022;\nPeng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8\ndata format for training DeepSeek-V3. While low-precision training holds great promise, it\nis often limited by the presence of outliers in activations, weights, and gradients (Fishman\net al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in in\nference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies\ndemonstrating successful application of low-precision techniques in large-scale language model 14 \u03a3 Fprop FP32 Input To FP8 BF16 Weight \u03a3 Dgrad FP32 Input\nGradient Output Output\nGradient\nBF16 To FP8 \u03a3 Wgrad FP32 To FP8 To FP8 Weight\nGradient Optimizer\nStates To\nBF16 Master\nWeight To FP8 To BF16 To BF16 To FP32 \u6216\u8005Input->Activation_L\nOutput->Activation_{L+1} FP32 Figure 6 | The overall mixed precision framework with FP8 data format. To address this challenge and effectively extend the dynamic\nrange of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping\nwith 1 \u00d7 elements or block-wise grouping with \u00d7 elements. The associated dequantiza\ntion overhead is largely mitigated under our increased-precision accumulation process, a critical\naspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further\nreduce memory and communication overhead in MoE training, we cache and dispatch activa\ntions in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8\nmixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeek\nV2, training for approximately 1 trillion tokens (see more details in Appendix B.1).",
            "[19] S. Rajbhandari, J. He, \u201cZero: Memory op\ntimizations toward training trillion parameter models,\u201d in SC20: In\nternational Conference for High Performance Computing, Networking,\nStorage and Analysis. IEEE, 2020, pp. 1\u201316. I, II-B1, II-B1, V-B3\n[20] HFAiLab, \u201cHai platform: A high-performance deep learning training\nplatform with task-level gpu compute time-sharing scheduling,\u201d 2023. [Online]. Available: https://github.com/HFAiLab/hai-platform I\n[21] A. Krizhevsky, I. Sutskever, and G. Vaswani, N. Shazeer, N. Jones, A. N. Polosukhin, \u201cAttention is all you need,\u201d Ad\nvances in neural information processing systems, vol. 30, 2017. W. Nelson, A. Bridgland et al., \u201cImproved protein\nstructure prediction using potentials from deep learning,\u201d Nature, vol. 577, no. 7792, pp. 706\u2013710, 2020. II-A\n[25] D. Silver, T. Hubert, J. Schrittwieser, I. 6419, pp. 1140\u20131144, 2018. II-A\n[26] L. Floridi and M. Chiriatti, \u201cGpt-3: Its nature, scope, limits, and con\nsequences,\u201d Minds and Machines, vol. 30, pp. 681\u2013694, 2020. II-A\n[27] A. Gehrmann et al., \u201cPalm: Scal\ning language modeling with pathways,\u201d Journal of Machine Learning\nResearch, vol. 24, no. 240, pp. 1\u2013113, 2023. Hinton, \u201cAdaptive\nmixtures of local experts,\u201d Neural computation, vol. 3, no. 2, pp. 181\u2013214,"
        ],
        "Q&A Gen Profile": {
            "Type": "Definitional",
            "Persona": "The Novice",
            "Difficulty": "Easy"
        },
        "Question": "What are the primary techniques utilized to parallelize the training process for large language models, and how does each technique achieve parallelization?",
        "Answer": "The primary techniques for parallelizing the training of large language models include Data Parallelism (DP), where models and optimizer states are replicated across multiple devices with data evenly distributed. Zero Redundancy Optimizer (ZeRO) enhances DP by sharding states and using allgather/reduce-scatter. Tensor Parallelism (TP) places a model layer on multiple GPUs for parallel computations, using allgather/all2all. Expert Parallelism (EP) distributes different expert models on different GPUs, using all2all communication. Fully Sharded Data Parallel (FSDP) partitions model parameters, optimizer states, and gradients across GPUs, employing allgather and reduce-scatter operations."
    },
    {
        "context": [
            "enable parameter scaling, while concurrently keeping computational costs at a modest level. Recent applications of MoE architectures in Transformers (Vaswani et al., 2017) have yielded\nsuccessful attempts at scaling language models to a substantial size (Du et al., 2022; Fedus et al.,\n2021; Lepikhin et al., 2021; Zoph, 2022), accompanied with remarkable performance. These\nachievements underscore the considerable potential and promise of MoE language models. Despite the promising potential of MoE architectures, existing MoE architectures potentially\nsuffer from issues of knowledge hybridity and knowledge redundancy, which limit the expert\nspecialization, i.e., each expert acquires non-overlapping and focused knowledge. Conventional\nMoE architectures substitute the Feed-Forward Networks (FFNs) in a Transformer with MoE\nlayers. Each MoE layer consists of multiple experts, with each structurally identical to a standard\nFFN, and each token is assigned to one (Fedus et al., 2021) or two (Lepikhin et al., 2021) experts. This architecture manifests two potential issues: (1) Knowledge Hybridity: existing MoE\npractices often employ a limited number of experts (e.g., 8 or 16), and thus tokens assigned to a\nspecific expert will be likely to cover diverse knowledge. Consequently, the designated expert\nwill intend to assemble vastly different types of knowledge in its parameters, which are hard to\nutilize simultaneously. (2) Knowledge Redundancy: tokens assigned to different experts may\nrequire common knowledge. As a result, multiple experts may converge in acquiring shared\nknowledge in their respective parameters, thereby leading to redundancy in expert parameters. These issues collectively hinder the expert specialization in existing MoE practices, preventing\nthem from reaching the theoretical upper-bound performance of MoE models. In response to the aforementioned issues, we introduce DeepSeekMoE, an innovative MoE\narchitecture specifically designed towards ultimate expert specialization. O u r   a r c h i t e c t u r e \n i n v o l v e s   t w o   p r i n c i p a l   s t r a t e g i e s :   ( 1 )   F i n e - G r a i n e d   E x p e r t   S e g m e n t a t i o n :   w h i l e   m a i n t a i n i n g \n t h e   n u m b e r   o f   p a r a m e t e r s   c o n s t a n t ,   w e   s e g m e n t   t h e   e x p e r t s   i n t o   a   f i n e r   g r a i n   b y   s p l i t t i n g   t h e   2",
            "Including DeepSeekMoE, we compare five models for validation experiments. Dense denotes a standard dense Transformer language model with 0.2B total parameters. Hash\nLayer (Roller et al., 2021) is an MoE architecture based on top-1 hash routing, with 2.0B total\nparameters and 0.2B activated parameters, aligned with the dense baseline. Switch Trans\nformer (Fedus et al., 2021) is another well-known MoE architecture based on top-1 learnable\nrouting, with total parameters and activated parameters the same as Hash Layer. GShard (Lep\nikhin et al., 2021) employs a top-2 learnable routing strategy, with 2.0B total parameters and 0.3B\nactivated parameters since one more expert is activated compared to top-1 routing methods. DeepSeekMoE has 1 shared expert and 63 routed experts, where each expert is 0.25 times the\nsize of a standard FFN. Including DeepSeekMoE, all compared models share the same training\ncorpus and training hyper-parameters. All compared MoE models have the same number of\ntotal parameters, and GShard has the same number of activated parameters as DeepSeekMoE. For all demonstrated models, we re\nport the final evaluation results after training on 100B tokens. From the table, we make the\nfollowing observations: (1) With sparse architectures and more total parameters, Hash Layer 9 Metric # Shot Dense Hash Layer Switch GShard DeepSeekMoE # Total Params N/A 0.2B 2.0B 2.0B 2.0B 2.0B # Activated Params N/A 0.2B 0.2B 0.2B 0.3B 0.3B FLOPs per 2K Tokens N/A 2.9T 2.9T 2.9T 4.3T 4.3T # Training Tokens N/A 100B 100B 100B 100B 100B Pile (Loss) N/A 2.060 1.932 1.881 1.867 1.808 HellaSwag (Acc.) 0-shot 38.8 46.2 49.1 50.5 54.8 PIQA (Acc.) 0-shot 66.8 68.4 70.5 70.6 72.3 ARC-easy (Acc.) 0-shot 41.0 45.3 45.9 43.9 49.4 ARC-challenge (Acc.) 0-shot 26.0 28.2 30.2 31.6 34.3 RACE-middle (Acc.) 5-shot 38.8 38.8 43.6 42.1 44.0 RACE-high (Acc.) 5-shot 29.0 30.0 30.9 30.4 31.7 HumanEval (Pass@1) 0-shot 0.0 1.2 2.4 3.7 4.9 MBPP (Pass@1) 3-shot 0.2 0.6 0.4 0.2 2.2 TriviaQA (EM) 5-shot 4.9 6.5 8.9 10.2 16.6 NaturalQuestions (EM) 5-shot 1.4 1.4 2.5 3.2 5.7 Table 1 | Evaluation results for validation experiments. Bold font indicates the best. Compared\nwith other MoE architectures, DeepSeekMoE exhibits a substantial performance advantage. and Switch Transformer achieve significantly stronger performance than the dense baseline\nwith the same number of activated parameters. (2) Compared with Hash Layer and Switch\nTransformer, GShard has more activated parameters and achieves slightly better performance\nthan Switch Transformer. (3) With the same number of total parameters and activated pa\nrameters, DeepSeekMoE demonstrates overwhelming advantages over GShard. These results\nshowcase the superiority of our DeepSeekMoE architecture within the existing landscape of\nMoE architectures.",
            "A typical practice to construct an MoE language model usually substitutes FFNs in a Trans\nformer with MoE layers at specified intervals (Du et al., 2022; Fedus et al., 2021; Lepikhin\net al., 2021; Zoph, 2022). An MoE layer is composed of multiple experts, where each expert is\nstructurally identical to a standard FFN. Then, each token will be assigned to one (Fedus et al.,\n2021) or two (Lepikhin et al., 2021) experts. If the -th FFN is substituted with an MoE layer, the\ncomputation for its output hidden state h is expressed as: h = \u2211\ufe01 =1 , FFN u + u , (3) , = , , , \u2208 Topk({ ,\n|1 \u2a7d \u2a7d }, ), 0, otherwise, (4) , = Softmax u e , (5) where denotes the total number of experts, FFN (\u00b7) is the -th expert FFN, , denotes the gate value for the -th expert, ,\ndenotes the token-to-expert affinity, Topk(\u00b7, ) denotes the set\ncomprising highest affinity scores among those calculated for the -th token and all experts,\nand e is the centroid of the -th expert in the -th layer. Note that , is sparse, indicating that\nonly out of gate values are nonzero. This sparsity property ensures computational efficiency\nwithin an MoE layer, i.e., each token will be assigned to and computed in only experts. Also,\nin the above formulations, we omit the layer normalization operation for brevity. 4 1 2 \u2026 Router Input Hidden Output Hidden \u2026 Router Input Hidden Output Hidden 1 2 3 4 2 -1 2 \u2026 Router Input Hidden Output Hidden 1 2 3 4 2 -1 2 Shared Expert\nRouted Expert = 2 = 4 = 3 (a) Conventional Top-2 Routing (b) + Fine-grained Expert Segmentation"
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Curious Onlooker",
            "Difficulty": "Mild"
        },
        "Question": "What are the primary limitations that existing Mixture of Experts (MoE) architectures face regarding knowledge acquisition and how do these limitations potentially impact their overall performance?",
        "Answer": "Existing Mixture of Experts (MoE) architectures face limitations in knowledge acquisition due to knowledge hybridity and knowledge redundancy. Knowledge hybridity occurs when individual experts are forced to learn diverse knowledge, making it difficult to utilize that knowledge effectively. Knowledge redundancy happens when different experts learn the same common knowledge, leading to inefficient use of parameters. These issues hinder expert specialization, preventing MoE models from achieving their potential optimal performance."
    },
    {
        "context": [
            "enable parameter scaling, while concurrently keeping computational costs at a modest level. Recent applications of MoE architectures in Transformers (Vaswani et al., 2017) have yielded\nsuccessful attempts at scaling language models to a substantial size (Du et al., 2022; Fedus et al.,\n2021; Lepikhin et al., 2021; Zoph, 2022), accompanied with remarkable performance. These\nachievements underscore the considerable potential and promise of MoE language models. Despite the promising potential of MoE architectures, existing MoE architectures potentially\nsuffer from issues of knowledge hybridity and knowledge redundancy, which limit the expert\nspecialization, i.e., each expert acquires non-overlapping and focused knowledge. Conventional\nMoE architectures substitute the Feed-Forward Networks (FFNs) in a Transformer with MoE\nlayers. Each MoE layer consists of multiple experts, with each structurally identical to a standard\nFFN, and each token is assigned to one (Fedus et al., 2021) or two (Lepikhin et al., 2021) experts. This architecture manifests two potential issues: (1) Knowledge Hybridity: existing MoE\npractices often employ a limited number of experts (e.g., 8 or 16), and thus tokens assigned to a\nspecific expert will be likely to cover diverse knowledge. Consequently, the designated expert\nwill intend to assemble vastly different types of knowledge in its parameters, which are hard to\nutilize simultaneously. (2) Knowledge Redundancy: tokens assigned to different experts may\nrequire common knowledge. As a result, multiple experts may converge in acquiring shared\nknowledge in their respective parameters, thereby leading to redundancy in expert parameters. These issues collectively hinder the expert specialization in existing MoE practices, preventing\nthem from reaching the theoretical upper-bound performance of MoE models. In response to the aforementioned issues, we introduce DeepSeekMoE, an innovative MoE\narchitecture specifically designed towards ultimate expert specialization. O u r   a r c h i t e c t u r e \n i n v o l v e s   t w o   p r i n c i p a l   s t r a t e g i e s :   ( 1 )   F i n e - G r a i n e d   E x p e r t   S e g m e n t a t i o n :   w h i l e   m a i n t a i n i n g \n t h e   n u m b e r   o f   p a r a m e t e r s   c o n s t a n t ,   w e   s e g m e n t   t h e   e x p e r t s   i n t o   a   f i n e r   g r a i n   b y   s p l i t t i n g   t h e   2",
            "Including DeepSeekMoE, we compare five models for validation experiments. Dense denotes a standard dense Transformer language model with 0.2B total parameters. Hash\nLayer (Roller et al., 2021) is an MoE architecture based on top-1 hash routing, with 2.0B total\nparameters and 0.2B activated parameters, aligned with the dense baseline. Switch Trans\nformer (Fedus et al., 2021) is another well-known MoE architecture based on top-1 learnable\nrouting, with total parameters and activated parameters the same as Hash Layer. GShard (Lep\nikhin et al., 2021) employs a top-2 learnable routing strategy, with 2.0B total parameters and 0.3B\nactivated parameters since one more expert is activated compared to top-1 routing methods. DeepSeekMoE has 1 shared expert and 63 routed experts, where each expert is 0.25 times the\nsize of a standard FFN. Including DeepSeekMoE, all compared models share the same training\ncorpus and training hyper-parameters. All compared MoE models have the same number of\ntotal parameters, and GShard has the same number of activated parameters as DeepSeekMoE. For all demonstrated models, we re\nport the final evaluation results after training on 100B tokens. From the table, we make the\nfollowing observations: (1) With sparse architectures and more total parameters, Hash Layer 9 Metric # Shot Dense Hash Layer Switch GShard DeepSeekMoE # Total Params N/A 0.2B 2.0B 2.0B 2.0B 2.0B # Activated Params N/A 0.2B 0.2B 0.2B 0.3B 0.3B FLOPs per 2K Tokens N/A 2.9T 2.9T 2.9T 4.3T 4.3T # Training Tokens N/A 100B 100B 100B 100B 100B Pile (Loss) N/A 2.060 1.932 1.881 1.867 1.808 HellaSwag (Acc.) 0-shot 38.8 46.2 49.1 50.5 54.8 PIQA (Acc.) 0-shot 66.8 68.4 70.5 70.6 72.3 ARC-easy (Acc.) 0-shot 41.0 45.3 45.9 43.9 49.4 ARC-challenge (Acc.) 0-shot 26.0 28.2 30.2 31.6 34.3 RACE-middle (Acc.) 5-shot 38.8 38.8 43.6 42.1 44.0 RACE-high (Acc.) 5-shot 29.0 30.0 30.9 30.4 31.7 HumanEval (Pass@1) 0-shot 0.0 1.2 2.4 3.7 4.9 MBPP (Pass@1) 3-shot 0.2 0.6 0.4 0.2 2.2 TriviaQA (EM) 5-shot 4.9 6.5 8.9 10.2 16.6 NaturalQuestions (EM) 5-shot 1.4 1.4 2.5 3.2 5.7 Table 1 | Evaluation results for validation experiments. Bold font indicates the best. Compared\nwith other MoE architectures, DeepSeekMoE exhibits a substantial performance advantage. and Switch Transformer achieve significantly stronger performance than the dense baseline\nwith the same number of activated parameters. (2) Compared with Hash Layer and Switch\nTransformer, GShard has more activated parameters and achieves slightly better performance\nthan Switch Transformer. (3) With the same number of total parameters and activated pa\nrameters, DeepSeekMoE demonstrates overwhelming advantages over GShard. These results\nshowcase the superiority of our DeepSeekMoE architecture within the existing landscape of\nMoE architectures.",
            "A typical practice to construct an MoE language model usually substitutes FFNs in a Trans\nformer with MoE layers at specified intervals (Du et al., 2022; Fedus et al., 2021; Lepikhin\net al., 2021; Zoph, 2022). An MoE layer is composed of multiple experts, where each expert is\nstructurally identical to a standard FFN. Then, each token will be assigned to one (Fedus et al.,\n2021) or two (Lepikhin et al., 2021) experts. If the -th FFN is substituted with an MoE layer, the\ncomputation for its output hidden state h is expressed as: h = \u2211\ufe01 =1 , FFN u + u , (3) , = , , , \u2208 Topk({ ,\n|1 \u2a7d \u2a7d }, ), 0, otherwise, (4) , = Softmax u e , (5) where denotes the total number of experts, FFN (\u00b7) is the -th expert FFN, , denotes the gate value for the -th expert, ,\ndenotes the token-to-expert affinity, Topk(\u00b7, ) denotes the set\ncomprising highest affinity scores among those calculated for the -th token and all experts,\nand e is the centroid of the -th expert in the -th layer. Note that , is sparse, indicating that\nonly out of gate values are nonzero. This sparsity property ensures computational efficiency\nwithin an MoE layer, i.e., each token will be assigned to and computed in only experts. Also,\nin the above formulations, we omit the layer normalization operation for brevity. 4 1 2 \u2026 Router Input Hidden Output Hidden \u2026 Router Input Hidden Output Hidden 1 2 3 4 2 -1 2 \u2026 Router Input Hidden Output Hidden 1 2 3 4 2 -1 2 Shared Expert\nRouted Expert = 2 = 4 = 3 (a) Conventional Top-2 Routing (b) + Fine-grained Expert Segmentation"
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "What are the primary limitations of conventional Mixture of Experts (MoE) architectures regarding expert specialization, and how does the DeepSeekMoE architecture aim to mitigate these limitations?",
        "Answer": "Conventional MoE architectures suffer from knowledge hybridity, where experts are likely to cover diverse knowledge, making it difficult to utilize simultaneously, and knowledge redundancy, where multiple experts acquire shared knowledge. DeepSeekMoE addresses these limitations by employing fine-grained expert segmentation and creating an architecture of 1 shared expert and 63 routed experts where each expert is 0.25 times the size of a standard FFN, promoting greater expert specialization."
    },
    {
        "context": [
            "end\n12 end\n13 //\n14 // Pass 2: gather reduced\ndata,individual thread\n15 receive data DR i from next node\n16 for j in GPU Count do\n17 Dg i = MemCopyAsync DR i to GPU-j // Dg_i\nis allreduced 18 end\n19 send DL i to prev node D. Deep Analysis of HFReduce\n1) Key Technical Strategies in Implementation:\n\u2022 Using GDRCopy accelerate small data transfer in D2H,\nand educing reads from host memory by three times com\npared to MemCpyAsyn. \u2022 Intra-Node Reduction: CPU utilizes SIMD instructions\nand supports FP32 / FP16 / BF16 / FP8 datatypes. \u2022 NUMA Awareness: D2H destination memory is inter\nleaved across two NUMA nodes for maximum band\nwidth. Memory for CPU-added results and network received data is bound to the IB-Nic\u2019s NUMA node to\nminimize latency. \u2022 Inter-Node Reduce:Implements a Double Binary Tree\nallreduce algorithm [65] via ibverbs RDMA Write, avoid\ning additional overhead. 2) HFReduce Overcomes Limitations of EPYC Rome CPU:\nWe consulted AMD and NVIDIA engineers to identify the\nroot cause of NCCL\u2019s suboptimal performance on PCIe ar\nchitecture, particularly with EPYC Rome CPU servers. It was\ndetermined that the Rome CPUs do not support the chained\nwrite feature, which can significantly accelerate PCIe peer-to\npeer (P2P) transfers between GPUs and IB NICs. Our tests\nindicate that the maximum bandwidth between the GPU and\nIB NIC on Rome CPUs is approximately 9 GiB/s, making the\nobserved 4GB/s all-reduce bandwidth for NCCL understand\nable. HFReduce circumvents this limitation by utilizing the\nCPU for reduction and transferring data through IB and host\nmemory. 3) Bottlenecks of HFReduce: When considering the total\nmemory operations on a single node during HFReduce, several\nfactors contribute to its performance limitations:\n1) D2H Phase requires 8 write operations. 2) Intra-node Reduce Add Phase involves 8 read operations\nand 1 write operation. 3) Inter-node Allreduce Phase: IB send demands 2 read\noperations, while IB receive requires 2 write operations,\nalong with 1 read operation for reduce add. 4) H2D Phase Utilizing GDRCopy can reduce this to only\n2 read operations, whereas MemCopy necessitates 8 read\noperations. In total, the memory operations amount to 24 times the orig\ninal data size in the GPU. A host equipped with 16 channels\nof DDR4-3200MHz memory can achieve a practical mem\nory access speed of 320GB/s. Consequently, the theoretical\nmaximum speed of HFReduce is approximately 13.3GB/s, but\nwhen considering the allreduce algorithm bandwidth and net\nwork bandwidth, this value realistically approximates 12GB/s. However, our tests only achieved slightly over 8GB/s. The root cause of this discrepancy is another limitation of\nthe EPYC CPUs. As previously mentioned, our GPU5 and 0.13 0.133 0.139 0.149 0.15 0.27 0.271 0.279 0.287 0.288 P E T S / S D N O C E S NUMBER OF GPUS HFReduce Torch DDP(NCCL) (a) HFReduce v.s. Torch DDP. 0 . 5 7   0 . 5 8   0 . 5 9 8   0 . 5 9 5   0 . 8 4   0 . 9 8 5   0 . 8 7 5   0 . 9 9   P   E   T   S   /   S   D   N   O   C   E   S   N U M B E R   O F   G P U S   H A I S C A L E   T O R C H   F S D P",
            "unit of PCIe bidirectional bandwidth. In con\ntrast, for each unit of data, HFReduce only requires one D2H\nand one H2D data transfer, only one unit of PCIe bidirec\ntional bandwidth is consumed. In our machine architecture,\nthe performance of NCCL is mainly limited by PCIe band\nwidth. Therefore, HFReduce can achieve better performance\nthan NCCL. 2) No GPU Kernel Overhead: HFReduce utilizes the\nGPU\u2019s Copy Engine (CE) for PCIe asynchronous transfers. In contrast, NCCL\u2019s allreduce operation requires GPU kernel\nexecution, which can affect other computational kernels on\nthe GPU. HFReduce achieves complete asynchrony with no\noverhead. As demonstrated in Figure 7a, HFReduce can reach a inter\nnode bandwidths of 6.3-8.1GB/s when performing allreduce\nwith a data size of 186 MiB on the Fire-Flyer 2 AI-HPC,\nwhile NCCL\u2019s inter-node bandwidth is only 1.6-4.8GB/s. C. Performance Improvements: HFReduce with NVLink\nBy installing the NVLink Bridge for PCIe A100 GPUs, effi\ncient communication is enabled between paired GPUs via the\n600 GB/s NVLink. To alleviate the memory bound issue of the\noriginal HFReduce, we implemented another allreduce pattern,\ntermed HFReduce with NVLink. The core concept involves\ninitially performing a reduction operation among GPUs in\nterconnected by NVLink before passing the gradient to the\nCPU. Subsequently, when the CPU returns the result, it splits\nthe result data and returns them to the paired GPUs connected\nby NVLink respectively, then performs allgather via NVLink. As illustrated in Figure 7b, HFReduce with NVLink achieves\ninter-node bandwidths exceeding 10 GB/s. 8.10 8.03 7.95 7.78 7.12 7.33 6.31 6.99 4.01 4.41 4.29 3.73 4.83 4.62 1.63 1.65 S / B G H T D I W D N A B NUMBER OF GPUS HFREDUCE NCCL (a) HFRudce and NCCL Allreduce Speed. 19.18 13 12.07 10.3 9.66 8.56 7.99 9.27 8.63 8.06 7.75 S / B G H T D I W D N A B NUMBER OF GPUS HFReduce with nvlink HFReduce with nvlink Cross Fat-Tree Zone (b) HFReduce with NVLink (Cross Fat-Tree Zone). Figure 7: Strong Scalability: (a) Network Bandwidth when HFReduce and NCCL do allreduce test with 186MiB data, scale\nfrom 16 to 1440 GPUs. (b) HFReduce with NVLink, and Cross Fat-Tree Zone. Note that tasks utilizing fewer than 128 GPUs\ndo not require cross-zone nodes and are restricted by platform defaults. Algorithm 2: Inter Node Reduce. Data: DL:local node reduced data by Algorithm 1\nData: DR: received other node reduced data\nResult: Dg: reduced data transfer to GPU\n1 // Pass 1: reduce data,individual\nthread\n2 for i in Chunk Size do 3 receive data DR i from prev node 4 if DL i ready then 5 DL i +=DR i // reduce data\n6 end\n7 if Thread is root of Tree then 8 send DL i to prev node // Dg_i\nfinished, go parse 2 9 else 10 send DL i to next node 11",
            "Figure 6: HFReduce Schematic: 1) do intra-node reduce, 2) do\ninter-node allreduce by CPU, 3) finally transfer reduced data\nto GPU. GPUs within each node. This inter-node allreduce leverages\na Double Binary Tree Algorithm [65], akin to NCCL, and is\npipelined by dividing data into chunks for transfer via Remote\nDirect Memory Access (RDMA), ensuring high performance. HFReduce is versatile and can be applied to any scenario re\nquiring allreduce, as well as general reduce and broadcast op\nerations. A. HFReduce Algorithm Steps\nIntra-node reduction, as shown in the Algorithm 1:\n1) When the gradients data on the GPUs require allre\nduce, HFReduce asynchronously transfers these data to\nthe CPU memory. This Device-To-Host (D2H) Trans\nfer can utilize GDRCopy [66] for small data and Mem\nCpyAsync for larger data. 2) Upon the arrival of the gradients in memory, perform\nreduction add operation using CPU vector instructions. Inter-node reduction, as shown in the Algorithm 2:\n1) Use the Double Binary Tree Algorithm [65] for inter\nnode allreduce, facilitating transfers between nodes us\ning RDMA verbs implementation. 2) Finally, the CPU returns reduced gradients to the GPU\nvia PCIe (Host-To-Device Phase). The final Host-To-Device (H2D) Transfer can be optimized by\nutilizing GDRCopy to write data to four GPUs within the same\nNUMA node, effectively reducing reads from host memory\nby threefold compared to MemCpyAsync. This efficiency is\nachieved because GDRCopy can read data from host memory\nand temporarily cache it in the CPU caches, allowing data to\nbe written to the four GPUs without additional reads from host\nmemory. B. Advantages of HFReduce over NCCL\n1) Reduced PCIe Bandwidth Consumption: Let n be the\ntotal number of GPUs involved in the communication. In\nNCCL\u2019s ring topology, each unit of data needs to go through\n2n \u2212 1 transmissions, each consuming one unit of inbound Algorithm 1: Intra Node Reduce. Data: Dg: data need to allreduce\nResult: Dc: data reduced in this node\n1 Split Dg by Chunk Size\n2 for Dg-i in Splited-Dg do 3 // Transfer GPU Memory Data to CPU\nmemory 4 Dc i = MemCopyAsync Dg i to CPU Mmeory\n5 end\n6 for i in Splited Count do 7 while every GPU\u2019s Dg i \u2192 Dc i finished do 8 // Wait for chunk-i transfer\nfinished in this node 9 for j in GPU Count do 10 // do intra-node reduce 11 Dc i += GPU-j\u2019s Dc i 12 end 13 // do inter-node reduce 14 Do Internode reduce(Dc i)\n15 end\n16 end bandwidth of one GPU and one unit of outbound bandwidth\nof another GPU. T h i s   m e a n s   f o r   a   s i n g l e   u n i t   o f   d a t a ,   i t   c o n   s u m e s \n 2 n \u2212 1   n"
        ],
        "Q&A Gen Profile": {
            "Type": "Decision Support",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "In a high-performance computing environment utilizing EPYC Rome CPUs and PCIe architecture, which allreduce method, HFReduce or NCCL, would be more advantageous for maximizing inter-node bandwidth, considering factors such as PCIe bandwidth limitations, GPU kernel overhead, and data transfer mechanisms?",
        "Answer": "HFReduce would be more advantageous due to its ability to circumvent the PCIe chained write limitation of EPYC Rome CPUs by utilizing the CPU for reduction and data transfer via IB and host memory. HFReduce also benefits from using the GPU's Copy Engine for asynchronous transfers, avoiding GPU kernel overhead associated with NCCL. Empirical evidence suggests HFReduce achieves higher inter-node bandwidths (6.3-8.1GB/s) compared to NCCL (1.6-4.8GB/s) in such environments."
    },
    {
        "context": [
            "end\n12 end\n13 //\n14 // Pass 2: gather reduced\ndata,individual thread\n15 receive data DR i from next node\n16 for j in GPU Count do\n17 Dg i = MemCopyAsync DR i to GPU-j // Dg_i\nis allreduced 18 end\n19 send DL i to prev node D. Deep Analysis of HFReduce\n1) Key Technical Strategies in Implementation:\n\u2022 Using GDRCopy accelerate small data transfer in D2H,\nand educing reads from host memory by three times com\npared to MemCpyAsyn. \u2022 Intra-Node Reduction: CPU utilizes SIMD instructions\nand supports FP32 / FP16 / BF16 / FP8 datatypes. \u2022 NUMA Awareness: D2H destination memory is inter\nleaved across two NUMA nodes for maximum band\nwidth. Memory for CPU-added results and network received data is bound to the IB-Nic\u2019s NUMA node to\nminimize latency. \u2022 Inter-Node Reduce:Implements a Double Binary Tree\nallreduce algorithm [65] via ibverbs RDMA Write, avoid\ning additional overhead. 2) HFReduce Overcomes Limitations of EPYC Rome CPU:\nWe consulted AMD and NVIDIA engineers to identify the\nroot cause of NCCL\u2019s suboptimal performance on PCIe ar\nchitecture, particularly with EPYC Rome CPU servers. It was\ndetermined that the Rome CPUs do not support the chained\nwrite feature, which can significantly accelerate PCIe peer-to\npeer (P2P) transfers between GPUs and IB NICs. Our tests\nindicate that the maximum bandwidth between the GPU and\nIB NIC on Rome CPUs is approximately 9 GiB/s, making the\nobserved 4GB/s all-reduce bandwidth for NCCL understand\nable. HFReduce circumvents this limitation by utilizing the\nCPU for reduction and transferring data through IB and host\nmemory. 3) Bottlenecks of HFReduce: When considering the total\nmemory operations on a single node during HFReduce, several\nfactors contribute to its performance limitations:\n1) D2H Phase requires 8 write operations. 2) Intra-node Reduce Add Phase involves 8 read operations\nand 1 write operation. 3) Inter-node Allreduce Phase: IB send demands 2 read\noperations, while IB receive requires 2 write operations,\nalong with 1 read operation for reduce add. 4) H2D Phase Utilizing GDRCopy can reduce this to only\n2 read operations, whereas MemCopy necessitates 8 read\noperations. In total, the memory operations amount to 24 times the orig\ninal data size in the GPU. A host equipped with 16 channels\nof DDR4-3200MHz memory can achieve a practical mem\nory access speed of 320GB/s. Consequently, the theoretical\nmaximum speed of HFReduce is approximately 13.3GB/s, but\nwhen considering the allreduce algorithm bandwidth and net\nwork bandwidth, this value realistically approximates 12GB/s. However, our tests only achieved slightly over 8GB/s. The root cause of this discrepancy is another limitation of\nthe EPYC CPUs. As previously mentioned, our GPU5 and 0.13 0.133 0.139 0.149 0.15 0.27 0.271 0.279 0.287 0.288 P E T S / S D N O C E S NUMBER OF GPUS HFReduce Torch DDP(NCCL) (a) HFReduce v.s. Torch DDP. 0 . 5 7   0 . 5 8   0 . 5 9 8   0 . 5 9 5   0 . 8 4   0 . 9 8 5   0 . 8 7 5   0 . 9 9   P   E   T   S   /   S   D   N   O   C   E   S   N U M B E R   O F   G P U S   H A I S C A L E   T O R C H   F S D P",
            "unit of PCIe bidirectional bandwidth. In con\ntrast, for each unit of data, HFReduce only requires one D2H\nand one H2D data transfer, only one unit of PCIe bidirec\ntional bandwidth is consumed. In our machine architecture,\nthe performance of NCCL is mainly limited by PCIe band\nwidth. Therefore, HFReduce can achieve better performance\nthan NCCL. 2) No GPU Kernel Overhead: HFReduce utilizes the\nGPU\u2019s Copy Engine (CE) for PCIe asynchronous transfers. In contrast, NCCL\u2019s allreduce operation requires GPU kernel\nexecution, which can affect other computational kernels on\nthe GPU. HFReduce achieves complete asynchrony with no\noverhead. As demonstrated in Figure 7a, HFReduce can reach a inter\nnode bandwidths of 6.3-8.1GB/s when performing allreduce\nwith a data size of 186 MiB on the Fire-Flyer 2 AI-HPC,\nwhile NCCL\u2019s inter-node bandwidth is only 1.6-4.8GB/s. C. Performance Improvements: HFReduce with NVLink\nBy installing the NVLink Bridge for PCIe A100 GPUs, effi\ncient communication is enabled between paired GPUs via the\n600 GB/s NVLink. To alleviate the memory bound issue of the\noriginal HFReduce, we implemented another allreduce pattern,\ntermed HFReduce with NVLink. The core concept involves\ninitially performing a reduction operation among GPUs in\nterconnected by NVLink before passing the gradient to the\nCPU. Subsequently, when the CPU returns the result, it splits\nthe result data and returns them to the paired GPUs connected\nby NVLink respectively, then performs allgather via NVLink. As illustrated in Figure 7b, HFReduce with NVLink achieves\ninter-node bandwidths exceeding 10 GB/s. 8.10 8.03 7.95 7.78 7.12 7.33 6.31 6.99 4.01 4.41 4.29 3.73 4.83 4.62 1.63 1.65 S / B G H T D I W D N A B NUMBER OF GPUS HFREDUCE NCCL (a) HFRudce and NCCL Allreduce Speed. 19.18 13 12.07 10.3 9.66 8.56 7.99 9.27 8.63 8.06 7.75 S / B G H T D I W D N A B NUMBER OF GPUS HFReduce with nvlink HFReduce with nvlink Cross Fat-Tree Zone (b) HFReduce with NVLink (Cross Fat-Tree Zone). Figure 7: Strong Scalability: (a) Network Bandwidth when HFReduce and NCCL do allreduce test with 186MiB data, scale\nfrom 16 to 1440 GPUs. (b) HFReduce with NVLink, and Cross Fat-Tree Zone. Note that tasks utilizing fewer than 128 GPUs\ndo not require cross-zone nodes and are restricted by platform defaults. Algorithm 2: Inter Node Reduce. Data: DL:local node reduced data by Algorithm 1\nData: DR: received other node reduced data\nResult: Dg: reduced data transfer to GPU\n1 // Pass 1: reduce data,individual\nthread\n2 for i in Chunk Size do 3 receive data DR i from prev node 4 if DL i ready then 5 DL i +=DR i // reduce data\n6 end\n7 if Thread is root of Tree then 8 send DL i to prev node // Dg_i\nfinished, go parse 2 9 else 10 send DL i to next node 11",
            "Figure 6: HFReduce Schematic: 1) do intra-node reduce, 2) do\ninter-node allreduce by CPU, 3) finally transfer reduced data\nto GPU. GPUs within each node. This inter-node allreduce leverages\na Double Binary Tree Algorithm [65], akin to NCCL, and is\npipelined by dividing data into chunks for transfer via Remote\nDirect Memory Access (RDMA), ensuring high performance. HFReduce is versatile and can be applied to any scenario re\nquiring allreduce, as well as general reduce and broadcast op\nerations. A. HFReduce Algorithm Steps\nIntra-node reduction, as shown in the Algorithm 1:\n1) When the gradients data on the GPUs require allre\nduce, HFReduce asynchronously transfers these data to\nthe CPU memory. This Device-To-Host (D2H) Trans\nfer can utilize GDRCopy [66] for small data and Mem\nCpyAsync for larger data. 2) Upon the arrival of the gradients in memory, perform\nreduction add operation using CPU vector instructions. Inter-node reduction, as shown in the Algorithm 2:\n1) Use the Double Binary Tree Algorithm [65] for inter\nnode allreduce, facilitating transfers between nodes us\ning RDMA verbs implementation. 2) Finally, the CPU returns reduced gradients to the GPU\nvia PCIe (Host-To-Device Phase). The final Host-To-Device (H2D) Transfer can be optimized by\nutilizing GDRCopy to write data to four GPUs within the same\nNUMA node, effectively reducing reads from host memory\nby threefold compared to MemCpyAsync. This efficiency is\nachieved because GDRCopy can read data from host memory\nand temporarily cache it in the CPU caches, allowing data to\nbe written to the four GPUs without additional reads from host\nmemory. B. Advantages of HFReduce over NCCL\n1) Reduced PCIe Bandwidth Consumption: Let n be the\ntotal number of GPUs involved in the communication. In\nNCCL\u2019s ring topology, each unit of data needs to go through\n2n \u2212 1 transmissions, each consuming one unit of inbound Algorithm 1: Intra Node Reduce. Data: Dg: data need to allreduce\nResult: Dc: data reduced in this node\n1 Split Dg by Chunk Size\n2 for Dg-i in Splited-Dg do 3 // Transfer GPU Memory Data to CPU\nmemory 4 Dc i = MemCopyAsync Dg i to CPU Mmeory\n5 end\n6 for i in Splited Count do 7 while every GPU\u2019s Dg i \u2192 Dc i finished do 8 // Wait for chunk-i transfer\nfinished in this node 9 for j in GPU Count do 10 // do intra-node reduce 11 Dc i += GPU-j\u2019s Dc i 12 end 13 // do inter-node reduce 14 Do Internode reduce(Dc i)\n15 end\n16 end bandwidth of one GPU and one unit of outbound bandwidth\nof another GPU. T h i s   m e a n s   f o r   a   s i n g l e   u n i t   o f   d a t a ,   i t   c o n   s u m e s \n 2 n \u2212 1   n"
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "In the context of distributed training, what is the rationale behind using the CPU for reduction in HFReduce, considering its memory operation overhead, and how does this design choice address the limitations of PCIe architectures, specifically concerning chained writes and bandwidth constraints between GPUs and IB NICs, that ultimately leads to a better allreduce performance than NCCL in certain scenarios?",
        "Answer": "HFReduce uses the CPU for reduction to circumvent the PCIe bandwidth limitations, particularly the lack of chained write support on CPUs like EPYC Rome which limits direct GPU-to-IB NIC transfer speeds and consequently hinders NCCL's performance. While HFReduce involves CPU memory operation overhead, it reduces PCIe bandwidth consumption compared to NCCL's ring topology by only requiring one D2H and one H2D data transfer per data unit and it avoids GPU kernel overhead due to its asynchronous transfers using the GPU Copy Engine."
    },
    {
        "context": [
            "1) NVLink Bridge Enables Tensor Parallel between PCIe\nGPUs: With the advent of LLMs, we integrated the NVLink\nBridge into our system. This addition established a bandwidth\nof 600GB/s between each pair of GPUs, enabling more effi\ncient when performing Tensor Parallelism. HaiScale\u2019s FSDP offers better engineering implementation,\noptimizing memory management to reduce fragmentation spe\ncific to model adjustments. And we overlap allgather and\nreduce-scatter communication with forward and backward\ncomputation, split the optimization step during backward prop\nagation for enhanced overlap. ADVANCED COST-EFFECTIVE AND CO-DESIGN\nOPTIMIZATIONS A. Ensuring Minimal Congestion in Our Computation-Storage\nIntegrated Network\nAs previously stated, our cost-effective network integrated\ncomputation communication and storage traffics together. To\nachieve maximum bandwidth, it is essential to isolate inter\nference between different types of traffic and control network\ncongestion. In practice, we implemented the following mea sures:",
            "1) Divergence of Different Traffics: In typical training\ntasks, there are four different types of traffic: HFReduce com\nmunication, NCCL communication, 3FS storage traffic, and\nother traffic. By using InfiniBand\u2019s Service Level (SL) tech\nnology [74] [75], we assign different value of SL when estab\nlishing connections between nodes and map SL to IB physical\nqueues Virtual Lanes (VLs) [74] [75]. The use of Virtual Lanes\nensures that flows in distinct lanes do not interfere with each\nother. Ultimately, we configured their proportions to imple\nment traffic isolation, thereby preventing network congestion\ncaused by Head-of-line (HOL) blocking [76] and different traf\nfic collisions. 2) Topology Adjustment and Route Optimization: In high\nthroughput storage scenarios, there naturally exist many incast\ncommunication patterns, leading to certain congestion in the\nnetwork. Under such circumstances, we observed that enabling\nadaptive routing would lead to more severe congestion spread\nin the network. Therefore, we opted for a static routing strat\negy. Based on the static routing scheme, to evenly disperse\nstorage traffic into leaf \u2192 spine links, we distribute various\nnodes (storage, computation, management nodes) evenly dis\nperse storage traffic into leaf \u2192 spine links. 3) NCCL Optimization: We adjusted the NCCL topology to\nroute through the IB NIC and GPUs within the same NUMA\nnode. This adjustment reduced PCIe congestion caused by\nCPU chiplet interconnects. Additionally, by using PCIe Re\nlaxed Ordering [77], we further reduced congestion and in\ncreased bandwidth. 4) Network Tuning in 3FS: 3FS implements a request-to\nsend control mechanism to mitigate the congestion. Details\nare discussed in the next subsection, Key Techinical Points of\n3FS. B. High-Throughput Distributed File System: 3FS\n1) Overview: 3FS is our in-house developed high perfor\nmance distributed file system, akin to WekaFS [78], DAOS\n[79], [80], and BeeGFS [81]. However, the design and im\nplementation of 3FS specifically focus on fully utilizing the\nhigh IOPS and throughput of NVMe SSDs and the RDMA\nnetwork. T a b l e   I V :   S t o r a g e   N o d e   H a r d w a r e   D e t a i l s   C P U   1   *   A M D   6 4   C o r e s   E P Y C   7 7 4 2   C P U   M e m o r y   5 1 2 G B   8 - C h a n n e l s   D D R 4 - 3 2 0 0 M h z   N I C s   2   *   M e l l a n o x   I n f i n i B a n d   C X 6   2 0 0 G b p s   N I C   D a t a   S S D s   1 6   *   1 5 . 3 6 T B   P C I e   4 . 0 x 4",
            "Figure 11: Trends of IB Network Failures: Link Flash Cuts flash cuts can lead to application communication disruption,\neven task failures. Since most tasks run on multiple nodes,\nan issue on a single node can impact many others, further\nreducing cluster utilization. Figure 11 illustrates the IB link\nfailures data over the past year, with raw data attached in\nAppendix: Supplementary Characterization, Table VIII, indi\ncating that these issues can occur randomly throughout the\ncluster\u2019s operational period. VIII. DISCUSSION\nA. Discussion on Congestion Control in RDMA Networks\nLossless RDMA networks offer several flow-control mech\nanisms, such as Priority Flow Control (PFC) [89] for RoCE\nnetworks and credit-based flow control [90] for IB networks. In network routing, static routing algorithms in IB or ECMP\n(Equal-Cost Multi-Path) [91] and AR (Adaptive Routing) [92]\neffectively handle routing issues. However, congestion can still\noccur when multiple servers send data to a single receiver, po\ntentially blocking the entire network. To mitigate this, IB NICs\nuse DCQCN (Data Center Quantized Congestion Notification)\n[93] as their congestion control algorithm. While Data Pro\ncessing Units (DPUs), such as the NVIDIA BF series, allow\nusers to customize congestion control algorithms (like HPCC\n[94] and TIMELY RTT-based CC [95]), they increase the cost\nand operational complexity of the cluster. In practice, we chose to disable DCQCN to avoid its\nshortcomings, as it could not find parameters that simul- taneously support HFReduce traffic and 3FS storage traffic\nin our Computation-Storage Integrated Network. Instead, we\nemployed the network tuning methods mentioned in Section\nVI-A, ensuring our network operates without congestion con\ntrol and remains congestion-free."
        ],
        "Q&A Gen Profile": {
            "Type": "Definitional",
            "Persona": "The Curious Onlooker",
            "Difficulty": "Mild"
        },
        "Question": "In a system integrating computation and storage, what strategies were employed to minimize network congestion and ensure efficient data flow between different types of traffic?",
        "Answer": "To minimize network congestion in a computation-storage integrated network, several strategies were implemented. These include isolating different types of traffic (HFReduce, NCCL, 3FS storage, and other traffic) using InfiniBand's Service Level (SL) technology, adjusting network topology to distribute storage traffic evenly using static routing, optimizing NCCL topology and using PCIe Relaxed Ordering to reduce PCIe congestion, and implementing a request-to-send control mechanism in 3FS. Furthermore, DCQCN was disabled due to its inability to simultaneously support HFReduce and 3FS traffic; instead, network tuning methods were employed to maintain a congestion-free network."
    },
    {
        "context": [
            "1) NVLink Bridge Enables Tensor Parallel between PCIe\nGPUs: With the advent of LLMs, we integrated the NVLink\nBridge into our system. This addition established a bandwidth\nof 600GB/s between each pair of GPUs, enabling more effi\ncient when performing Tensor Parallelism. HaiScale\u2019s FSDP offers better engineering implementation,\noptimizing memory management to reduce fragmentation spe\ncific to model adjustments. And we overlap allgather and\nreduce-scatter communication with forward and backward\ncomputation, split the optimization step during backward prop\nagation for enhanced overlap. ADVANCED COST-EFFECTIVE AND CO-DESIGN\nOPTIMIZATIONS A. Ensuring Minimal Congestion in Our Computation-Storage\nIntegrated Network\nAs previously stated, our cost-effective network integrated\ncomputation communication and storage traffics together. To\nachieve maximum bandwidth, it is essential to isolate inter\nference between different types of traffic and control network\ncongestion. In practice, we implemented the following mea sures:",
            "1) Divergence of Different Traffics: In typical training\ntasks, there are four different types of traffic: HFReduce com\nmunication, NCCL communication, 3FS storage traffic, and\nother traffic. By using InfiniBand\u2019s Service Level (SL) tech\nnology [74] [75], we assign different value of SL when estab\nlishing connections between nodes and map SL to IB physical\nqueues Virtual Lanes (VLs) [74] [75]. The use of Virtual Lanes\nensures that flows in distinct lanes do not interfere with each\nother. Ultimately, we configured their proportions to imple\nment traffic isolation, thereby preventing network congestion\ncaused by Head-of-line (HOL) blocking [76] and different traf\nfic collisions. 2) Topology Adjustment and Route Optimization: In high\nthroughput storage scenarios, there naturally exist many incast\ncommunication patterns, leading to certain congestion in the\nnetwork. Under such circumstances, we observed that enabling\nadaptive routing would lead to more severe congestion spread\nin the network. Therefore, we opted for a static routing strat\negy. Based on the static routing scheme, to evenly disperse\nstorage traffic into leaf \u2192 spine links, we distribute various\nnodes (storage, computation, management nodes) evenly dis\nperse storage traffic into leaf \u2192 spine links. 3) NCCL Optimization: We adjusted the NCCL topology to\nroute through the IB NIC and GPUs within the same NUMA\nnode. This adjustment reduced PCIe congestion caused by\nCPU chiplet interconnects. Additionally, by using PCIe Re\nlaxed Ordering [77], we further reduced congestion and in\ncreased bandwidth. 4) Network Tuning in 3FS: 3FS implements a request-to\nsend control mechanism to mitigate the congestion. Details\nare discussed in the next subsection, Key Techinical Points of\n3FS. B. High-Throughput Distributed File System: 3FS\n1) Overview: 3FS is our in-house developed high perfor\nmance distributed file system, akin to WekaFS [78], DAOS\n[79], [80], and BeeGFS [81]. However, the design and im\nplementation of 3FS specifically focus on fully utilizing the\nhigh IOPS and throughput of NVMe SSDs and the RDMA\nnetwork. T a b l e   I V :   S t o r a g e   N o d e   H a r d w a r e   D e t a i l s   C P U   1   *   A M D   6 4   C o r e s   E P Y C   7 7 4 2   C P U   M e m o r y   5 1 2 G B   8 - C h a n n e l s   D D R 4 - 3 2 0 0 M h z   N I C s   2   *   M e l l a n o x   I n f i n i B a n d   C X 6   2 0 0 G b p s   N I C   D a t a   S S D s   1 6   *   1 5 . 3 6 T B   P C I e   4 . 0 x 4",
            "Figure 11: Trends of IB Network Failures: Link Flash Cuts flash cuts can lead to application communication disruption,\neven task failures. Since most tasks run on multiple nodes,\nan issue on a single node can impact many others, further\nreducing cluster utilization. Figure 11 illustrates the IB link\nfailures data over the past year, with raw data attached in\nAppendix: Supplementary Characterization, Table VIII, indi\ncating that these issues can occur randomly throughout the\ncluster\u2019s operational period. VIII. DISCUSSION\nA. Discussion on Congestion Control in RDMA Networks\nLossless RDMA networks offer several flow-control mech\nanisms, such as Priority Flow Control (PFC) [89] for RoCE\nnetworks and credit-based flow control [90] for IB networks. In network routing, static routing algorithms in IB or ECMP\n(Equal-Cost Multi-Path) [91] and AR (Adaptive Routing) [92]\neffectively handle routing issues. However, congestion can still\noccur when multiple servers send data to a single receiver, po\ntentially blocking the entire network. To mitigate this, IB NICs\nuse DCQCN (Data Center Quantized Congestion Notification)\n[93] as their congestion control algorithm. While Data Pro\ncessing Units (DPUs), such as the NVIDIA BF series, allow\nusers to customize congestion control algorithms (like HPCC\n[94] and TIMELY RTT-based CC [95]), they increase the cost\nand operational complexity of the cluster. In practice, we chose to disable DCQCN to avoid its\nshortcomings, as it could not find parameters that simul- taneously support HFReduce traffic and 3FS storage traffic\nin our Computation-Storage Integrated Network. Instead, we\nemployed the network tuning methods mentioned in Section\nVI-A, ensuring our network operates without congestion con\ntrol and remains congestion-free."
        ],
        "Q&A Gen Profile": {
            "Type": "Decision Support",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "In a high-performance computing environment utilizing a computation-storage integrated network and RDMA, what is the recommended approach for managing network congestion, considering the trade-offs between employing DCQCN and implementing network tuning methods such as traffic isolation and topology adjustment?",
        "Answer": "In a computation-storage integrated network utilizing RDMA, the recommended approach is to implement network tuning methods, such as traffic isolation and topology adjustments, instead of employing DCQCN. The text explicitly states that DCQCN was disabled because it could not simultaneously support HFReduce traffic and 3FS storage traffic within the integrated network. The implemented network tuning methods are 'Divergence of Different Traffics,' 'Topology Adjustment and Route Optimization,' 'NCCL Optimization,' and 'Network Tuning in 3FS.'"
    },
    {
        "context": [
            "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence Daya Guo*1\n, Qihao Zhu\u22171,2, Dejian Yang1 ,\nZhenda Xie1 , Kai Dong1 ,\nWentao Zhang1 Guanting Chen1 , Xiao Bi\n1 , Y. Wu1 , Y.K. Li1 , Fuli Luo1\n, Yingfei Xiong2 , Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder Abstract The rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. Figure 1 | The Performance of DeepSeek-Coder *Core contributors, ordered alphabetically by the name. arXiv:2401.14196v2 [cs.SE] 26 Jan 2024",
            "Looking ahead, our commitment is to develop and openly share\neven more powerful code-focused LLMs based on larger-scale general LLMs. Acknowledgements\nWe would like to express our gratitude to Bo Liu, Chengqi Deng, Chong Ruan, Damai Dai,\nJiashi Li, Kang Guan, Mingchuan Zhang, Panpan Huang, Shuiping Yu, Shirong Ma, Yaofeng\nSun, Yishi Piao, Zhihong Shao, and Zhewen Hao for their invaluable discussions and assistance\nduring training DeepSeek-Coder models. References\nL. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra,\nA. Gu, M. Dey, et al. Santacoder: don\u2019t reach for the stars! arXiv preprint arXiv:2301.03988,\n2023. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Terry,\nQ. Le, and C. M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen. F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee,\nY. Zi, C. J. Anderson, M. Q. Feldman, et al. Multipl-e: a scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Transactions on Software Engineering, 2023. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021. S. Chen, S. Wong, L. Chen, and Y. Tian. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. arXiv preprint\narXiv:1803.05457, 2018. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. T. Dao.",
            "\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. We find that it can significantly boost the capability of\ncross-file code generation. \u2022 Our analysis rigorously examines the impact of FIM training strategies on the pretraining\nphase of code models. 2. Data Collection The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1 , which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository\nlevel deduplication, and quality screening, as illustrated in Figure 2."
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Curious Onlooker",
            "Difficulty": "Mild"
        },
        "Question": "What types of data, including programming languages and natural languages, were used to train the DeepSeek-Coder model, and what purpose did each data type serve in improving the model's capabilities?",
        "Answer": "The DeepSeek-Coder model was trained on 87% source code, 10% English code-related natural language corpus, and 3% code-unrelated Chinese natural language corpus. The English corpus, composed of materials from GitHub's Markdown and StackExchange, enhances the model's understanding of code-related concepts and its ability to handle tasks like library usage and bug fixing. The Chinese corpus improves the model's proficiency in understanding the Chinese language. The model exhibits proficiency in understanding 87 programming languages."
    },
    {
        "context": [
            "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence Daya Guo*1\n, Qihao Zhu\u22171,2, Dejian Yang1 ,\nZhenda Xie1 , Kai Dong1 ,\nWentao Zhang1 Guanting Chen1 , Xiao Bi\n1 , Y. Wu1 , Y.K. Li1 , Fuli Luo1\n, Yingfei Xiong2 , Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder Abstract The rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. Figure 1 | The Performance of DeepSeek-Coder *Core contributors, ordered alphabetically by the name. arXiv:2401.14196v2 [cs.SE] 26 Jan 2024",
            "Looking ahead, our commitment is to develop and openly share\neven more powerful code-focused LLMs based on larger-scale general LLMs. Acknowledgements\nWe would like to express our gratitude to Bo Liu, Chengqi Deng, Chong Ruan, Damai Dai,\nJiashi Li, Kang Guan, Mingchuan Zhang, Panpan Huang, Shuiping Yu, Shirong Ma, Yaofeng\nSun, Yishi Piao, Zhihong Shao, and Zhewen Hao for their invaluable discussions and assistance\nduring training DeepSeek-Coder models. References\nL. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra,\nA. Gu, M. Dey, et al. Santacoder: don\u2019t reach for the stars! arXiv preprint arXiv:2301.03988,\n2023. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Terry,\nQ. Le, and C. M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen. F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee,\nY. Zi, C. J. Anderson, M. Q. Feldman, et al. Multipl-e: a scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Transactions on Software Engineering, 2023. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021. S. Chen, S. Wong, L. Chen, and Y. Tian. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. arXiv preprint\narXiv:1803.05457, 2018. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. T. Dao.",
            "\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. We find that it can significantly boost the capability of\ncross-file code generation. \u2022 Our analysis rigorously examines the impact of FIM training strategies on the pretraining\nphase of code models. 2. Data Collection The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1 , which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository\nlevel deduplication, and quality screening, as illustrated in Figure 2."
        ],
        "Q&A Gen Profile": {
            "Type": "How-to",
            "Persona": "The Curious Onlooker",
            "Difficulty": "Hard"
        },
        "Question": "How is the data for training the DeepSeek-Coder language models prepared, from initial collection to final use?",
        "Answer": "The data preparation for DeepSeek-Coder involves several steps, starting with a dataset composed of 87% source code, 10% English code-related natural language corpus from sources like GitHub's Markdown and StackExchange to improve the model's understanding of code-related concepts, and 3% code-unrelated Chinese natural language corpus of high-quality articles to enhance Chinese language proficiency. The process further includes data crawling, rule-based filtering, dependency parsing, repository-level deduplication, and quality screening, as illustrated in Figure 2."
    },
    {
        "context": [
            "\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. Additionally, they are available in various model scales to cater to a wide range\nof computational and application needs. \u2022 We make the first attempt to incorporate repository-level data construction during the\npre-training phase of our models. 2. Data Collection The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1 , which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository\nlevel deduplication, and quality screening, as illustrated in Figure 2. In the following, we will\ndescribe the data creation procedure step by step. Rule\nFiltering Dependency\nParsing Repo-level\nDeduplication Quality\nScreening Data Crawling Figure 2 | The Procedure of Dataset Creation",
            "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence Daya Guo*1\n, Qihao Zhu\u22171,2, Dejian Yang1 ,\nZhenda Xie1 , Kai Dong1 ,\nWentao Zhang1 Guanting Chen1 , Xiao Bi\n1 , Y. Wu1 , Y.K. Li1 , Fuli Luo1\n, Yingfei Xiong2 , Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder Abstract The rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. These models are pre-trained on a high-quality project-level code corpus and\nemploy a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art\nperformance among open-source code models across multiple benchmarks but also surpasses\nexisting closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted commercial use. Figure 1 | The Performance of DeepSeek-Coder *Core contributors, ordered alphabetically by the name. arXiv:2401.14196v2 [cs.SE] 26 Jan 2024",
            "1. Introduction\nThe field of software development has been significantly transformed by the swift advancement\nof large language models (OpenAI, 2023; Touvron et al., 2023), which have brought about\na new era of code intelligence. This series comprises\na range of open-source code models, varying in size from 1.3B to 33B, including the base\nversion and instructed version for each size. Each model in the series has been trained from\nscratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive\nunderstanding of coding languages and syntax. This adjustment allows our models to handle more\ncomplex and extensive coding tasks, thereby increasing their versatility and applicability in\nvarious coding scenarios. We have carried out comprehensive experiments using a variety of public code-related\nbenchmarks."
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "Considering the objective to create a versatile and commercially viable code-focused large language model, what were the critical motivations behind the specific choices made in the DeepSeek-Coder's training data composition, pre-training methodology, range of model sizes, and open-source licensing?",
        "Answer": "The DeepSeek-Coder's design choices were motivated by the intention to create a versatile code-focused large language model for both research and commercial applications. The data composition prioritizes source code (87%) with supporting English (10%) and Chinese (3%) natural language corpora to enhance code understanding, bug fixing, library usage, and Chinese language proficiency. The pre-training methodology, including repository-level data construction and fill-in-the-blank tasks, was designed to improve code generation and infilling. The range of model sizes (1.3B to 33B) caters to various computational needs. Finally, the permissive open-source license was chosen to address the limitations of closed-source models and promote wider research and development in the field."
    },
    {
        "context": [
            "\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. Additionally, they are available in various model scales to cater to a wide range\nof computational and application needs. \u2022 We make the first attempt to incorporate repository-level data construction during the\npre-training phase of our models. 2. Data Collection The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1 , which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository\nlevel deduplication, and quality screening, as illustrated in Figure 2. In the following, we will\ndescribe the data creation procedure step by step. Rule\nFiltering Dependency\nParsing Repo-level\nDeduplication Quality\nScreening Data Crawling Figure 2 | The Procedure of Dataset Creation",
            "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence Daya Guo*1\n, Qihao Zhu\u22171,2, Dejian Yang1 ,\nZhenda Xie1 , Kai Dong1 ,\nWentao Zhang1 Guanting Chen1 , Xiao Bi\n1 , Y. Wu1 , Y.K. Li1 , Fuli Luo1\n, Yingfei Xiong2 , Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder Abstract The rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. These models are pre-trained on a high-quality project-level code corpus and\nemploy a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art\nperformance among open-source code models across multiple benchmarks but also surpasses\nexisting closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted commercial use. Figure 1 | The Performance of DeepSeek-Coder *Core contributors, ordered alphabetically by the name. arXiv:2401.14196v2 [cs.SE] 26 Jan 2024",
            "1. Introduction\nThe field of software development has been significantly transformed by the swift advancement\nof large language models (OpenAI, 2023; Touvron et al., 2023), which have brought about\na new era of code intelligence. This series comprises\na range of open-source code models, varying in size from 1.3B to 33B, including the base\nversion and instructed version for each size. Each model in the series has been trained from\nscratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive\nunderstanding of coding languages and syntax. This adjustment allows our models to handle more\ncomplex and extensive coding tasks, thereby increasing their versatility and applicability in\nvarious coding scenarios. We have carried out comprehensive experiments using a variety of public code-related\nbenchmarks."
        ],
        "Q&A Gen Profile": {
            "Type": "Definitional",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "What is the composition of the training data used for DeepSeek-Coder models, and what is the specific purpose of including each type of data in the training corpus?",
        "Answer": "The training data for DeepSeek-Coder models is composed of 87% source code, 10% English code-related natural language corpus, and 3% code-unrelated Chinese natural language corpus. The English corpus is included to enhance the model's understanding of code-related concepts and improve its ability to handle tasks like library usage and bug fixing, while the Chinese corpus is included to improve the model's proficiency in understanding the Chinese language."
    },
    {
        "context": [
            "\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. Additionally, they are available in various model scales to cater to a wide range\nof computational and application needs. \u2022 We make the first attempt to incorporate repository-level data construction during the\npre-training phase of our models. We find that it can significantly boost the capability of\ncross-file code generation. \u2022 Our analysis rigorously examines the impact of FIM training strategies on the pretraining\nphase of code models. The outcomes of these comprehensive studies shed light on intriguing\naspects of FIM configurations, offering valuable insights that significantly contribute to the\nenhancement and development of code pretrained models. \u2022 We conduct extensive evaluations of our code LLMs against a wide array of benchmarks en\ncompassing numerous code-related tasks. 2. Data Collection The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1 , which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository\nlevel deduplication, and quality screening, as illustrated in Figure 2. In the following, we will\ndescribe the data creation procedure step by step. Rule\nFiltering Dependency\nParsing Repo-level\nDeduplication Quality\nScreening Data Crawling Figure 2 | The Procedure of Dataset Creation",
            "We use CrossCodeEval (Ding et al., 2023) to evaluate the\ncapabilities of currently available open-source code models of 7B scale in cross-file completion\ntasks. This dataset is constructed on a diverse set of real-world, open-sourced, permissively\nlicensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. The dataset is specifically designed to strictly require cross-file context for accurate completion. Notably, this dataset was constructed from repositories created between March and June 2023,\nwhile our pre-training data only includes code created before February 2023, which ensures that\nthis dataset was not present in our pre-training data, thus avoiding data leakage. Model Size Python Java TypeScript C# EM ES EM ES EM ES EM ES CodeGeex2 6B 8.11% 59.55% 7.34% 59.60% 6.14% 55.50% 1.70% 51.66% + Retrieval 10.73% 61.76% 10.10% 59.56% 7.72% 55.17% 4.64% 52.30% StarCoder-Base 7B 6.68% 59.55% 8.65% 62.57% 5.01% 48.83% 4.75% 59.53% + Retrieval 13.06% 64.24% 15.61% 64.78% 7.54% 42.06% 14.20% 65.03% CodeLlama-Base 7B 7.32% 59.66% 9.68% 62.64% 8.19% 58.50% 4.07% 59.19% + Retrieval 13.02% 64.30% 16.41% 64.64% 12.34% 60.64% 13.19% 63.04% DeepSeek-Coder-Base 6.7B 9.53% 61.65% 10.80% 61.77% 9.59% 60.17% 5.26% 61.32% + Retrieval 16.14% 66.51% 17.72% 63.18% 14.03% 61.77% 16.23% 63.42% + Retrieval w/o Repo Pre-training 16.02% 66.65% 16.64% 61.88% 13.23% 60.92% 14.48% 62.38% Table 7 | Performance of different models on cross-file code completion. In our evaluation of various models, we set the maximum sequence length to 2048 tokens,\nthe maximum output length to 50 tokens, and a limit of 512 tokens for the cross-file context. For\nthe cross-file context, we utilize the official BM25 search results provided by Ding et al. (2023). When only utilizing file-level code corpus (w/o Repo Pre-training) to pre-train DeepSeek-Coder,\nwe observe a decrease in performance in the Java, TypeScript, and C# languages, indicating the\neffectiveness of the repository-level pre-training. 14",
            "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence Daya Guo*1\n, Qihao Zhu\u22171,2, Dejian Yang1 ,\nZhenda Xie1 , Kai Dong1 ,\nWentao Zhang1 Guanting Chen1 , Xiao Bi\n1 , Y. Wu1 , Y.K. Li1 , Fuli Luo1\n, Yingfei Xiong2 , Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder Abstract The rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. These models are pre-trained on a high-quality project-level code corpus and\nemploy a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art\nperformance among open-source code models across multiple benchmarks but also surpasses\nexisting closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted commercial use. Figure 1 | The Performance of DeepSeek-Coder *Core contributors, ordered alphabetically by the name. arXiv:2401.14196v2 [cs.SE] 26 Jan 2024"
        ],
        "Q&A Gen Profile": {
            "Type": "Explanatory",
            "Persona": "The Expert",
            "Difficulty": "Hard"
        },
        "Question": "What key pre-training strategies were implemented during the development of DeepSeek-Coder to significantly improve its performance in code generation and what were the intended effects of these strategies?",
        "Answer": "The DeepSeek-Coder models employed several key pre-training strategies aimed at improving code generation. These include: 1) Extensive training on a large corpus of code data consisting of 87% source code, 10% English code-related natural language corpus, and 3% code-unrelated Chinese natural language corpus to improve general understanding of coding concepts. 2) Repository-level data construction during pre-training to enhance cross-file code generation. 3) Utilizing Fill-In-Middle (FIM) training strategies to enhance pre-training. 4) Training the models from scratch on 2 trillion tokens and pre-training on a high-quality project-level code corpus using a fill-in-the-blank task with a 16K window, all geared toward improving code generation and infilling."
    },
    {
        "context": [
            "\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. Additionally, they are available in various model scales to cater to a wide range\nof computational and application needs. \u2022 We make the first attempt to incorporate repository-level data construction during the\npre-training phase of our models. We find that it can significantly boost the capability of\ncross-file code generation. \u2022 Our analysis rigorously examines the impact of FIM training strategies on the pretraining\nphase of code models. The outcomes of these comprehensive studies shed light on intriguing\naspects of FIM configurations, offering valuable insights that significantly contribute to the\nenhancement and development of code pretrained models. \u2022 We conduct extensive evaluations of our code LLMs against a wide array of benchmarks en\ncompassing numerous code-related tasks. 2. Data Collection The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1 , which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository\nlevel deduplication, and quality screening, as illustrated in Figure 2. In the following, we will\ndescribe the data creation procedure step by step. Rule\nFiltering Dependency\nParsing Repo-level\nDeduplication Quality\nScreening Data Crawling Figure 2 | The Procedure of Dataset Creation",
            "We use CrossCodeEval (Ding et al., 2023) to evaluate the\ncapabilities of currently available open-source code models of 7B scale in cross-file completion\ntasks. This dataset is constructed on a diverse set of real-world, open-sourced, permissively\nlicensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. The dataset is specifically designed to strictly require cross-file context for accurate completion. Notably, this dataset was constructed from repositories created between March and June 2023,\nwhile our pre-training data only includes code created before February 2023, which ensures that\nthis dataset was not present in our pre-training data, thus avoiding data leakage. Model Size Python Java TypeScript C# EM ES EM ES EM ES EM ES CodeGeex2 6B 8.11% 59.55% 7.34% 59.60% 6.14% 55.50% 1.70% 51.66% + Retrieval 10.73% 61.76% 10.10% 59.56% 7.72% 55.17% 4.64% 52.30% StarCoder-Base 7B 6.68% 59.55% 8.65% 62.57% 5.01% 48.83% 4.75% 59.53% + Retrieval 13.06% 64.24% 15.61% 64.78% 7.54% 42.06% 14.20% 65.03% CodeLlama-Base 7B 7.32% 59.66% 9.68% 62.64% 8.19% 58.50% 4.07% 59.19% + Retrieval 13.02% 64.30% 16.41% 64.64% 12.34% 60.64% 13.19% 63.04% DeepSeek-Coder-Base 6.7B 9.53% 61.65% 10.80% 61.77% 9.59% 60.17% 5.26% 61.32% + Retrieval 16.14% 66.51% 17.72% 63.18% 14.03% 61.77% 16.23% 63.42% + Retrieval w/o Repo Pre-training 16.02% 66.65% 16.64% 61.88% 13.23% 60.92% 14.48% 62.38% Table 7 | Performance of different models on cross-file code completion. In our evaluation of various models, we set the maximum sequence length to 2048 tokens,\nthe maximum output length to 50 tokens, and a limit of 512 tokens for the cross-file context. For\nthe cross-file context, we utilize the official BM25 search results provided by Ding et al. (2023). When only utilizing file-level code corpus (w/o Repo Pre-training) to pre-train DeepSeek-Coder,\nwe observe a decrease in performance in the Java, TypeScript, and C# languages, indicating the\neffectiveness of the repository-level pre-training. 14",
            "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence Daya Guo*1\n, Qihao Zhu\u22171,2, Dejian Yang1 ,\nZhenda Xie1 , Kai Dong1 ,\nWentao Zhang1 Guanting Chen1 , Xiao Bi\n1 , Y. Wu1 , Y.K. Li1 , Fuli Luo1\n, Yingfei Xiong2 , Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder Abstract The rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. These models are pre-trained on a high-quality project-level code corpus and\nemploy a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art\nperformance among open-source code models across multiple benchmarks but also surpasses\nexisting closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted commercial use. Figure 1 | The Performance of DeepSeek-Coder *Core contributors, ordered alphabetically by the name. arXiv:2401.14196v2 [cs.SE] 26 Jan 2024"
        ],
        "Q&A Gen Profile": {
            "Type": "How-to",
            "Persona": "The Novice",
            "Difficulty": "Mild"
        },
        "Question": "What are the steps involved in creating the code training data for the DeepSeek-Coder large language model?",
        "Answer": "The creation of code training data for the DeepSeek-Coder large language model involves several key steps: data crawling to gather the initial data, rule-based filtering to remove irrelevant or low-quality data, dependency parsing to understand the relationships between code elements, repository-level deduplication to eliminate redundant data, and quality screening to ensure the final dataset's integrity. The training dataset is composed of 87% source code, 10% English code related natural language corpus (materials from GitHub\u2019s Markdown and StackExchange), and 3% code-unrelated Chinese natural language corpus."
    }
]