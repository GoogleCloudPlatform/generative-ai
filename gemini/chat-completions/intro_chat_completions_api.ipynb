{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Call Gemini by using the OpenAI Library\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fchat-completions%2Fintro_chat_completions_api.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/chat-completions/intro_chat_completions_api.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://goo.gle/4jeQztq\">\n",
        "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in  Cloud Skills Boost\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Developers already working with OpenAI's libraries can easily tap into the power of Gemini by leveraging the Chat Completions API. The Chat Completions API offers a streamlined way to experiment with and incorporate Gemini's capabilities into your existing AI applications.\n",
        "\n",
        "If you are not already using the OpenAI libraries, we recommend using the Google Gen AI SDK. Learn more about [calling Vertex AI models by using the OpenAI library](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-gemini-using-openai-library).\n",
        "\n",
        "In this tutorial, you learn to call Gemini using the OpenAI library. You will complete the following tasks:\n",
        "\n",
        "- Configure OpenAI SDK for the Chat Completions API\n",
        "- Send a chat completions request\n",
        "- Stream chat completions response\n",
        "- Send a multimodal request\n",
        "- Send a function calling request\n",
        "- Send a function calling request with the `tool_choice` parameter\n",
        "- Use controlled generation\n",
        "- Control thinking budget\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet openai google-auth requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")\n",
        "\n",
        "print(f\"Using Vertex AI with project: {PROJECT_ID} in location: {LOCATION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Chat Completions API Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHwJCyNF6u0O"
      },
      "source": [
        "### Configure OpenAI SDK for the Chat Completions API\n",
        "\n",
        "#### Import libraries\n",
        "\n",
        "The `google-auth` library is used to programmatically get Google credentials. Colab already has this library pre-installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kolJkBLdHaw0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from google.auth import default\n",
        "from google.auth.transport.requests import Request\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0K6VSJRHhH2"
      },
      "source": [
        "#### Authentication\n",
        "\n",
        "You can request an access token from the default credentials for the current environment. Note that the access token lives for [1 hour by default](https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0qceuiQEPHv"
      },
      "outputs": [],
      "source": [
        "# Programmatically get an access token\n",
        "credentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
        "credentials.refresh(Request())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q04wJmA0HT6X"
      },
      "source": [
        "Then configure the OpenAI SDK to point to the Chat Completions API endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-MRhsnlj6iw"
      },
      "outputs": [],
      "source": [
        "api_host = \"aiplatform.googleapis.com\"\n",
        "if LOCATION != \"global\":\n",
        "    api_host = f\"{LOCATION}-aiplatform.googleapis.com\"\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"https://{api_host}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi\",\n",
        "    api_key=credentials.token,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGokrtdiIHrX"
      },
      "source": [
        "### Supported models\n",
        "\n",
        "The Chat Completions API supports both Gemini models and select self-deployed models from Model Garden. Learn more about [supported models](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library#supported_models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7OhyH46H2H5"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"google/gemini-2.0-flash-001\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiWrF7U4Q5-E"
      },
      "source": [
        "### **Example**: Send a chat completions request\n",
        "\n",
        "The Chat Completions API takes a list of messages as input and returns a generated message as output. Although the message format is designed to make multi-turn conversations easy, it's just as useful for single-turn tasks without any conversation.\n",
        "\n",
        "In this example, you use the Chat Completions API to send a request to the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQTWLCzoHyUA"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID, messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNZGyOVOr1C"
      },
      "source": [
        "An example Chat Completions API response looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdfWBfggOvtf"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipbzw5FmO5zX"
      },
      "source": [
        "The generated content can be extracted with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxpdxYCxH51u"
      },
      "outputs": [],
      "source": [
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSIVXw7bMr4F"
      },
      "source": [
        "You can use `Markdown` to display the formatted text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOwEvNkCMiMF"
      },
      "outputs": [],
      "source": [
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpHc6sC6PIcI"
      },
      "source": [
        "### **Example**: Stream chat completions response\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hO8yLqnJEgh"
      },
      "outputs": [],
      "source": [
        "output_text = \"\"\n",
        "markdown_display_area = display(Markdown(output_text), display_id=True)\n",
        "\n",
        "for chunk in client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
        "    stream=True,\n",
        "):\n",
        "    output_text += chunk.choices[0].delta.content\n",
        "    markdown_display_area.update(Markdown(output_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqNdZLK8VLaf"
      },
      "source": [
        "### **Example**: Send a multimodal request\n",
        "You can send a multimodal prompt in a request to Gemini and get a text output.\n",
        "\n",
        "In this example, you ask the model to create a blog post for [this image](https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png) stored in a Cloud Storage bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOIu1Me-PQ6e"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Write a short, engaging blog post based on this picture.\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": \"gs://cloud-samples-data/generative-ai/image/meal.png\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJot5D_dE6G"
      },
      "source": [
        "### **Example**: Send a function calling request\n",
        "\n",
        "You can use the Chat Completions API for function calling with the Gemini models. The `tools` parameter in the API is used to provide function specifications. This is to enable models to generate function arguments which adhere to the provided specifications.\n",
        "\n",
        "In this example, you create function specifications to interface with a hypothetical weather API, then pass these function specifications to the Chat Completions API to generate function arguments that adhere to the specification.\n",
        "\n",
        "**Note** that in this example, the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arshz2C4W_ck"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.tool_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B82QRs9Oc1HL"
      },
      "source": [
        "### **Example**: Send a function calling request with the `tool_choice` parameter\n",
        "\n",
        "Using the `tools` parameter, if the functions parameter is provided then by default the model will decide when it is appropriate to use one of the functions.\n",
        "\n",
        "By default, `tool_choice` is set to `auto`. This lets the model decide whether to call functions and, if so, which functions to call. To disable function calling and force the model to only generate a user-facing message, you can set `tool_choice` to `none`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoEv7CKNXZu1"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.tool_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0706c915f870"
      },
      "source": [
        "### **Example**: Use controlled generation\n",
        "\n",
        "The Gemini models allow you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field. The response schema is specified in the `response_format` parameter, and the model output will strictly follow that schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "735d2eafeb2c"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"List a few popular cookie recipes and their ingredients.\",\n",
        "        }\n",
        "    ],\n",
        "    response_format=Recipe,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3857c959d8f5"
      },
      "source": [
        "### **Example**: Control thinking budget\n",
        "\n",
        "Gemini 2.5 models are trained to think through complex problems, leading to significantly improved reasoning. The Gemini API comes with a [`thinking_budget`](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking#budget) parameter which gives you control how much the model thinks during its responses.\n",
        "\n",
        "See the following mapping between the OpenAI API parameter `reasoning_effort` and the Gemini API parameter `thinking_budget`:\n",
        "\n",
        "| `reasoning_effort` | `thinking_budget` |\n",
        "|--------------------|-------------------|\n",
        "| `none`             | `0`               |\n",
        "| `low`              | `1024`            |\n",
        "| `medium`           | `8192`            |\n",
        "| `high`             | `24576`           |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98f16a7eb5c9"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Write a bash script that takes a matrix represented as a string with \n",
        "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"google/gemini-2.5-flash-preview-04-17\",\n",
        "    reasoning_effort=\"medium\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_chat_completions_api.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
