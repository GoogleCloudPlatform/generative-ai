{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMQCs0oQf5Jo"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR0jdheRGG89"
      },
      "source": [
        "# Introduction to Generative AI functions in BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td9kx9LVgSve"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fapplying-llms-to-data%2Fbigquery_generative_ai_intro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery Studio logo\"><br> Open in BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_generative_ai_intro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h3O5b6P8WEx"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Alicia Williams](https://github.com/aliciawilliams) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial will guide you through powerful generative AI capabilities available in BigQuery. You'll get hands-on experience using the suite of generative **`AI.*` functions** that integrate directly with powerful models like Gemini. This allows you to perform sophisticated AI-driven analysis on your data right within your familiar SQL environment.\n",
        "\n",
        "While the examples in this notebook will focus on **text inputs** to the generative AI models, many of these capabilities extend to **multi-modal analysis**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwQLISjqGTHd"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "We'll cover how to:\n",
        "\n",
        "* **Prompt models to generate text and structured data** with `AI.GENERATE`, `AI.GENERATE_TABLE`, and `ML.GENERATE_TEXT`.\n",
        "* **Perform powerful, row-level analysis** using scalar functions like `AI.GENERATE_BOOL`, `AI.GENERATE_DOUBLE`, and `AI.GENERATE_INT`.\n",
        "* **Forecast future trends** with time-series data using `AI.FORECAST`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs_md"
      },
      "source": [
        "### Services and Costs\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* **BigQuery**: [Pricing](https://cloud.google.com/bigquery/pricing)\n",
        "\n",
        "* **BigQuery ML**: [Pricing](https://cloud.google.com/bigquery/pricing#bqml)\n",
        "\n",
        "* **Vertex AI**: [Pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing)\n",
        "\n",
        "You can use the [Pricing Calculator](https://cloud.google.com/products/calculator) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkoCFoFVSPii"
      },
      "source": [
        "---\n",
        "\n",
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_md_1"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the BigQuery, BigQuery Connection, and Vertex AI APIs](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com,bigqueryconnection.googleapis.com,aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ3g-h7uTaSf"
      },
      "source": [
        "### Set your project ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auth_md"
      },
      "source": [
        "### Authenticate to your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6NjZRCXU5Ro"
      },
      "source": [
        "**1. Colab Enterprise or BigQuery Studio Notebooks**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0dV1hvAU1ed"
      },
      "source": [
        "**2. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth_code"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pp4LAJ3UyRP"
      },
      "source": [
        "**3. Local JupyterLab instance, uncomment and run:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzU7S3fMVDkW"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conn_md"
      },
      "source": [
        "### Create BigQuery Cloud resource connection\n",
        "\n",
        "You will need to create a [Cloud resource connection](https://cloud.google.com/bigquery/docs/create-cloud-resource-connection) to enable BigQuery to interact with Vertex AI services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "conn_code"
      },
      "outputs": [],
      "source": [
        "!bq mk --connection --location=us \\\n",
        "    --connection_type=CLOUD_RESOURCE test_connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perms_md"
      },
      "source": [
        "### Set permissions for Service Account\n",
        "\n",
        "The resource connection service account requires certain project-level permissions to interact with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OqpZNY953xR"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCT = !bq show --format=prettyjson --connection us.test_connection | grep \"serviceAccountId\" | cut -d '\"' -f 4\n",
        "SERVICE_ACCT_EMAIL = SERVICE_ACCT[-1]\n",
        "print(SERVICE_ACCT_EMAIL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perms_code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "!gcloud projects add-iam-policy-binding --format=none $PROJECT_ID --member=serviceAccount:$SERVICE_ACCT_EMAIL --role='roles/bigquery.connectionUser'\n",
        "!gcloud projects add-iam-policy-binding --format=none $PROJECT_ID --member=serviceAccount:$SERVICE_ACCT_EMAIL --role='roles/aiplatform.user'\n",
        "\n",
        "# wait 60 seconds, give IAM updates time to propagate, otherwise, following cells will fail\n",
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_md_1"
      },
      "source": [
        "---\n",
        "\n",
        "## Generate text and structured data with `AI.GENERATE` and `AI.GENERATE_TABLE`\n",
        "\n",
        "These functions allow you to leverage the power of large language models (LLMs) directly within BigQuery to generate new text content, including creating content with a specified schema. Both functions work by sending requests to your choice of a [generally available](https://cloud.google.com/vertex-ai/generative-ai/docs/models#generally_available_models) or [preview](https://cloud.google.com/vertex-ai/generative-ai/docs/models#preview_models) Gemini model, and then returning that model's response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_text_md"
      },
      "source": [
        "### Using `AI.GENERATE`: Extract keywords from reviews\n",
        "\n",
        "Let's use the [`AI.GENERATE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate) function to extract keywords from the movie reviews in the `bigquery-public-data.imdb.reviews` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_text_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  AI.GENERATE(('Extract the keywords from the text below: ', review),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result AS keywords\n",
        "FROM\n",
        "  `bigquery-public-data.imdb.reviews`\n",
        "WHERE movie_id = \"tt0067345\"\n",
        "LIMIT 5\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVKjzbVcZ2x"
      },
      "source": [
        "You can see that BigQuery has prompted the specified model for each row of the data and returned the response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulkyyt6pUpfQ"
      },
      "source": [
        "#### Using arguments to adjust model configuration\n",
        "In addition to the `prompt`, `connection_id`, and `endpoint`, there are additional arguments available in `AI.GENERATE` at your disposal for customizing your generation request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY5ksh35WMnd"
      },
      "source": [
        "##### The `model_params` argument\n",
        "There are several parameters you can specify within the `model_params` argument. You can read more about which specific parameters are available for `AI.GENERATE` in the [documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate#arguments). The next query adds custom `temperature` and `maxOutputTokens` parameters.\n",
        "\n",
        "* The `temperature` parameter controls the randomness of the response, where a lower value makes the output more predictable and focused, while a higher value encourages more creative and diverse results.\n",
        "\n",
        "* The `maxOutputTokens` parameter sets the maximum length of the model's response by limiting the total number of tokens (pieces of words) it can generate.\n",
        "\n",
        "You can read more about the various parameters [here](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#generationconfig), and the parameters' default values are documented in the model card in the [model documentation for Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61nesgOtWkWs"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  AI.GENERATE(('Extract the keywords from the text below: ', review),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash',\n",
        "    model_params => JSON '{\"generationConfig\":{\"temperature\": 0.5, \"maxOutputTokens\": 1000}}'\n",
        "    ).result AS keywords\n",
        "FROM\n",
        "  `bigquery-public-data.imdb.reviews`\n",
        "WHERE movie_id = \"tt0067345\"\n",
        "LIMIT 5\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnSyQrNzb_4A"
      },
      "source": [
        "Why are some of the responses empty? By setting the `maxOutputTokens` value to 1,000 (much lower than the default 65,535 which was used in the first queries), you are seeing this limit in action. Gemini 2.5 Flash has [thinking capabilities](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking), which means the model goes through a \"thinking process\" before determining and returning its response, and this thinking process consumes output tokens. In this case, the model is hitting its `maxOutputTokens` value before it moves from its thinking process to returning the actual response.\n",
        "\n",
        "You can resolve this issue by increasing the `maxOutputTokens` value and/or setting a `thinking_budget` to limit the amount of tokens that can be consumed by the thinking process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBBpEQIYJ2Dt"
      },
      "source": [
        "###### Setting a `thinking_budget`\n",
        "\n",
        "You can set a [`thinking_budget`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate#thinking-budget) to manage how many output tokens are allocated to the thinking phase of the model's response development.\n",
        "\n",
        "The next query shows how to set the `thinking_budget` as an additional parameter in the `model_params` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2fNyr7aGbDP"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  AI.GENERATE(('Extract the keywords from the text below: ', review),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash',\n",
        "    model_params => JSON '{\"generationConfig\":{\"temperature\": 0.5, \"maxOutputTokens\": 1000, \"thinking_config\": {\"thinking_budget\": 500}}}'\n",
        "    ).result AS keywords\n",
        "FROM\n",
        "  `bigquery-public-data.imdb.reviews`\n",
        "WHERE movie_id = \"tt0067345\"\n",
        "LIMIT 5\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edQ8hdPb1QMm"
      },
      "source": [
        "##### The `output_schema` argument\n",
        "\n",
        "As you can see in the last query, the model's responses were not provided in a standard structure. You can use the **`output_schema`** argument to define the format of the responses.\n",
        "\n",
        "First, you'll specify the output as a string of key words by adding `output_schema => 'keywords ARRAY<STRING>'` to the `AI.GENERATE` request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w-oUv8UimxT"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  AI.GENERATE(\n",
        "    ('Extract the keywords from the text below: ', review),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash',\n",
        "    output_schema => 'keywords ARRAY<STRING>').keywords\n",
        "FROM\n",
        "  `bigquery-public-data.imdb.reviews`\n",
        "WHERE movie_id = \"tt0067345\"\n",
        "LIMIT 5\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoF8XxEa1HsW"
      },
      "source": [
        "Now the responses are now arrays of keywords - a more useful format for this use case!\n",
        "\n",
        "Next, you'll adjust the prompt to the model to extract the sentiment as well, and adjust the `output_schema` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1wfmGVZkA1x"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  AI.GENERATE(\n",
        "    ('Extract the keywords and sentiment (Positive, Negative, or Neutral) from the following review: ', review),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash',\n",
        "    output_schema => 'keywords ARRAY<STRING>, sentiment STRING').full_response\n",
        "FROM\n",
        "  `bigquery-public-data.imdb.reviews`\n",
        "WHERE movie_id = \"tt0067345\"\n",
        "LIMIT 5\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmpYd5Dg2AJy"
      },
      "source": [
        "This time, the response is in a JSON format. Why?\n",
        "\n",
        "When using `output_schema` with [`AI.GENERATE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate), you can specify in the SQL which field you want to return. In this case the SQL specifies the `.full_response` which will provide a JSON object containing the various text response elements. You may alternatively specify to return any one of the schema columns you defined, such as `.keywords` (as was used in the first query of this section) or `.sentiment`. However, you cannot specify to return multiple columns.\n",
        "\n",
        " You could use SQL to parse the JSON provided in the `.full_response`,  but for ease of use, let's look at the [`AI.GENERATE_TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-table) function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_table_md"
      },
      "source": [
        "### Using `AI.GENERATE_TABLE`: Extract keywords and sentiment from movie reviews\n",
        "\n",
        "The [`AI.GENERATE_TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-table) function provides expanded functionality for turning unstructured text into a structured table. Here, you'll perform the same analysis to extract keywords and overall sentiment from the `bigquery-public-data.imdb_reviews` table and return the response as multiple columns in one query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_md"
      },
      "source": [
        "#### Create a BigQuery Dataset\n",
        "\n",
        "The syntax for using [`AI.GENERATE_TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-table) differs from [`AI.GENERATE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate), and it requires that you first create a remote model in BigQuery that connects to the one of the [generally available](https://cloud.google.com/vertex-ai/generative-ai/docs/models#generally_available_models) or [preview](https://cloud.google.com/vertex-ai/generative-ai/docs/models#preview_models) Gemini models.\n",
        "\n",
        "Running the following query will create a BigQuery dataset called **`bq_ai_tutorial`** to house the remote model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE SCHEMA\n",
        "  `bq_ai_tutorial` OPTIONS (location = 'US');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model_md"
      },
      "source": [
        "#### Create a remote model for Gemini 2.5 Flash\n",
        "\n",
        "Now you'll create the remote model, which is simply a pointer to the model endpoint. In this example, you'll continue to use the `gemini-2.5-flash` model endpoint.\n",
        "\n",
        "Once you create this remote model, it is saved in your `bq_ai_tutorial` dataset so that you can use it in any future analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE OR REPLACE MODEL\n",
        "  `bq_ai_tutorial.gemini_2_5_flash`\n",
        "REMOTE WITH\n",
        "    CONNECTION `us.test_connection`\n",
        "    OPTIONS (ENDPOINT = 'gemini-2.5-flash');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBiwwjzhendG"
      },
      "source": [
        "#### Run analysis using the remote model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmVE4UxdnoqB"
      },
      "source": [
        "Now that the remote model has been created, you can prompt it with [`AI.GENERATE_TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-table) to analyze data and specify the output schema with multiple columns returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_table_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  keywords,\n",
        "  sentiment\n",
        "FROM\n",
        "  AI.GENERATE_TABLE(\n",
        "    MODEL `bq_ai_tutorial.gemini_2_5_flash`,\n",
        "    (\n",
        "      SELECT\n",
        "        review,\n",
        "        ('Extract the keywords and sentiment (Positive, Negative, or Neutral) from the following review:',review) AS prompt,\n",
        "      FROM\n",
        "        `bigquery-public-data.imdb.reviews`\n",
        "      WHERE movie_id = \"tt0067345\"\n",
        "      LIMIT 5\n",
        "    ),\n",
        "    STRUCT(\n",
        "      'keywords ARRAY<STRING>, sentiment STRING' AS output_schema\n",
        "    )\n",
        "  );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkdFwoOp1cRm"
      },
      "source": [
        "#### Saving results in a BigQuery table\n",
        "Wrapping any `SELECT` statement with the [`CREATE TABLE` statement](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement) allows you to create a permanent table from your query results. You can run the next two queries to see that in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0yvijK71Zbm"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE OR REPLACE TABLE `bq_ai_tutorial.reviews_keywords_sentiment` AS (\n",
        "  SELECT\n",
        "    review,\n",
        "    keywords,\n",
        "    sentiment\n",
        "  FROM\n",
        "    AI.GENERATE_TABLE(\n",
        "      MODEL `bq_ai_tutorial.gemini_2_5_flash`,\n",
        "      (\n",
        "        SELECT\n",
        "          review,\n",
        "          ('Extract the keywords and sentiment (Positive, Negative, or Neutral) from the following review:',review) AS prompt,\n",
        "        FROM\n",
        "          `bigquery-public-data.imdb.reviews`\n",
        "        WHERE movie_id = \"tt0067345\"\n",
        "        LIMIT 5\n",
        "      ),\n",
        "      STRUCT(\n",
        "        'keywords ARRAY<STRING>, sentiment STRING' AS output_schema)));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW88rus93d9D"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT *\n",
        "FROM `bq_ai_tutorial.reviews_keywords_sentiment`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Hz06RiecoI"
      },
      "source": [
        "#### Using arguments to adjust model configuration\n",
        "Similar to `AI.GENERATE`, [`AI.GENERATE_TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-table) also allows you to customize the parameters for your generation request. The placement of these parameters is within the `STRUCT` value. The [documentation page](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-table#:~:text=dataset.table%60%20%7C%20(query_statement)%20%7D%2C-,STRUCT(,-output_schema%20AS%20output_schema) contains the available fields you can define within the `STRUCT`.\n",
        "\n",
        "Next you'll take the same query and now define values for `temperature` and `max_output_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyGREvkxeXzh"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  keywords,\n",
        "  sentiment\n",
        "FROM\n",
        "  AI.GENERATE_TABLE(\n",
        "    MODEL `bq_ai_tutorial.gemini_2_5_flash`,\n",
        "    (\n",
        "      SELECT\n",
        "        review,\n",
        "        ('Extract the key words and sentiment (Positive, Negative, or Neutral) from the following review:',review) AS prompt,\n",
        "      FROM\n",
        "        `bigquery-public-data.imdb.reviews`\n",
        "      WHERE movie_id = \"tt0067345\"\n",
        "      LIMIT 5\n",
        "    ),\n",
        "    STRUCT(\n",
        "      'keywords ARRAY<STRING>, sentiment STRING' AS output_schema,\n",
        "      0.5 AS temperature,\n",
        "      1000 AS max_output_tokens\n",
        "    )\n",
        "  );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN51GtTMYEoF"
      },
      "source": [
        "## Generate text with partner and open models with `ML.GENERATE_TEXT`\n",
        "\n",
        "The tutorial thus far has performed analysis using the [Gemini family of models](https://cloud.google.com/vertex-ai/generative-ai/docs/models#generally_available_models). However, the generative analysis capabilities of BigQuery extend to several [partner models](https://cloud.google.com/bigquery/docs/generate-text#enable-model), including Anthropic Claude, Llama, and Mistral AI, as well as [open models](https://cloud.google.com/bigquery/docs/generate-text#deploy_an_open_model) from the [Vertex AI Model Garden and Hugging Face](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model-open#supported_open_models).\n",
        "\n",
        "When choosing a partner or open model, you will use the [`ML.GENERATE_TEXT` function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-text) to perform your analysis. The syntax for the [`ML.GENERATE_TEXT` function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-text) is similar to `AI.GENERATE_TABLE`. Let's walk through an example analysis using a partner model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB90JcraZrwf"
      },
      "source": [
        "### Enabling a partner model\n",
        "\n",
        "You must enable partner models in Vertex AI before you can use them in BigQuery.\n",
        "\n",
        "**BEFORE MOVING AHEAD**, enable the [**Anthropic Claude 3 Haiku** model](https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-3-haiku) by following the [instructions in the documentation](https://cloud.google.com/bigquery/docs/generate-text#enable-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plOQu7eIjeEs"
      },
      "source": [
        "### Create a remote model for Anthropic Claude 3 Haiku\n",
        "\n",
        "Now you'll create a new remote model, pointing to the `claude-3-haiku@20240307` model endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrYOI_HGjQT2"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE OR REPLACE MODEL\n",
        "  `bq_ai_tutorial.claude_3_haiku`\n",
        "REMOTE WITH\n",
        "    CONNECTION `us.test_connection`\n",
        "    OPTIONS (ENDPOINT = 'claude-3-haiku@20240307');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtJ5nuMbmY0C"
      },
      "source": [
        "### Run analysis using the partner model\n",
        "\n",
        "Now you are ready to use the remote model with an [`ML.GENERATE_TEXT` function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-text). The following query repeats your previous analysis done to extract key words and sentiment from the reviews, now using Anthropic Claude 3 Haiku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaPTzBYrZnr4"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  review,\n",
        "  ml_generate_text_llm_result as response\n",
        "FROM\n",
        "  ML.GENERATE_TEXT(\n",
        "    MODEL `bq_ai_tutorial.claude_3_haiku`,\n",
        "    (\n",
        "      SELECT\n",
        "        review,\n",
        "        CONCAT('Extract the keywords and sentiment (Positive, Negative, or Neutral) from the following review:',review) AS prompt,\n",
        "      FROM\n",
        "        `bigquery-public-data.imdb.reviews`\n",
        "      WHERE movie_id = \"tt0067345\"\n",
        "      LIMIT 5\n",
        "    ),\n",
        "    STRUCT(\n",
        "      TRUE AS flatten_json_output,\n",
        "      500 AS max_output_tokens\n",
        "    )\n",
        "  );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scalar_md"
      },
      "source": [
        "## Perform row-Level AI with scalar functions `AI.GENERATE_BOOL`, `AI.GENERATE_DOUBLE`, and `AI.GENERATE_INT`\n",
        "\n",
        "BigQuery's scalar AI functions ([`AI.GENERATE_BOOL`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-bool), [`AI.GENERATE_DOUBLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-double), and [`AI.GENERATE_INT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-int)) bring the power of LLMs for AI-driven filtering, classification, and data extraction directly within your SQL queries.\n",
        "\n",
        "On the surface, these functions perform tasks similar to that of using `AI.GENERATE` with a defined `output_schema` of data type of `BOOL`, `FLOAT64`, or `INT64`. However, these functions aren't only available for use in a `SELECT` clause, but also in a `WHERE` clause, giving you the power to do complex filtering with natural language. The next few examples focus on the filtering use case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_bool_md"
      },
      "source": [
        "### Using `AI.GENERATE_BOOL`: Filtering for drought tolerant tree species\n",
        "\n",
        "Let's use [`AI.GENERATE_BOOL`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-bool) to return tree species in `bigquery-public-data.new_york_trees.tree_species` that are drought tolerant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_bool_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  species_scientific_name,\n",
        "  species_common_name\n",
        "FROM\n",
        "  `bigquery-public-data.new_york_trees.tree_species`\n",
        "WHERE\n",
        "  AI.GENERATE_BOOL(\n",
        "    ('Is this tree species drought tolerant?', species_scientific_name),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result = true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_double_md"
      },
      "source": [
        "### Using `AI.GENERATE_DOUBLE`: Filtering for tree species with a minimum average lifespan\n",
        "\n",
        "With [`AI.GENERATE_DOUBLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-double), we can get a numerical response of type `FLOAT64`, such as average lifespan in years of each tree species. Again, we'll use `bigquery-public-data.new_york_trees.tree_species` table, this time filtering for species that meet a minimum average lifespan requirement of 19 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT9vakyw1PZ2"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  species_scientific_name,\n",
        "  species_common_name\n",
        "FROM\n",
        "  `bigquery-public-data.new_york_trees.tree_species`\n",
        "WHERE\n",
        "  AI.GENERATE_INT(\n",
        "    ('What is the average life in years of this tree species?', species_scientific_name),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result > 19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_int_md"
      },
      "source": [
        "### Using `AI.GENERATE_INT`: Filtering for tree species of specific average mature height\n",
        "\n",
        "[`AI.GENERATE_INT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-int) is similar to [`AI.GENERATE_DOUBLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-double), except that it returns data of type `INT64`, instead of `FLOAT64`. Let's use it to return tree species with an average mature height of 10-20 meters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_int_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  species_scientific_name,\n",
        "  species_common_name\n",
        "FROM\n",
        "  `bigquery-public-data.new_york_trees.tree_species`\n",
        "WHERE\n",
        "  AI.GENERATE_DOUBLE(\n",
        "    ('What is the average height in meters of this tree species at full maturity?', species_scientific_name),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result BETWEEN 10 AND 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXSfuNKs521-"
      },
      "source": [
        "You can also combine `AI.GENERATE_DOUBLE`, `AI.GENERATE_INT`, and `AI.GENERATE_BOOL` scalar functions within one query. Here's an example combining the AI filters of the last three queries into one `WHERE` clause:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mpFmVGG5yrG"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  species_scientific_name,\n",
        "  species_common_name\n",
        "FROM\n",
        "  `bigquery-public-data.new_york_trees.tree_species`\n",
        "WHERE\n",
        "  AI.GENERATE_DOUBLE(\n",
        "    ('What is the average height in meters of this tree species at full maturity?', species_scientific_name),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result BETWEEN 10 AND 20 AND\n",
        "  AI.GENERATE_INT(\n",
        "    ('What is the average life in years of this tree species?', species_scientific_name),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result > 19 AND\n",
        "  AI.GENERATE_BOOL(\n",
        "    ('Is this tree species drought tolerant?', species_scientific_name),\n",
        "    connection_id => 'us.test_connection',\n",
        "    endpoint => 'gemini-2.5-flash').result = true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forecast_md"
      },
      "source": [
        "---\n",
        "\n",
        "## Forecast time series with `AI.FORECAST`\n",
        "\n",
        "The Google Research [`TimesFM`](https://cloud.google.com/bigquery/docs/timesfm-model) model is a foundation model for time-series forecasting that has been pre-trained on billions of time-points from many real-world datasets, so you can apply it to new forecasting datasets across many domains.\n",
        "\n",
        "The [`AI.FORECAST`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-forecast) function in BigQuery leverages the model to provide high-quality time series forecasting without the need to create and train your own model.\n",
        "\n",
        "For this example, we'll use the `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` data to forecast hourly bikeshare trips by subscriber type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "forecast_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery forecast_results --project {PROJECT_ID}\n",
        "\n",
        "SELECT *\n",
        "FROM\n",
        "  AI.FORECAST(\n",
        "    (\n",
        "      SELECT\n",
        "        TIMESTAMP_TRUNC(start_date, HOUR) as trip_hour,\n",
        "        subscriber_type,\n",
        "        COUNT(*) as num_trips\n",
        "      FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n",
        "      WHERE start_date >= TIMESTAMP('2018-01-01')\n",
        "      GROUP BY TIMESTAMP_TRUNC(start_date, HOUR), subscriber_type\n",
        "    ),\n",
        "    horizon => 720,\n",
        "    confidence_level => 0.95,\n",
        "    timestamp_col => 'trip_hour',\n",
        "    data_col => 'num_trips',\n",
        "    id_cols => ['subscriber_type']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forecast_text_md"
      },
      "source": [
        "The results of this query were saved as a pandas DataFrame. Running the next cell will display the DataFrame, including the forecasted number of trips by subscriber type (annualy/monthly subscriber, or short-term customer), for the next 720 hours (30 days), along with the prediction interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rkQpI4KQpB5"
      },
      "outputs": [],
      "source": [
        "forecast_results.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGJwVQbcRRcv"
      },
      "source": [
        "We can view the results as a chart using python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVwumTBNUZSO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate the data by subscriber type\n",
        "customer_df = forecast_results[forecast_results[\"subscriber_type\"] == \"Customer\"]\n",
        "subscriber_df = forecast_results[forecast_results[\"subscriber_type\"] == \"Subscriber\"]\n",
        "\n",
        "# Create a figure and axes for the plot\n",
        "fig, ax = plt.subplots(figsize=(15, 7))\n",
        "\n",
        "# Plot the forecast for 'Customer'\n",
        "ax.plot(\n",
        "    customer_df[\"forecast_timestamp\"],\n",
        "    customer_df[\"forecast_value\"],\n",
        "    label=\"Customer Forecast\",\n",
        ")\n",
        "ax.fill_between(\n",
        "    customer_df[\"forecast_timestamp\"],\n",
        "    customer_df[\"prediction_interval_lower_bound\"],\n",
        "    customer_df[\"prediction_interval_upper_bound\"],\n",
        "    color=\"blue\",\n",
        "    alpha=0.1,\n",
        "    label=\"Customer Confidence Interval\",\n",
        ")\n",
        "\n",
        "# Plot the forecast for 'Subscriber'\n",
        "ax.plot(\n",
        "    subscriber_df[\"forecast_timestamp\"],\n",
        "    subscriber_df[\"forecast_value\"],\n",
        "    label=\"Subscriber Forecast\",\n",
        ")\n",
        "ax.fill_between(\n",
        "    subscriber_df[\"forecast_timestamp\"],\n",
        "    subscriber_df[\"prediction_interval_lower_bound\"],\n",
        "    subscriber_df[\"prediction_interval_upper_bound\"],\n",
        "    color=\"orange\",\n",
        "    alpha=0.1,\n",
        "    label=\"Subscriber Confidence Interval\",\n",
        ")\n",
        "\n",
        "\n",
        "# Set the title and labels\n",
        "ax.set_title(\"Bikeshare Trips Forecast\")\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Number of Trips\")\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Rotate the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc_YRVNIfS7Q"
      },
      "source": [
        "The forecast shows usage patterns you might expect:\n",
        "\n",
        "* **Subscribers** are typically commuters who use the service for regular, predictable trips, such as going to and from work. Their forecast shows expected weekday peaks during morning and evening commute hours, and lower weekend usage.\n",
        "\n",
        "* **Customers** are usually tourists or casual riders and you might expect them to have higher usage on weekends and holidays for leisure activities, and more trips during the middle of the day, rather than concentrated during traditional commute times.\n",
        "\n",
        "An interesting start to an analysis without any model training!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup_md"
      },
      "source": [
        "# Cleaning Up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup_code"
      },
      "outputs": [],
      "source": [
        "# Delete BigQuery dataset, including the BigQuery remote model you just created, and the BigQuery Connection\n",
        "! bq rm -r -f $PROJECT_ID:bq_ai_tutorial\n",
        "! bq rm --connection --project_id=$PROJECT_ID --location=us test_connection"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "bigquery_generative_ai_intro.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
