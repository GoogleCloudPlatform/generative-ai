{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIMTPB1WoTq"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPmcMNgZpo9V"
      },
      "source": [
        "# Analyze a codebase with Gemini in Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fcode%2Fanalyze_codebase.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/code/analyze_codebase.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://goo.gle/40yTNki\">\n",
        "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in  Cloud Skills Boost\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EExYZvij2ve"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner), [Aakash Gouda](https://github.com/aksstar)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yVV6txOmNMn"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Gemini features a breakthrough long context window of up to 1 million tokens that can help seamlessly analyze, classify and summarize large amounts of content within a given prompt.\n",
        "\n",
        "With its long-context reasoning, Gemini can analyze an entire codebase for deeper insights.\n",
        "\n",
        "In this tutorial, you learn how to analyze an entire codebase with Gemini 2.0 and prompt the model to:\n",
        "\n",
        "- **Analyze**: Summarize codebases effortlessly.\n",
        "- **Guide**: Generate clear developer getting-started documentation.\n",
        "- **Debug**: Uncover critical bugs and provide fixes.\n",
        "- **Enhance**: Implement new features and improve reliability and security.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdMGtr18rFdL"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK for Python and other libraries\n",
        "\n",
        "In addition to the [Google Gen AI SDK for Python](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview), we will be using [Gitingest](https://gitingest.com/) to load the repository into the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai gitingest gitpython PyGithub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and create client\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbozY-XKee95"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NSCFmvOWBas9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from IPython.display import Markdown, display\n",
        "import git\n",
        "from github import Github\n",
        "from gitingest import ingest\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from google.genai.types import CreateCachedContentConfig, GenerateContentConfig\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNoOTMp2fe33"
      },
      "source": [
        "## Cloning a codebase\n",
        "\n",
        "You will use the repo [Online Boutique](https://github.com/GoogleCloudPlatform/microservices-demo) as an example in this notebook.the Online Boutique is a cloud-first microservices demo application. The application is a web-based e-commerce app where users can browse items, add them to the cart, and purchase them. This application consists of 11 microservices across multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GlDOs49qgStM"
      },
      "outputs": [],
      "source": [
        "# The GitHub repository URL\n",
        "repo_url = \"https://github.com/GoogleCloudPlatform/microservices-demo\"  # @param {type:\"string\"}\n",
        "\n",
        "# The location to clone the repo\n",
        "repo_dir = \"./repo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAm1ly9pfIEX"
      },
      "source": [
        "#### Define helper functions for processing GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "stNia6UaHau2"
      },
      "outputs": [],
      "source": [
        "def clone_repo(repo_url: str, repo_dir: str) -> None:\n",
        "    \"\"\"Shallow clone a GitHub repository.\"\"\"\n",
        "    if os.path.exists(repo_dir):\n",
        "        shutil.rmtree(repo_dir)\n",
        "    os.makedirs(repo_dir, exist_ok=True)\n",
        "    git.Repo.clone_from(repo_url, repo_dir, depth=2)\n",
        "\n",
        "\n",
        "def get_github_issue(owner: str, repo: str, issue_number: int) -> str | None:\n",
        "    \"\"\"\n",
        "    Fetch the contents of a GitHub issue.\n",
        "\n",
        "    Args:\n",
        "        owner (str): The owner of the repository.\n",
        "        repo (str): The name of the repository.\n",
        "        issue_number (int): The issue number to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str | None: The issue body if found, otherwise None.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs while fetching the issue.\n",
        "    \"\"\"\n",
        "    g = Github()\n",
        "\n",
        "    try:\n",
        "        repository = g.get_repo(f\"{owner}/{repo}\")\n",
        "        issue = repository.get_issue(number=issue_number)\n",
        "        return issue.body\n",
        "    except Exception as error:\n",
        "        print(f\"Error fetching issue: {error}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_git_diff(repo_dir: str) -> str:\n",
        "    \"\"\"Fetches commit IDs from a local Git repository on a specified branch.\"\"\"\n",
        "    repo = git.Repo(repo_dir)\n",
        "    branch_name = \"main\"\n",
        "\n",
        "    # A list of commit IDs (SHA-1 hashes) in reverse chronological order (newest first)\n",
        "    commit_ids = [commit.hexsha for commit in repo.iter_commits(branch_name)]\n",
        "    if len(commit_ids) >= 2:\n",
        "        return repo.git.diff(commit_ids[0], commit_ids[1])\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2947e8614485"
      },
      "source": [
        "### Create an index and extract content of a codebase\n",
        "\n",
        "Clone the repo and create an index and extract content of code/text files.\n",
        "\n",
        "Gitingest will extract all of the contents of the files into a long string and create a directory tree of the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eb417d380c9"
      },
      "outputs": [],
      "source": [
        "clone_repo(repo_url, repo_dir)\n",
        "\n",
        "_, tree, content = ingest(repo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiVQB5SKekS0"
      },
      "source": [
        "## Analyzing the codebase with Gemini\n",
        "\n",
        "With its long-context reasoning, Gemini can process the codebase and answer questions about the codebase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1nfXrqRxVX"
      },
      "source": [
        "### Load the Gemini model\n",
        "\n",
        "Learn more about the [Gemini API models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vB9gY3WruzK9"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f3c0bbef54"
      },
      "source": [
        "### Create a context cache\n",
        "\n",
        "We will create a [context cache](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) of the codebase so we don't have to send the entire context with every request, saving processing time and cost.\n",
        "\n",
        "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`).\n",
        "\n",
        "For more information, see [Available Gemini stable model versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versioning#stable-versions-available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee211faaf336"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"You are a coding expert. Your mission is to answer all code related questions with given context and instructions.\"\n",
        "\n",
        "contents = [\n",
        "    \"\"\"\n",
        "    Context:\n",
        "    - The entire codebase is provided below.\n",
        "    - Here is directory tree of all of the files in the codebase:\n",
        "    \"\"\",\n",
        "    tree,\n",
        "    \"\"\"\n",
        "    - Then each of the files are concatenated together. You will find all of the code you need:\n",
        "    \"\"\",\n",
        "    content,\n",
        "]\n",
        "\n",
        "cached_content = client.caches.create(\n",
        "    model=MODEL_ID,\n",
        "    config=CreateCachedContentConfig(\n",
        "        contents=contents,\n",
        "        system_instruction=system_instruction,\n",
        "        ttl=\"3600s\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OtaszvJt9L"
      },
      "source": [
        "### 1. Summarizing the codebase\n",
        "\n",
        "Generate a summary of the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMexx1Qtf1ML"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Give me a summary of this codebase, and tell me the top 3 things that I can learn from it.\n",
        "\"\"\"\n",
        "\n",
        "# Generate text using non-streaming method\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    # Use the cached content\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Print generated text and usage metadata\n",
        "display(Markdown(response.text))\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCshJHPCYoxI"
      },
      "source": [
        "### 2. Creating a developer getting started guide\n",
        "\n",
        "Generate a getting started guide for developers. This sample uses the streaming option to generate the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Kns7vCYm1P"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Provide a getting started guide to onboard new developers to the codebase.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Generate text using streaming method\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXurINu-jelb"
      },
      "source": [
        "### 3. Finding bugs\n",
        "\n",
        "Find the top 3 most severe issues in the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy3AWPRgNhu_"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Find the top 3 most severe issues in the codebase.\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCilrR6FjmfB"
      },
      "source": [
        "### 4. Fixing bug\n",
        "\n",
        "Find the most severe issue in the codebase that can be fixed and provide a code fix for it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwjDh0xGKE2r"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Find the most severe bug in the codebase that you can provide a code fix for.\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2pCULT_xKE"
      },
      "source": [
        "### 5. Implementing a feature request using Function Calling\n",
        "\n",
        "Get the feature request text from a GitHub Issue URL.\n",
        "\n",
        "We will use [Function Calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) to extract the feature request data from the prompt, then call the GitHub API to retrieve the contents.\n",
        "\n",
        "Note: We can't use the previously created cached content, because tools cannot be added at runtime when using cached content, and the other prompts in this notebook do not need this tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMjOy0gJ1_xx"
      },
      "outputs": [],
      "source": [
        "FEATURE_REQUEST_URL = (\n",
        "    \"https://github.com/GoogleCloudPlatform/microservices-demo/issues/2205\"\n",
        ")\n",
        "\n",
        "question = f\"What is the feature request of the following {FEATURE_REQUEST_URL}\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    # Use the function as a tool\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_github_issue],\n",
        "    ),\n",
        ")\n",
        "\n",
        "issue_description = response.text\n",
        "display(Markdown(f\"# Feature Request\\n{issue_description}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19455545f4c9"
      },
      "source": [
        "Use the GitHub Issue text to implement the feature request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78e6df259be8"
      },
      "outputs": [],
      "source": [
        "# Combine feature request content and cached code content\n",
        "question = f\"\"\"Implement the following feature request\n",
        "{issue_description}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Generate code response\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOk_Qe35b_cJ"
      },
      "source": [
        "### 6. Creating a troubleshooting guide\n",
        "\n",
        "Create a troubleshooting guide to help resolve common issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKn85LS-v0iw"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "    Provide a troubleshooting guide to help resolve common issues.\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h23z0sTsj5pL"
      },
      "source": [
        "### 7. Making the app more reliable\n",
        "\n",
        "Recommend best practices to make the application more reliable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOBSulTPLUAo"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  How can I make this application more reliable? Consider best practices from https://www.r9y.dev/\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf1jNDpJj8u0"
      },
      "source": [
        "### 8. Making the app more secure\n",
        "\n",
        "Recommend best practices to make the application more secure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_mCyFVLlXU"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  How can you secure the application?\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFfwMOb6kYfw"
      },
      "source": [
        "### 9. Learning the codebase\n",
        "\n",
        "Create a quiz about the concepts used in the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7jQIUwsNRH4"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Create a quiz about the concepts used in the codebase to help me solidify my understanding.\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjo1UrZwGLan"
      },
      "source": [
        "### 10. Creating a quickstart tutorial\n",
        "\n",
        "Create an end-to-end quickstart tutorial for a specific component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRwmRyDDFRMB"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Please write an end-to-end quickstart tutorial that introduces AlloyDB,\n",
        "  shows how to configure it with the CartService,\n",
        "  and highlights key capabilities of AlloyDB in context of the Online Boutique application.\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAJ-kBCZnlH_"
      },
      "source": [
        "### 11. Creating a Git Changelog Generator\n",
        "\n",
        "Understanding changes made between Git commits and highlighting the most important aspects of the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdeWO8crnlH_"
      },
      "outputs": [],
      "source": [
        "diff_text = get_git_diff(repo_dir)\n",
        "question = f\"\"\"\n",
        "    Given the below git diff output, Summarize the important changes made.\n",
        "```diff\n",
        "{diff_text}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "responses = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=question,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kUeIBfGyoX7"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, you've learned how to use Gemini to analyze a codebase and prompt the model to:\n",
        "\n",
        "- Summarize codebases effortlessly.\n",
        "- Generate clear developer getting-started documentation.\n",
        "- Uncover critical bugs and provide fixes.\n",
        "- Implement new features and improve reliability and security.\n",
        "- Understanding changes made between Git commits"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "analyze_codebase.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
