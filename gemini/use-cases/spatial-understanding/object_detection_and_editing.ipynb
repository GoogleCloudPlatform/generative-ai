{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Detecting and Editing Visual Objects with Gemini\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fspatial-understanding%2Fobject_detection_and_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<p>\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author                                           |\n",
        "| ------------------------------------------------ |\n",
        "| [Laurent Picard](https://github.com/PicardParis) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ú® Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Traditional vision machine learning (ML) models are typically trained to detect a fixed set of object classes, like \"person\", \"cat\", or \"car\". If you want to detect something specific that wasn't in the training set, such as an \"illustration\" in a book photograph, you usually have to gather a dataset, label it manually, and train a custom model, which can take hours or even days.\n",
        "\n",
        "In this exploration, we'll test a different approach using Gemini. We will leverage its spatial understanding capabilities to perform open-vocabulary object detection. This allows us to find objects based solely on a natural language description, without any training.\n",
        "\n",
        "Once detected, we'll use Gemini's image editing capabilities (specifically the Nano Banana models) to extract, restore, and creatively transform these visual objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üî• Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are dealing with unstructured data: photos of books, magazines, and objects in the wild. These images present several difficulties for traditional computer vision:\n",
        "\n",
        "- Variety: The objects we want to find (illustrations, engravings, and any visuals in general) vary wildly in style and content.\n",
        "- Distortion: Pages are curved, photos are taken at angles, and lighting is uneven.\n",
        "- Noise: Old books have stains, paper grain, and text bleeding through from the other side.\n",
        "\n",
        "Our challenge is to build a robust pipeline that can detect these objects despite the distortions, extract them cleanly, and edit them to look like high-quality digital assets‚Ä¶ all using simple text prompts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÅ Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üêç Python packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the following packages:\n",
        "\n",
        "- `google-genai`: the [Google Gen AI Python SDK](https://pypi.org/project/google-genai) lets us call Gemini with a few lines of code\n",
        "- `pillow` for image management\n",
        "- `matplotlib` for result visualization\n",
        "\n",
        "We'll also use these packages (dependencies of `google-genai`):\n",
        "\n",
        "- `pydantic` for data management\n",
        "- `tenacity` for request management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet \"google-genai>=1.61.0\" \"pillow>=11.3.0\" \"matplotlib>=3.10.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîó Gemini API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use the Gemini API, we have two main options:\n",
        "\n",
        "1. Via **Vertex AI** with a Google Cloud project\n",
        "2. Via **Google AI Studio** with a Gemini API key\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these APIs, and we can use environment variables for the configuration.\n",
        "\n",
        "**üõ†Ô∏è Option 1 - Gemini API via Vertex AI**\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- A Google Cloud project\n",
        "- The Vertex AI API must be enabled for this project: ‚ñ∂Ô∏è [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,storage-component.googleapis.com)\n",
        "\n",
        "Gen AI SDK environment variables:\n",
        "\n",
        "- `GOOGLE_GENAI_USE_VERTEXAI=\"True\"`\n",
        "- `GOOGLE_CLOUD_PROJECT=\"<PROJECT_ID>\"`\n",
        "- `GOOGLE_CLOUD_LOCATION=\"<LOCATION>\"`\n",
        "\n",
        "> üí° For preview models, the location must be set to `global`. For generally available models, we can choose the closest location among the [Google model endpoint locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#google_model_endpoint_locations).\n",
        "\n",
        "> ‚ÑπÔ∏è Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "**üõ†Ô∏è Option 2 - Gemini API via Google AI Studio**\n",
        "\n",
        "Requirement:\n",
        "\n",
        "- A Gemini API key\n",
        "\n",
        "Gen AI SDK environment variables:\n",
        "\n",
        "- `GOOGLE_GENAI_USE_VERTEXAI=\"False\"`\n",
        "- `GOOGLE_API_KEY=\"<API_KEY>\"`\n",
        "\n",
        "> ‚ÑπÔ∏è Learn more about [getting a Gemini API key from Google AI Studio](https://aistudio.google.com/app/apikey).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° You can store your environment configuration outside of the source code:\n",
        "\n",
        "| Environment         | Method                                                      |\n",
        "| ------------------- | ----------------------------------------------------------- |\n",
        "| IDE                 | `.env` file (or equivalent)                                 |\n",
        "| Colab               | Colab Secrets (üóùÔ∏è icon in left panel, see code below)       |\n",
        "| Colab Enterprise    | Google Cloud project and location are automatically defined |\n",
        "| Vertex AI Workbench | Google Cloud project and location are automatically defined |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the following environment detection functions. You can also define your configuration manually if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "import os\n",
        "import sys\n",
        "from collections.abc import Callable\n",
        "\n",
        "from google import genai\n",
        "\n",
        "# Manual setup (leave unchanged if setup is environment-defined)\n",
        "\n",
        "# @markdown **Which API: Vertex AI or Google AI Studio?**\n",
        "GOOGLE_GENAI_USE_VERTEXAI = True  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown **Option A - Google Cloud project [+location]**\n",
        "GOOGLE_CLOUD_PROJECT = \"\"  # @param {type: \"string\"}\n",
        "GOOGLE_CLOUD_LOCATION = \"global\"  # @param {type: \"string\"}\n",
        "\n",
        "# @markdown **Option B - Google AI Studio API key**\n",
        "GOOGLE_API_KEY = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "def check_environment() -> bool:\n",
        "    check_colab_user_authentication()\n",
        "    return check_manual_setup() or check_vertex_ai() or check_colab() or check_local()\n",
        "\n",
        "\n",
        "def check_manual_setup() -> bool:\n",
        "    return check_define_env_vars(\n",
        "        GOOGLE_GENAI_USE_VERTEXAI,\n",
        "        GOOGLE_CLOUD_PROJECT.strip(),  # Might have been pasted with line return\n",
        "        GOOGLE_CLOUD_LOCATION,\n",
        "        GOOGLE_API_KEY,\n",
        "    )\n",
        "\n",
        "\n",
        "def check_vertex_ai() -> bool:\n",
        "    # Workbench and Colab Enterprise\n",
        "    match os.getenv(\"VERTEX_PRODUCT\", \"\"):\n",
        "        case \"WORKBENCH_INSTANCE\":\n",
        "            pass\n",
        "        case \"COLAB_ENTERPRISE\":\n",
        "            if not running_in_colab_env():\n",
        "                return False\n",
        "        case _:\n",
        "            return False\n",
        "\n",
        "    return check_define_env_vars(\n",
        "        True,\n",
        "        os.getenv(\"GOOGLE_CLOUD_PROJECT\", \"\"),\n",
        "        os.getenv(\"GOOGLE_CLOUD_REGION\", \"\"),\n",
        "        \"\",\n",
        "    )\n",
        "\n",
        "\n",
        "def check_colab() -> bool:\n",
        "    if not running_in_colab_env():\n",
        "        return False\n",
        "\n",
        "    # Colab Enterprise was checked before, so this is Colab only\n",
        "    from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "    colab_auth.authenticate_user()\n",
        "\n",
        "    # Use Colab Secrets (üóùÔ∏è icon in left panel) to store the environment variables\n",
        "    # Secrets are private, visible only to you and the notebooks that you select\n",
        "    # - Vertex AI: Store your settings as secrets\n",
        "    # - Google AI: Directly import your Gemini API key from the UI\n",
        "    vertexai, project, location, api_key = get_vars(get_colab_secret)\n",
        "\n",
        "    return check_define_env_vars(vertexai, project, location, api_key)\n",
        "\n",
        "\n",
        "def check_local() -> bool:\n",
        "    vertexai, project, location, api_key = get_vars(os.getenv)\n",
        "\n",
        "    return check_define_env_vars(vertexai, project, location, api_key)\n",
        "\n",
        "\n",
        "def running_in_colab_env() -> bool:\n",
        "    # Colab or Colab Enterprise\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "\n",
        "def check_colab_user_authentication() -> None:\n",
        "    if running_in_colab_env():\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "        colab_auth.authenticate_user()\n",
        "\n",
        "\n",
        "def get_colab_secret(secret_name: str, default: str) -> str:\n",
        "    from google.colab import errors, userdata  # type: ignore\n",
        "\n",
        "    try:\n",
        "        return userdata.get(secret_name)\n",
        "    except errors.SecretNotFoundError:\n",
        "        return default\n",
        "\n",
        "\n",
        "def disable_colab_cell_scrollbar() -> None:\n",
        "    if running_in_colab_env():\n",
        "        from google.colab import output  # type: ignore\n",
        "\n",
        "        output.no_vertical_scroll()\n",
        "\n",
        "\n",
        "def get_vars(getenv: Callable[[str, str], str]) -> tuple[bool, str, str, str]:\n",
        "    # Limit getenv calls to the minimum (may trigger UI confirmation for secret access)\n",
        "    vertexai_str = getenv(\"GOOGLE_GENAI_USE_VERTEXAI\", \"\")\n",
        "    if vertexai_str:\n",
        "        vertexai = vertexai_str.lower() in [\"true\", \"1\"]\n",
        "    else:\n",
        "        vertexai = bool(getenv(\"GOOGLE_CLOUD_PROJECT\", \"\"))\n",
        "\n",
        "    project = getenv(\"GOOGLE_CLOUD_PROJECT\", \"\") if vertexai else \"\"\n",
        "    location = getenv(\"GOOGLE_CLOUD_LOCATION\", \"\") if project else \"\"\n",
        "    api_key = getenv(\"GOOGLE_API_KEY\", \"\") if not project else \"\"\n",
        "\n",
        "    return vertexai, project, location, api_key\n",
        "\n",
        "\n",
        "def check_define_env_vars(\n",
        "    vertexai: bool,\n",
        "    project: str,\n",
        "    location: str,\n",
        "    api_key: str,\n",
        ") -> bool:\n",
        "    match (vertexai, bool(project), bool(location), bool(api_key)):\n",
        "        case (True, True, _, _):\n",
        "            # Vertex AI - Google Cloud project [+location]\n",
        "            location = location or \"global\"\n",
        "            define_env_vars(vertexai, project, location, \"\")\n",
        "        case (True, False, _, True):\n",
        "            # Vertex AI - API key\n",
        "            define_env_vars(vertexai, \"\", \"\", api_key)\n",
        "        case (False, _, _, True):\n",
        "            # Google AI Studio - API key\n",
        "            define_env_vars(vertexai, \"\", \"\", api_key)\n",
        "        case _:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def define_env_vars(vertexai: bool, project: str, location: str, api_key: str) -> None:\n",
        "    os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = str(vertexai)\n",
        "    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project\n",
        "    os.environ[\"GOOGLE_CLOUD_LOCATION\"] = location\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "def check_configuration(client: genai.Client) -> None:\n",
        "    service = \"Vertex AI\" if client.vertexai else \"Google AI Studio\"\n",
        "    print(f\"‚úÖ Using the {service} API\", end=\"\")\n",
        "\n",
        "    if client._api_client.project:\n",
        "        print(f' with project \"{client._api_client.project[:7]}‚Ä¶\"', end=\"\")\n",
        "        print(f' in location \"{client._api_client.location}\"')\n",
        "    elif client._api_client.api_key:\n",
        "        api_key = client._api_client.api_key\n",
        "        print(f' with API key \"{api_key[:5]}‚Ä¶{api_key[-5:]}\"', end=\"\")\n",
        "        print(f\" (in case of error, make sure it was created for {service})\")\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Environment functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§ñ Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To send Gemini requests, create a `google.genai` client:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "check_environment()\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "check_configuration(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üñºÔ∏è Image test suite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a list of images for our tests:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from enum import StrEnum\n",
        "from typing import TypeAlias\n",
        "\n",
        "Url: TypeAlias = str\n",
        "\n",
        "\n",
        "class Source(StrEnum):\n",
        "    incunable = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2014:2014rosen0487:0165/full/pct:25/0/default.jpg\"\n",
        "    engravings = \"https://tile.loc.gov/image-services/iiif/service:gdc:gdcscd:00:34:07:66:92:1:00340766921:0121/full/pct:50/0/default.jpg\"\n",
        "    museum_guidebook = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2014:2014gen34181:0033/full/pct:75/0/default.jpg\"\n",
        "    denver_illustrated = \"https://tile.loc.gov/image-services/iiif/service:gdc:gdclccn:rc:01:00:04:94:rc01000494:0051/full/pct:50/0/default.jpg\"\n",
        "    physics_textbook = \"https://tile.loc.gov/image-services/iiif/service:gdc:gdcscd:00:03:64:87:31:8:00036487318:0103/full/pct:50/0/default.jpg\"\n",
        "    portrait_miniatures = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2024:2024rosen013592v02:0249/full/pct:50/0/default.jpg\"\n",
        "    wizard_of_oz_drawings = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2006:2006gen32405:0048/full/pct:25/0/default.jpg\"\n",
        "    paintings = \"https://images.unsplash.com/photo-1714146681164-f26fed839692?w=1920\"\n",
        "    alice_in_wonderland_drawing = (\n",
        "        \"https://images.unsplash.com/photo-1630595011903-689853b04ee2?w=1024\"\n",
        "    )\n",
        "    electronic_board = \"https://images.unsplash.com/photo-1751887687766-c18728bc238a\"\n",
        "    book = \"https://images.unsplash.com/photo-1643451533573-ee364ba6e330?w=1024\"\n",
        "    manual = \"https://images.unsplash.com/photo-1623666936367-a100f62ba9b7?w=1024\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SourceMetadata:\n",
        "    title: str\n",
        "    webpage_url: Url\n",
        "    credit_line: str\n",
        "\n",
        "\n",
        "LOC = \"Library of Congress\"\n",
        "LOC_RARE_BOOKS = \"Library of Congress, Rare Book and Special Collections Division\"\n",
        "LOC_MEETING_FRONTIERS = \"Library of Congress, Meeting of Frontiers\"\n",
        "\n",
        "metadata_by_source: dict[Source, SourceMetadata] = {\n",
        "    Source.incunable: SourceMetadata(\n",
        "        \"Vergaderinge der historien van Troy (1485)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2014rosen0487/?sp=165\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.engravings: SourceMetadata(\n",
        "        \"Harper's illustrated catalogue (1847)\",\n",
        "        \"https://www.loc.gov/resource/gdcscd.00340766921/?sp=121\",\n",
        "        LOC,\n",
        "    ),\n",
        "    Source.museum_guidebook: SourceMetadata(\n",
        "        \"Barnum's American Museum illustrated (1850)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2014gen34181/?sp=33\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.denver_illustrated: SourceMetadata(\n",
        "        \"Denver illustrated (1893)\",\n",
        "        \"https://www.loc.gov/resource/gdclccn.rc01000494/?sp=51\",\n",
        "        LOC_MEETING_FRONTIERS,\n",
        "    ),\n",
        "    Source.physics_textbook: SourceMetadata(\n",
        "        \"Lessons in physics (1916)\",\n",
        "        \"https://www.loc.gov/resource/gdcscd.00036487318/?sp=103\",\n",
        "        LOC,\n",
        "    ),\n",
        "    Source.portrait_miniatures: SourceMetadata(\n",
        "        \"The history of portrait miniatures (1904)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2024rosen013592v02/?sp=249\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.wizard_of_oz_drawings: SourceMetadata(\n",
        "        \"The wonderful Wizard of Oz (1899)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2006gen32405/?sp=48\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.paintings: SourceMetadata(\n",
        "        \"Open book with reproductions of paintings\",\n",
        "        \"https://unsplash.com/photos/9hD7qrxICag\",\n",
        "        \"Photo by Trung Manh cong on Unsplash\",\n",
        "    ),\n",
        "    Source.book: SourceMetadata(\n",
        "        \"Open book sitting on top of a table\",\n",
        "        \"https://unsplash.com/photos/4IDqcNj827I\",\n",
        "        \"Photo by Ranurte on Unsplash\",\n",
        "    ),\n",
        "    Source.manual: SourceMetadata(\n",
        "        \"Instruction manual\",\n",
        "        \"https://unsplash.com/photos/aaFU96eYASk\",\n",
        "        \"Photo by Annie Spratt on Unsplash\",\n",
        "    ),\n",
        "    Source.alice_in_wonderland_drawing: SourceMetadata(\n",
        "        \"Alice's Adventures in Wonderland\",\n",
        "        \"https://unsplash.com/photos/bewzr_Q9u2o\",\n",
        "        \"Photo by Brett Jordan on Unsplash\",\n",
        "    ),\n",
        "    Source.electronic_board: SourceMetadata(\n",
        "        \"Circuit board filled with electronic components\",\n",
        "        \"https://unsplash.com/photos/5oUWerS9zYg\",\n",
        "        \"Photo by @unavailable_parts on Unsplash\",\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Test images defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß† Gemini models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gemini comes in different [versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models). We can currently use the following models:\n",
        "\n",
        "- For object detection: Gemini 2.5 or Gemini 3, each available in Flash or Pro versions.\n",
        "- For object editing: Gemini 2.5 Flash Image or Gemini 3 Pro Image, also known as Nano Banana and Nano Banana Pro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Helpers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's add core helper classes and functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "from enum import Enum, auto\n",
        "from pathlib import Path\n",
        "from typing import Any, cast\n",
        "\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import pydantic\n",
        "import tenacity\n",
        "from google.genai.errors import ClientError\n",
        "from google.genai.types import (\n",
        "    FinishReason,\n",
        "    GenerateContentConfig,\n",
        "    GenerateContentResponse,\n",
        "    PIL_Image,\n",
        "    ThinkingConfig,\n",
        "    ThinkingLevel,\n",
        ")\n",
        "\n",
        "\n",
        "class Model(Enum):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Multimodal models with spatial understanding and structured outputs\n",
        "class MultimodalModel(Model):\n",
        "    # Generally Available (GA)\n",
        "    GEMINI_2_5_FLASH = \"gemini-2.5-flash\"\n",
        "    GEMINI_2_5_PRO = \"gemini-2.5-pro\"\n",
        "    # Preview\n",
        "    GEMINI_3_FLASH_PREVIEW = \"gemini-3-flash-preview\"\n",
        "    GEMINI_3_PRO_PREVIEW = \"gemini-3-pro-preview\"\n",
        "    # Default model used for object detection\n",
        "    DEFAULT = GEMINI_3_FLASH_PREVIEW\n",
        "\n",
        "\n",
        "# Image generation and editing models\n",
        "class ImageModel(Model):\n",
        "    # Generally Available (GA)\n",
        "    GEMINI_2_5_FLASH_IMAGE = \"gemini-2.5-flash-image\"  # Nano Banana üçå\n",
        "    # Preview\n",
        "    GEMINI_3_PRO_IMAGE_PREVIEW = \"gemini-3-pro-image-preview\"  # Nano Banana Pro üçå\n",
        "    # Default model used for image editing\n",
        "    DEFAULT = GEMINI_2_5_FLASH_IMAGE\n",
        "\n",
        "\n",
        "def generate_content(\n",
        "    contents: list[Any],\n",
        "    model: Model,\n",
        "    config: GenerateContentConfig | None,\n",
        "    should_display_response_info: bool = False,\n",
        ") -> GenerateContentResponse | None:\n",
        "    response = None\n",
        "    client = check_client_for_model(model)\n",
        "\n",
        "    for attempt in get_retrier():\n",
        "        with attempt:\n",
        "            response = client.models.generate_content(\n",
        "                model=model.value,\n",
        "                contents=contents,\n",
        "                config=config,\n",
        "            )\n",
        "    if should_display_response_info:\n",
        "        display_response_info(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def check_client_for_model(model: Model) -> genai.Client:\n",
        "    if (\n",
        "        model.value.endswith(\"-preview\")\n",
        "        and client.vertexai\n",
        "        and client._api_client.location != \"global\"\n",
        "    ):\n",
        "        # Preview models are only available on the \"global\" location\n",
        "        return genai.Client(location=\"global\")\n",
        "\n",
        "    return client\n",
        "\n",
        "\n",
        "def display_response_info(response: GenerateContentResponse | None) -> None:\n",
        "    if response is None:\n",
        "        print(\"‚ùå No response\")\n",
        "        return\n",
        "\n",
        "    if usage_metadata := response.usage_metadata:\n",
        "        if usage_metadata.prompt_token_count:\n",
        "            print(f\"Input tokens   : {usage_metadata.prompt_token_count:9,d}\")\n",
        "        if usage_metadata.candidates_token_count:\n",
        "            print(f\"Output tokens  : {usage_metadata.candidates_token_count:9,d}\")\n",
        "        if usage_metadata.thoughts_token_count:\n",
        "            print(f\"Thoughts tokens: {usage_metadata.thoughts_token_count:9,d}\")\n",
        "\n",
        "    if response.parsed is None:\n",
        "        print(\"‚ùå Could not parse the JSON response\")\n",
        "        return\n",
        "    if not response.candidates:\n",
        "        print(\"‚ùå No `response.candidates`\")\n",
        "        return\n",
        "    if (finish_reason := response.candidates[0].finish_reason) != FinishReason.STOP:\n",
        "        print(f\"‚ùå {finish_reason = }\")\n",
        "    if not response.text:\n",
        "        print(\"‚ùå No `response.text`\")\n",
        "        return\n",
        "\n",
        "\n",
        "def generate_image(\n",
        "    sources: list[PIL_Image],\n",
        "    prompt: str,\n",
        "    model: ImageModel,\n",
        "    config: GenerateContentConfig | None = None,\n",
        ") -> PIL_Image | None:\n",
        "    contents = [*sources, prompt.strip()]\n",
        "    response = generate_content(contents, model, config)\n",
        "\n",
        "    return check_get_output_image_from_response(response)\n",
        "\n",
        "\n",
        "def check_get_output_image_from_response(\n",
        "    response: GenerateContentResponse | None,\n",
        ") -> PIL_Image | None:\n",
        "    if response is None:\n",
        "        print(\"‚ùå No `response`\")\n",
        "        return None\n",
        "    if not response.candidates:\n",
        "        print(\"‚ùå No `response.candidates`\")\n",
        "        if response.prompt_feedback:\n",
        "            if block_reason := response.prompt_feedback.block_reason:\n",
        "                print(f\"{block_reason = :s}\")\n",
        "            if block_reason_message := response.prompt_feedback.block_reason_message:\n",
        "                print(f\"{block_reason_message = }\")\n",
        "        return None\n",
        "    if not (content := response.candidates[0].content):\n",
        "        print(\"‚ùå No `response.candidates[0].content`\")\n",
        "        return None\n",
        "    if not (parts := content.parts):\n",
        "        print(\"‚ùå No `response.candidates[0].content.parts`\")\n",
        "        return None\n",
        "\n",
        "    output_image: PIL_Image | None = None\n",
        "    for part in parts:\n",
        "        if part.text:\n",
        "            display_markdown(part.text)\n",
        "            continue\n",
        "        assert (sdk_image := part.as_image())\n",
        "        assert (output_image := sdk_image._pil_image)\n",
        "\n",
        "    return output_image\n",
        "\n",
        "\n",
        "def get_thinking_config(model: Model) -> ThinkingConfig | None:\n",
        "    match model:\n",
        "        case MultimodalModel.GEMINI_2_5_FLASH:\n",
        "            return ThinkingConfig(thinking_budget=0)\n",
        "        case MultimodalModel.GEMINI_2_5_PRO:\n",
        "            return ThinkingConfig(thinking_budget=128, include_thoughts=False)\n",
        "        case MultimodalModel.GEMINI_3_FLASH_PREVIEW:\n",
        "            return ThinkingConfig(thinking_level=ThinkingLevel.MINIMAL)\n",
        "        case MultimodalModel.GEMINI_3_PRO_PREVIEW:\n",
        "            return ThinkingConfig(thinking_level=ThinkingLevel.LOW)\n",
        "        case _:\n",
        "            return None  # Default\n",
        "\n",
        "\n",
        "def display_markdown(markdown: str) -> None:\n",
        "    IPython.display.display(IPython.display.Markdown(markdown))\n",
        "\n",
        "\n",
        "def display_image(image: PIL_Image) -> None:\n",
        "    IPython.display.display(image)\n",
        "\n",
        "\n",
        "def get_retrier() -> tenacity.Retrying:\n",
        "    return tenacity.Retrying(\n",
        "        stop=tenacity.stop_after_attempt(7),\n",
        "        wait=tenacity.wait_incrementing(start=10, increment=1),\n",
        "        retry=should_retry_request,\n",
        "        reraise=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def should_retry_request(retry_state: tenacity.RetryCallState) -> bool:\n",
        "    if not retry_state.outcome:\n",
        "        return False\n",
        "    err = retry_state.outcome.exception()\n",
        "    if not isinstance(err, ClientError):\n",
        "        return False\n",
        "    print(f\"‚ùå ClientError {err.code}: {err.message}\")\n",
        "\n",
        "    retry = False\n",
        "    match err.code:\n",
        "        case 400 if err.message is not None and \" try again \" in err.message:\n",
        "            # Workshop: first time access to Cloud Storage (service agent provisioning)\n",
        "            retry = True\n",
        "        case 429:\n",
        "            # Workshop: temporary project with 1 QPM quota\n",
        "            retry = True\n",
        "    print(f\"üîÑ Retry: {retry}\")\n",
        "\n",
        "    return retry\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Helpers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Detecting visual objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform visual object detection, craft the prompt to indicate what you'd like to detect and how results should be returned. In the same request, it's possible to also extract additional information about each detected object. This can be virtually anything, from labels such as \"furniture\", \"table\", or \"chair\", to more precise classifications like \"mammals\" or \"reptiles\", or to contextual data such as captions, colors, shapes, etc.\n",
        "\n",
        "For the next tests, we'll experiment with detecting illustrations within book photos. Here's a possible prompt:\n",
        "\n",
        "```python\n",
        "OBJECT_DETECTION_PROMPT = \"\"\"\n",
        "Detect every illustration within the book photo and extract the following data for each:\n",
        "- `box_2d`: Bounding box coordinates of the illustration only (ignoring any caption).\n",
        "- `caption`: Verbatim caption or legend such as \"Figure 1\". Use \"\" if not found.\n",
        "- `label`: Single-word label describing the illustration. Use \"\" if not found.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Bounding boxes are very useful to be able to locate or extract the detected objects.\n",
        "- Typically, for Gemini models, a `box_2d` bounding box represents coordinates normalized to a `(0, 0, 1000, 1000)` space, for a `(0, 0, width, height)` input image.\n",
        "- We're also requesting to extract captions (metadata often present in reference books) and labels (dynamic metadata).\n",
        "\n",
        "To automate response processing, it's convenient to define a Pydantic class that matches the prompt, such as:\n",
        "\n",
        "```python\n",
        "class DetectedObject(pydantic.BaseModel):\n",
        "    box_2d: list[int]\n",
        "    caption: str\n",
        "    label: str\n",
        "\n",
        "DetectedObjects: TypeAlias = list[DetectedObject]\n",
        "```\n",
        "\n",
        "Then, request a structured output with config fields `response_mime_type` and `response_schema`:\n",
        "\n",
        "```python\n",
        "config = GenerateContentConfig(\n",
        "    # ‚Ä¶,\n",
        "    response_mime_type=\"application/json\",\n",
        "    response_schema=DetectedObjects,\n",
        "    # ‚Ä¶,\n",
        ")\n",
        "```\n",
        "\n",
        "This will generate a JSON response which the SDK can parse automatically, letting us directly use object instances:\n",
        "\n",
        "```python\n",
        "detected_objects = cast(DetectedObjects, response.parsed)\n",
        "```\n",
        "\n",
        "Let's add a few object-detection-specific classes and functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import urllib.request\n",
        "from dataclasses import field\n",
        "\n",
        "import PIL.Image\n",
        "from google.genai.types import Part, PartMediaResolutionLevel\n",
        "\n",
        "OBJECT_DETECTION_PROMPT = \"\"\"\n",
        "Detect every illustration within the book photo and extract the following data for each:\n",
        "- `box_2d`: Bounding box coordinates of the illustration only (ignoring any caption).\n",
        "- `caption`: Verbatim caption or legend such as \"Figure 1\". Use \"\" if not found.\n",
        "- `label`: Single-word label describing the illustration. Use \"\" if not found.\n",
        "\"\"\"\n",
        "\n",
        "# Margin added to detected/cropped objects, giving more context for a better understanding of spatial distortions\n",
        "CROP_MARGIN_PX = 10\n",
        "\n",
        "\n",
        "# Matching class for structured output generation\n",
        "class DetectedObject(pydantic.BaseModel):\n",
        "    box_2d: list[int]\n",
        "    caption: str\n",
        "    label: str\n",
        "\n",
        "\n",
        "# Misc data classes\n",
        "InputImage: TypeAlias = Path | Url\n",
        "DetectedObjects: TypeAlias = list[DetectedObject]\n",
        "WorkflowStepImages: TypeAlias = list[PIL_Image]\n",
        "\n",
        "\n",
        "class WorkflowStep(StrEnum):\n",
        "    CROPPED = auto()\n",
        "    RESTORED = auto()\n",
        "    COLORIZED = auto()\n",
        "    CINEMATIZED = auto()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VisualObjectWorkflow:\n",
        "    source_image: PIL_Image\n",
        "    detected_objects: DetectedObjects\n",
        "    images_by_step: dict[WorkflowStep, WorkflowStepImages] = field(default_factory=dict)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        denormalize_bounding_boxes(self)\n",
        "\n",
        "\n",
        "workflow_by_image: dict[InputImage, VisualObjectWorkflow] = dict()\n",
        "\n",
        "\n",
        "def denormalize_bounding_boxes(self: VisualObjectWorkflow) -> None:\n",
        "    \"\"\"Convert the box_2d coordinates.\n",
        "    - Before: [y1, x1, y2, x2] normalized to 0-1000, as returned by Gemini\n",
        "    - After:  [x1, y1, x2, y2] in source_image coordinates, as used in Pillow\n",
        "    \"\"\"\n",
        "\n",
        "    def to_image_coord(coord: int, dim: int) -> int:\n",
        "        return int(coord * dim / 1000 + 0.5)\n",
        "\n",
        "    w, h = self.source_image.size\n",
        "    for obj in self.detected_objects:\n",
        "        y1, x1, y2, x2 = obj.box_2d\n",
        "        x1, x2 = to_image_coord(x1, w), to_image_coord(x2, w)\n",
        "        y1, y2 = to_image_coord(y1, h), to_image_coord(y2, h)\n",
        "        obj.box_2d = [x1, y1, x2, y2]\n",
        "\n",
        "\n",
        "def detect_objects(\n",
        "    image: InputImage,\n",
        "    prompt: str = OBJECT_DETECTION_PROMPT,\n",
        "    model: MultimodalModel = MultimodalModel.DEFAULT,\n",
        "    config: GenerateContentConfig | None = None,\n",
        "    media_resolution: PartMediaResolutionLevel | None = None,\n",
        "    display_results: bool = True,\n",
        ") -> None:\n",
        "    pil_image, content_part = get_pil_image_and_part(image, model, media_resolution)\n",
        "    contents = [content_part, prompt.strip()]\n",
        "    config = config or get_object_detection_config(model)\n",
        "\n",
        "    response = generate_content(contents, model, config)\n",
        "\n",
        "    detected_objects = DetectedObjects()\n",
        "    if response is not None and response.parsed is not None:\n",
        "        detected_objects = cast(DetectedObjects, response.parsed)\n",
        "\n",
        "    workflow = VisualObjectWorkflow(pil_image, detected_objects)\n",
        "    workflow_by_image[image] = workflow\n",
        "    add_cropped_objects(workflow)\n",
        "\n",
        "    if display_results:\n",
        "        display_detected_objects(workflow)\n",
        "\n",
        "\n",
        "def get_pil_image_and_part(\n",
        "    image: InputImage,\n",
        "    model: MultimodalModel,\n",
        "    media_resolution: PartMediaResolutionLevel | None,\n",
        ") -> tuple[PIL_Image, Part]:\n",
        "    if isinstance(image, Path):\n",
        "        image_bytes = image.read_bytes()\n",
        "    else:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        req = urllib.request.Request(image, headers=headers)\n",
        "        with urllib.request.urlopen(req, timeout=10) as response:\n",
        "            image_bytes = response.read()\n",
        "\n",
        "    pil_image = PIL.Image.open(io.BytesIO(image_bytes))\n",
        "    content_part = Part.from_bytes(\n",
        "        data=image_bytes,\n",
        "        mime_type=\"image/*\",\n",
        "        media_resolution=media_resolution,\n",
        "    )\n",
        "\n",
        "    return pil_image, content_part\n",
        "\n",
        "\n",
        "def get_object_detection_config(model: Model) -> GenerateContentConfig:\n",
        "    # Low randomness for more determinism\n",
        "    return GenerateContentConfig(\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        seed=42,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=DetectedObjects,\n",
        "        thinking_config=get_thinking_config(model),\n",
        "    )\n",
        "\n",
        "\n",
        "def add_cropped_objects(\n",
        "    workflow: VisualObjectWorkflow,\n",
        "    crop_margin: int = CROP_MARGIN_PX,\n",
        ") -> None:\n",
        "    cropped_images: list[PIL_Image] = []\n",
        "    for obj in workflow.detected_objects:\n",
        "        cropped_image, _ = extract_object_image(workflow.source_image, obj, crop_margin)\n",
        "        cropped_images.append(cropped_image)\n",
        "    workflow.images_by_step[WorkflowStep.CROPPED] = cropped_images\n",
        "\n",
        "\n",
        "def extract_object_image(\n",
        "    image: PIL_Image,\n",
        "    obj: DetectedObject,\n",
        "    margin: int = 0,\n",
        ") -> tuple[PIL_Image, tuple[int, int, int, int]]:\n",
        "    def clamp(coord: int, dim: int) -> int:\n",
        "        return min(max(coord, 0), dim)\n",
        "\n",
        "    x1, y1, x2, y2 = obj.box_2d\n",
        "    w, h = image.size\n",
        "    if margin != 0:\n",
        "        x1, x2 = clamp(x1 - margin, w), clamp(x2 + margin, w)\n",
        "        y1, y2 = clamp(y1 - margin, h), clamp(y2 + margin, h)\n",
        "\n",
        "    box = (x1, y1, x2, y2)\n",
        "    object_image = image.crop(box)\n",
        "\n",
        "    return object_image, box\n",
        "\n",
        "\n",
        "# Matplotlib\n",
        "FIGURE_FG_COLOR = \"#F1F3F4\"\n",
        "FIGURE_BG_COLOR = \"#202124\"\n",
        "EDGE_COLOR = \"#80868B\"\n",
        "rcParams = {\n",
        "    \"figure.dpi\": 300,\n",
        "    \"text.color\": FIGURE_FG_COLOR,\n",
        "    \"figure.edgecolor\": FIGURE_FG_COLOR,\n",
        "    \"axes.titlecolor\": FIGURE_FG_COLOR,\n",
        "    \"axes.edgecolor\": FIGURE_FG_COLOR,\n",
        "    \"xtick.color\": FIGURE_FG_COLOR,\n",
        "    \"ytick.color\": FIGURE_FG_COLOR,\n",
        "    \"figure.facecolor\": FIGURE_BG_COLOR,\n",
        "    \"axes.edgecolor\": EDGE_COLOR,\n",
        "    \"xtick.bottom\": False,\n",
        "    \"xtick.top\": False,\n",
        "    \"ytick.left\": False,\n",
        "    \"ytick.right\": False,\n",
        "    \"xtick.labelbottom\": False,\n",
        "    \"ytick.labelleft\": False,\n",
        "}\n",
        "plt.rcParams.update(rcParams)\n",
        "\n",
        "\n",
        "def display_detected_objects(workflow: VisualObjectWorkflow) -> None:\n",
        "    source_image = workflow.source_image\n",
        "    detected_objects = PIL.Image.new(\"RGB\", source_image.size, \"white\")\n",
        "    for obj in workflow.detected_objects:\n",
        "        obj_image, box = extract_object_image(source_image, obj)\n",
        "        detected_objects.paste(obj_image, (box[0], box[1]))\n",
        "\n",
        "    fig = plt.figure(layout=\"compressed\")\n",
        "    horizontal = True\n",
        "    rows, cols = (1, 2) if horizontal else (2, 1)\n",
        "    gs = fig.add_gridspec(rows, cols)\n",
        "    ax = fig.add_subplot(gs[0, 0])\n",
        "    ax.imshow(source_image)\n",
        "    ax = fig.add_subplot(gs[rows - 1, cols - 1])\n",
        "    ax.imshow(detected_objects)\n",
        "\n",
        "    disable_colab_cell_scrollbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Object detection helpers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's start simple: can we detect the single illustration in this incunable from 1485?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.incunable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° This works nicely. The bounding box is very precise, enclosing the hand-colored woodcut illustration very tightly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Now, let's check the detection of the multiple visuals in this museum guidebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - The bounding boxes are again very precise.\n",
        "> - The results are perfect: there are no false positives and no false negatives.\n",
        "> - The captions below the visuals are not enclosed within the bounding boxes, which was specifically requested. The bounding box granularity can be controlled by changing the prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ What about slightly warped visuals?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.paintings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° This doesn't make a difference. Notice how the bottom-right painting is partially covered by the orange bookmark. We'll try to fix that in the restoration step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ What about the tilted visuals in this book about the architecture in Denver?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.denver_illustrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° Each visual is perfectly detected: spatial understanding covers tilted objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Finally, let's check the detection on this significantly warped book page from Alice's Adventures in Wonderland:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.alice_in_wonderland_drawing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° Page curvature and other distortions don't prevent non-rectangular objects from being detected. In fact, spatial understanding works at the pixel level, which explains this precision for warped objects. If you'd like to work at a lower level, you can also ask for a \"segmentation mask\" in the prompt and you'll get a base64-encoded PNG (each pixel giving the 0-255 probability it belongs to the object within the bounding box). See the [segmentation doc](https://ai.google.dev/gemini-api/docs/image-understanding#segmentation) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî≠ Generalizing object detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the same principles for other object types. We'll generally keep requesting bounding boxes, to identify object positions within images. Without changing our current output structure (i.e., no code change), captions and labels can be used to extract different object metadata depending on different input types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ See how we can detect electronic components with a slightly changed prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ELECTRONIC_COMPONENT_DETECTION_PROMPT = \"\"\"\n",
        "Detect every electronic component in the image and extract the following data for each:\n",
        "- `box_2d`: Bounding box coordinates.\n",
        "- `caption`: Verbatim identifier (printed on or immediately adjacent to the component such as \"R42\" or \"C123\"), or \"\" if no identifier is found.\n",
        "- `label`: Specific type of component.\n",
        "\"\"\"\n",
        "\n",
        "detect_objects(Source.electronic_board, ELECTRONIC_COMPONENT_DETECTION_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This demonstrates the versatility of the approach. Without retraining a model, we switched from detecting 15th-century visuals to identifying modern electronics just by changing the prompt. Such detections, including the caption and label metadata, could be used to auto-crop components for a parts catalog, verify assembly lines, or create interactive schematics‚Ä¶ all without a single labeled training image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü™Ñ Editing visual objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we can detect visual objects, we can envision an automation workflow to extract and reuse them. For this, we'll use Gemini 2.5 Flash Image (also known as Nano Banana üçå) by default, a state-of-the-art image generation and editing model.\n",
        "\n",
        "Our object editing functions will follow the same template, taking one step as input and generating an edited image for the output step. Let's define core helpers for this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Protocol\n",
        "\n",
        "\n",
        "class ObjectEditingFunction(Protocol):\n",
        "    def __call__(\n",
        "        self,\n",
        "        image: InputImage,\n",
        "        prompt: str | None = None,\n",
        "        model: ImageModel | None = None,\n",
        "        config: GenerateContentConfig | None = None,\n",
        "        display_results: bool = True,\n",
        "    ) -> None: ...\n",
        "\n",
        "\n",
        "SourceTargetSteps = tuple[WorkflowStep, WorkflowStep]\n",
        "registered_functions: dict[SourceTargetSteps, ObjectEditingFunction] = dict()\n",
        "\n",
        "DEFAULT_EDITING_CONFIG = GenerateContentConfig(\n",
        "    response_modalities=[\"IMAGE\"],\n",
        ")\n",
        "EMPTY_IMAGE = PIL.Image.new(\"1\", (1, 1), \"white\")\n",
        "\n",
        "\n",
        "def object_editing_function(\n",
        "    default_prompt: str,\n",
        "    source_step: WorkflowStep,\n",
        "    target_step: WorkflowStep,\n",
        "    default_model: ImageModel = ImageModel.DEFAULT,\n",
        "    default_config: GenerateContentConfig = DEFAULT_EDITING_CONFIG,\n",
        ") -> ObjectEditingFunction:\n",
        "    def editing_function(\n",
        "        image: InputImage,\n",
        "        prompt: str | None = default_prompt,\n",
        "        model: ImageModel | None = default_model,\n",
        "        config: GenerateContentConfig | None = default_config,\n",
        "        display_results: bool = True,\n",
        "    ) -> None:\n",
        "        workflow, source_images = get_workflow_and_step_images(image, source_step)\n",
        "        if prompt is None:\n",
        "            prompt = default_prompt\n",
        "        if model is None:\n",
        "            model = default_model\n",
        "        # Note: \"config is None\" is valid and will use the model endpoint default config\n",
        "\n",
        "        target_images: list[PIL_Image] = []\n",
        "        for source_image in source_images:\n",
        "            target_image = generate_image([source_image], prompt, model, config)\n",
        "            target_images.append(target_image if target_image else EMPTY_IMAGE)\n",
        "\n",
        "        workflow.images_by_step[target_step] = target_images\n",
        "        if display_results:\n",
        "            display_sources_and_targets(workflow, source_step, target_step)\n",
        "\n",
        "    registered_functions[(source_step, target_step)] = editing_function\n",
        "\n",
        "    return editing_function\n",
        "\n",
        "\n",
        "def get_workflow_and_step_images(\n",
        "    image: InputImage,\n",
        "    step: WorkflowStep,\n",
        ") -> tuple[VisualObjectWorkflow, list[PIL_Image]]:\n",
        "    # Objects detected?\n",
        "    if image not in workflow_by_image:\n",
        "        detect_objects(image, display_results=False)\n",
        "    workflow = workflow_by_image.get(image, None)\n",
        "    assert workflow is not None\n",
        "\n",
        "    # Workflow step objects? (single level, could be extended to a dynamical graph)\n",
        "    operation = (WorkflowStep.CROPPED, step)\n",
        "    if step not in workflow.images_by_step and operation in registered_functions:\n",
        "        source_function = registered_functions[operation]\n",
        "        source_function(image, display_results=False)\n",
        "\n",
        "    # Source images\n",
        "    source_images = workflow.images_by_step.get(step, None)\n",
        "    assert source_images is not None\n",
        "\n",
        "    return workflow, source_images\n",
        "\n",
        "\n",
        "def display_sources_and_targets(\n",
        "    workflow: VisualObjectWorkflow,\n",
        "    source_step: WorkflowStep,\n",
        "    target_step: WorkflowStep,\n",
        ") -> None:\n",
        "    source_images = workflow.images_by_step[source_step]\n",
        "    target_images = workflow.images_by_step[target_step]\n",
        "    assert len(source_images) == len(target_images)\n",
        "\n",
        "    fig = plt.figure(layout=\"compressed\")\n",
        "    if horizontal := (2 <= len(source_images)):\n",
        "        rows, cols = 2, len(source_images)\n",
        "    else:\n",
        "        rows, cols = len(source_images), 2\n",
        "    gs = fig.add_gridspec(rows, cols)\n",
        "    source_image = workflow.source_image\n",
        "\n",
        "    for i, (source_image, target_image) in enumerate(zip(source_images, target_images)):\n",
        "        for dim in [0, 1]:\n",
        "            image = source_image if dim == 0 else target_image\n",
        "            grid_spec = gs[dim, i] if horizontal else gs[i, dim]\n",
        "            ax = fig.add_subplot(grid_spec)\n",
        "            ax.set_axis_off()\n",
        "            ax.imshow(image)\n",
        "\n",
        "    disable_colab_cell_scrollbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Object editing helpers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's define a first editing step, to restore the detected objects that can contain many real-life artifacts‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ú® Restoring visual objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this restoration step, we need to craft a prompt that is generic enough (to cover most use cases) but also specific enough (to take into account restoration needs).\n",
        "\n",
        "An image editing prompt is based on natural language, typically using imperative or declarative instructions. With an imperative prompt, you describe the actions to perform on the input, while with a declarative prompt, you describe the expected output. Both are possible and will provide equivalent results. Your choice is really a matter of preference, as long as the prompt makes sense.\n",
        "\n",
        "Our test suite is mostly composed of book photos, which can contain various photographic and paper artifacts. The Nano Banana models understand these subtleties and can edit images accordingly, which simplifies the prompt.\n",
        "\n",
        "Here is a possible restoration function using an imperative prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESTORATION_PROMPT = \"\"\"\n",
        "- Extract the visual, excluding peripheral text and clutter.\n",
        "- Restore over a pure white background, eliminating physical artifacts while preserving original style and detail.\n",
        "- Scale to fill the canvas with minimal uniform margins, without distortion or cropping.\n",
        "\"\"\"\n",
        "\n",
        "# Default config with low randomness for more deterministic restoration outputs\n",
        "RESTORATION_CONFIG = GenerateContentConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=0.0,\n",
        "    seed=42,\n",
        "    response_modalities=[\"IMAGE\"],\n",
        ")\n",
        "\n",
        "restore_objects = object_editing_function(\n",
        "    RESTORATION_PROMPT,\n",
        "    WorkflowStep.CROPPED,\n",
        "    WorkflowStep.RESTORED,\n",
        "    default_config=RESTORATION_CONFIG,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Restoration function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's try to restore the illustration from the 1485 incunable:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "restore_objects(Source.incunable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° We now have a nice restoration of the hand-colored woodcut illustration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ What about the illustrations from the museum guidebook?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "restore_objects(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ What about the slightly warped visuals?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "restore_objects(Source.paintings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - Notice how, on the last painting, the orange bookmark gets properly removed and the hidden part outpainted to complete the painting.\n",
        "> - We requested to \"fill the canvas with minimal uniform margins, without distortion or cropping\". Depending on the aspect ratio and type of the visual, this degree of freedom can result in different white margins.\n",
        "> - This example shows famous paintings by Vincent Van Gogh. Nano Banana does not fetch any reference image and only uses the provided input. These could be photos of private paintings and they would be restored in the same way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Denver architecture book, the illustrations can be tilted, which our generic prompt does not really take into account. When several geometric transformations are involved, it can be challenging to craft an imperative prompt trying to detail all the operations to perform. Instead, a descriptive prompt can be more straightforward, by directly describing the expected output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Here's an example of a descriptive prompt focusing on the restoration of tilted visuals:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tilted_visual_prompt = \"\"\"\n",
        "An upright, high-fidelity rendition of the visual on a pure white background, filling the canvas with minimal uniform margins. The output is clean, sharp, and free of physical artifacts.\n",
        "\"\"\"\n",
        "\n",
        "restore_objects(Source.denver_illustrated, tilted_visual_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - To get these results, the prompt focuses on requesting an \"upright\" visual \"filling the canvas\", which proves more straightforward to write than trying to account for all possible geometric corrections.\n",
        "> - The native visual understanding automatically identifies the content type (photo, illustration, etc.), the different artifacts (photographic, paper, printing, scanning‚Ä¶), allowing for precise restorations out of the box.\n",
        "> - Notice how the consistency is preserved: the fourth visual is restored as an illustration, while the first visuals maintain their photographic style.\n",
        "> - The results, with this rather generic prompt, are impressive. It's of course possible to be more specific and request specific lighting, styles, colors‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this last test, the input visual not only has distortions from the page curvature but also from the photo perspective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Here's an example of a descriptive prompt focusing on restoring warped illustrations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warped_visual_prompt = \"\"\"\n",
        "A digital extraction of the illustration from the provided book photo, excluding any peripheral text. All page curvature and perspective distortions are corrected, resulting in an image framed in a perfect rectangle, on a pure white canvas with minimal uniform margins.\n",
        "\"\"\"\n",
        "\n",
        "restore_objects(Source.alice_in_wonderland_drawing, warped_visual_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° It is really impressive that such a restoration can be performed in a single step. If you have complex transformations, test descriptive prompts iteratively, using precise and concise instructions, and you might be pleasantly surprised. In the worst case, it's also possible to process the transformations in successive easier steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's add a colorization step‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Colorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our restoration step respected the original styles of the input images. Recent image editing models excel at transforming image styles, starting with colors. This can generally be performed directly with a simple, precise instruction.\n",
        "\n",
        "Here is a possible colorization function using an imperative prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLORIZATION_PROMPT = \"\"\"\n",
        "Colorize this image in a modern book illustration style, maintaining all original details without any additions.\n",
        "\"\"\"\n",
        "\n",
        "colorize = object_editing_function(\n",
        "    COLORIZATION_PROMPT,\n",
        "    WorkflowStep.RESTORED,\n",
        "    WorkflowStep.COLORIZED,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Colorization function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's modernize our 1485 illustration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colorize(Source.incunable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° All details are preserved, as requested in the prompt. Notice how the colorization can naturally fix some remaining artifacts (e.g., the paper discoloration in the sword or the bleeding ink in the armor).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's colorize our museum guidebook illustrations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colorize(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° Our prompt is very open as it only specifies \"modern book illustration style\". This can generate very creative colorizations, but they all seem to make perfect sense.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ What about our Denver buildings?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colorize(Source.denver_illustrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° As requested, they all look like modern illustrations, including the first visuals (originating from noisy photos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's possible to go further by not only \"colorizing\", but also \"transforming\" the image into a different one overall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's make our \"Alice's Adventures in Wonderland\" drawing into a watercolor painting:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "watercolor_prompt = \"\"\"\n",
        "Transform this visual into a watercolor painting.\n",
        "\"\"\"\n",
        "colorize(Source.alice_in_wonderland_drawing, watercolor_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ What about making it a traditional painting?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "painting_prompt = \"\"\"\n",
        "Transform this visual into a traditional painting.\n",
        "\"\"\"\n",
        "colorize(Source.alice_in_wonderland_drawing, painting_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also change image compositions. Depending on the context, some compositions are more or less implied by default. For example, illustrations often have margins while photos generally have edge-to-edge (full-bleed in the printing world) compositions. When possible, it's interesting to refer to a type of visual (which intrinsically brings a lot of semantics to the context) and adjust the instructions accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's see how we can detect engravings in this 1847 book, restore them, and transform them into modern digital graphics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.engravings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "restore_objects(Source.engravings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visual_to_digital_graphic_prompt = \"\"\"\n",
        "Transform this visual into a full-color, modern digital graphic, extending the content for a full-bleed effect.\n",
        "\"\"\"\n",
        "colorize(Source.engravings, visual_to_digital_graphic_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ We can also transform the same engravings into photos with a very simple prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visual_to_photo_prompt = \"\"\"\n",
        "Transform this visual into a high-end, modern camera photograph.\n",
        "\"\"\"\n",
        "colorize(Source.engravings, visual_to_photo_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's really up to our imagination, as Nano Banana seems to grasp every aspect of the visual semantics.\n",
        "\n",
        "Let's add a final step to see how far we can go, reimagining images as cinematic movie stills‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéûÔ∏è Cinematization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've used rather \"closed\" prompts so far, crafting specific instructions and constraints to control the outputs. It's possible to go even further with \"open\" prompts and generate images in full creative mode. Notably, it can be interesting to refer to photographic or cinematographic terminology as it encompasses many visual techniques.\n",
        "\n",
        "Here is a possible generic cinematization function to reimagine images as movie stills:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CINEMATIZATION_PROMPT = \"\"\"\n",
        "Reimagine this image as a modern, live-action cinematic movie still featuring professional lighting and composition.\n",
        "\"\"\"\n",
        "\n",
        "cinematize = object_editing_function(\n",
        "    CINEMATIZATION_PROMPT,\n",
        "    WorkflowStep.RESTORED,\n",
        "    WorkflowStep.CINEMATIZED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's cinematize the \"Alice's Adventures in Wonderland\" drawing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cinematize(Source.alice_in_wonderland_drawing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° This looks like a high-budget movie still, with foreground figures in sharp focus, a gradual background blur, \"golden hour\" lighting (used so much by movie makers), and detailed textures. Such images really evoke different atmospheres compared to the photos generated in the previous test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üß™ Let's test the workflow on a page from the Wonderful Wizard of Oz containing three drawings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detect_objects(Source.wizard_of_oz_drawings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "restore_objects(Source.wizard_of_oz_drawings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cinematize(Source.wizard_of_oz_drawings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° The cast for a new movie is ready ;)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cinematic images are useful in different use cases:\n",
        "\n",
        "- They can be used directly as reference images to generate actual videos. See [Generate Veo videos from reference images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/use-reference-images-to-guide-video-generation#googlegenaisdk_videogen_with_image-python_genai_sdk).\n",
        "- As they are advanced photorealistic representations, they can also be used to regenerate 2D or 3D visuals with realistic figures, perfect proportions, advanced lighting, enhanced compositions‚Ä¶\n",
        "- You can use them in many professional contexts or for high-end products: presentations, magazines, posters, storyboards, brainstorming sessions‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÅ Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Gemini's native spatial understanding enables the detection of specific visual objects based on a single prompt in natural language.\n",
        "- We tested the detection of illustrations in book photos, which traditional machine learning (ML) models don't detect, as they are typically trained to detect people, animals, vehicles, food, and a finite set of physical object classes.\n",
        "- We tested the detection of straight, tilted, and even significantly warped illustrations, and they were always precisely identified.\n",
        "- This was straightforward to implement: just a few lines of code with the Python SDK and a customized prompt. Using a traditional, class-bound ML model would require a time-intensive prior process: sourcing images to gather a training set, precisely identifying the objects in the training set, then training and fine-tuning the model, and this only for a specific set of object classes.\n",
        "- This solution is very flexible: we could switch from detecting illustrations to electronic components, by adapting the prompt, while keeping the code unchanged.\n",
        "- Using structured outputs (with a JSON schema or Pydantic classes and the Python SDK), the code is both easy to implement and ready to deploy to production.\n",
        "- Then, Nano Banana allows editing these visual objects in many different ways, in virtually any way imaginable.\n",
        "- We tested a workflow with restoration, colorization, and even cinematization steps, using imperative and descriptive prompts.\n",
        "- The possibilities seem really endless, and the principles in this exploration can be reused in different contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ûï More!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Check out the documentation page about [Image understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding)\n",
        "- Explore additional use cases in the [Vertex AI Prompt Gallery](https://console.cloud.google.com/vertex-ai/studio/prompt-gallery)\n",
        "- Stay updated by following the [Vertex AI Release Notes](https://cloud.google.com/vertex-ai/generative-ai/docs/release-notes)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
