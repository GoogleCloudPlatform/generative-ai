{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Detecting and Editing Visual Objects with Gemini\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fspatial-understanding%2Fobject_detection_and_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<p>\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/object_detection_and_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author                                           |\n",
        "| ------------------------------------------------ |\n",
        "| [Laurent Picard](https://github.com/PicardParis) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b93adfc3c8fa"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚ú® Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5edadf8dcfc9"
      },
      "source": [
        "![intro image](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/spatial-understanding/object_detection_and_editing/_cover_image_.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1cf8e3b6d5"
      },
      "source": [
        "Traditional computer vision models are typically trained to detect a fixed set of object classes, like \"person\", \"cat\", or \"car\". If you want to detect something specific that wasn't in the training set, such as an \"illustration\" in a book photograph, you usually have to gather a dataset, label it manually, and train a custom model, which can take hours or even days.\n",
        "\n",
        "In this exploration, we'll test a different approach using Gemini. We will leverage its spatial understanding capabilities to perform open-vocabulary object detection. This allows us to find objects based solely on a natural language description, without any training.\n",
        "\n",
        "Once the visual objects are detected, we'll extract them and then use Gemini's image editing capabilities (specifically the Nano Banana models) to restore and creatively transform them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc142a7c92d6"
      },
      "source": [
        "---\n",
        "\n",
        "## üî• Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f40e2bbcdde2"
      },
      "source": [
        "We are dealing with unstructured data: photos of books, magazines, and objects in the wild. These images present several difficulties for traditional computer vision:\n",
        "\n",
        "- Variety: The objects we want to find (illustrations, engravings, and any visuals in general) vary wildly in style and content.\n",
        "- Distortion: Pages are curved, photos are taken at angles, and lighting is uneven.\n",
        "- Noise: Old books have stains, paper grain, and text bleeding through from the other side.\n",
        "\n",
        "Our challenge is to build a robust pipeline that can detect these objects despite the distortions, extract them cleanly, and edit them to look like high-quality digital assets‚Ä¶ all using simple text prompts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c75e2b06714"
      },
      "source": [
        "---\n",
        "\n",
        "## üèÅ Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcddf2ea48bb"
      },
      "source": [
        "### üêç Python packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73170c232f7e"
      },
      "source": [
        "We'll use the following packages:\n",
        "\n",
        "- `google-genai`: the [Google Gen AI Python SDK](https://pypi.org/project/google-genai) lets us call Gemini with a few lines of code\n",
        "- `pillow` for image management\n",
        "- `matplotlib` for result visualization\n",
        "\n",
        "We'll also use these packages (dependencies of `google-genai`):\n",
        "\n",
        "- `pydantic` for data management\n",
        "- `tenacity` for request management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb223df22157"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet \"google-genai>=1.63.0\" \"pillow>=11.3.0\" \"matplotlib>=3.10.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aad111e0a6c"
      },
      "source": [
        "### üîó Gemini API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2a3b681afa7"
      },
      "source": [
        "To use the Gemini API, we have two main options:\n",
        "\n",
        "1. Via **Vertex AI** with a Google Cloud project\n",
        "2. Via **Google AI Studio** with a Gemini API key\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these APIs, and we can use environment variables for the configuration.\n",
        "\n",
        "**üõ†Ô∏è Option 1 - Gemini API via Vertex AI**\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- A Google Cloud project\n",
        "- The Vertex AI API must be enabled for this project: ‚ñ∂Ô∏è [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,storage-component.googleapis.com)\n",
        "\n",
        "Gen AI SDK environment variables:\n",
        "\n",
        "- `GOOGLE_GENAI_USE_VERTEXAI=\"True\"`\n",
        "- `GOOGLE_CLOUD_PROJECT=\"<PROJECT_ID>\"`\n",
        "- `GOOGLE_CLOUD_LOCATION=\"<LOCATION>\"`\n",
        "\n",
        "> üí° For preview models, the location must be set to `global`. For generally available models, we can choose the closest location among the [Google model endpoint locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#google_model_endpoint_locations).\n",
        "\n",
        "> ‚ÑπÔ∏è Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "**üõ†Ô∏è Option 2 - Gemini API via Google AI Studio**\n",
        "\n",
        "Requirement:\n",
        "\n",
        "- A Gemini API key\n",
        "\n",
        "Gen AI SDK environment variables:\n",
        "\n",
        "- `GOOGLE_GENAI_USE_VERTEXAI=\"False\"`\n",
        "- `GOOGLE_API_KEY=\"<API_KEY>\"`\n",
        "\n",
        "> ‚ÑπÔ∏è Learn more about [getting a Gemini API key from Google AI Studio](https://aistudio.google.com/app/apikey).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "707a440313af"
      },
      "source": [
        "üí° You can store your environment configuration outside of the source code:\n",
        "\n",
        "| Environment         | Method                                                      |\n",
        "| ------------------- | ----------------------------------------------------------- |\n",
        "| IDE                 | `.env` file (or equivalent)                                 |\n",
        "| Colab               | Colab Secrets (üóùÔ∏è icon in left panel, see code below)       |\n",
        "| Colab Enterprise    | Google Cloud project and location are automatically defined |\n",
        "| Vertex AI Workbench | Google Cloud project and location are automatically defined |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a162746ccbca"
      },
      "source": [
        "Define the following environment detection functions. You can also define your configuration manually if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "153be2d25ccc"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "import os\n",
        "import sys\n",
        "from collections.abc import Callable\n",
        "\n",
        "from google import genai\n",
        "\n",
        "# Manual setup (leave unchanged if setup is environment-defined)\n",
        "\n",
        "# @markdown **Which API: Vertex AI or Google AI Studio?**\n",
        "GOOGLE_GENAI_USE_VERTEXAI = True  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown **Option A - Google Cloud project [+location]**\n",
        "GOOGLE_CLOUD_PROJECT = \"\"  # @param {type: \"string\"}\n",
        "GOOGLE_CLOUD_LOCATION = \"global\"  # @param {type: \"string\"}\n",
        "\n",
        "# @markdown **Option B - Google AI Studio API key**\n",
        "GOOGLE_API_KEY = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "def check_environment() -> bool:\n",
        "    check_colab_user_authentication()\n",
        "    return check_manual_setup() or check_vertex_ai() or check_colab() or check_local()\n",
        "\n",
        "\n",
        "def check_manual_setup() -> bool:\n",
        "    return check_define_env_vars(\n",
        "        GOOGLE_GENAI_USE_VERTEXAI,\n",
        "        GOOGLE_CLOUD_PROJECT.strip(),  # Might have been pasted with line return\n",
        "        GOOGLE_CLOUD_LOCATION,\n",
        "        GOOGLE_API_KEY,\n",
        "    )\n",
        "\n",
        "\n",
        "def check_vertex_ai() -> bool:\n",
        "    # Workbench and Colab Enterprise\n",
        "    match os.getenv(\"VERTEX_PRODUCT\", \"\"):\n",
        "        case \"WORKBENCH_INSTANCE\":\n",
        "            pass\n",
        "        case \"COLAB_ENTERPRISE\":\n",
        "            if not running_in_colab_env():\n",
        "                return False\n",
        "        case _:\n",
        "            return False\n",
        "\n",
        "    return check_define_env_vars(\n",
        "        True,\n",
        "        os.getenv(\"GOOGLE_CLOUD_PROJECT\", \"\"),\n",
        "        os.getenv(\"GOOGLE_CLOUD_REGION\", \"\"),\n",
        "        \"\",\n",
        "    )\n",
        "\n",
        "\n",
        "def check_colab() -> bool:\n",
        "    if not running_in_colab_env():\n",
        "        return False\n",
        "\n",
        "    # Colab Enterprise was checked before, so this is Colab only\n",
        "    from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "    colab_auth.authenticate_user()\n",
        "\n",
        "    # Use Colab Secrets (üóùÔ∏è icon in left panel) to store the environment variables\n",
        "    # Secrets are private, visible only to you and the notebooks that you select\n",
        "    # - Vertex AI: Store your settings as secrets\n",
        "    # - Google AI: Directly import your Gemini API key from the UI\n",
        "    vertexai, project, location, api_key = get_vars(get_colab_secret)\n",
        "\n",
        "    return check_define_env_vars(vertexai, project, location, api_key)\n",
        "\n",
        "\n",
        "def check_local() -> bool:\n",
        "    vertexai, project, location, api_key = get_vars(os.getenv)\n",
        "\n",
        "    return check_define_env_vars(vertexai, project, location, api_key)\n",
        "\n",
        "\n",
        "def running_in_colab_env() -> bool:\n",
        "    # Colab or Colab Enterprise\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "\n",
        "def check_colab_user_authentication() -> None:\n",
        "    if running_in_colab_env():\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "        colab_auth.authenticate_user()\n",
        "\n",
        "\n",
        "def get_colab_secret(secret_name: str, default: str) -> str:\n",
        "    from google.colab import errors, userdata  # type: ignore\n",
        "\n",
        "    try:\n",
        "        return userdata.get(secret_name)\n",
        "    except errors.SecretNotFoundError:\n",
        "        return default\n",
        "\n",
        "\n",
        "def disable_colab_cell_scrollbar() -> None:\n",
        "    if running_in_colab_env():\n",
        "        from google.colab import output  # type: ignore\n",
        "\n",
        "        output.no_vertical_scroll()\n",
        "\n",
        "\n",
        "def get_vars(getenv: Callable[[str, str], str]) -> tuple[bool, str, str, str]:\n",
        "    # Limit getenv calls to the minimum (may trigger UI confirmation for secret access)\n",
        "    vertexai_str = getenv(\"GOOGLE_GENAI_USE_VERTEXAI\", \"\")\n",
        "    if vertexai_str:\n",
        "        vertexai = vertexai_str.lower() in [\"true\", \"1\"]\n",
        "    else:\n",
        "        vertexai = bool(getenv(\"GOOGLE_CLOUD_PROJECT\", \"\"))\n",
        "\n",
        "    project = getenv(\"GOOGLE_CLOUD_PROJECT\", \"\") if vertexai else \"\"\n",
        "    location = getenv(\"GOOGLE_CLOUD_LOCATION\", \"\") if project else \"\"\n",
        "    api_key = getenv(\"GOOGLE_API_KEY\", \"\") if not project else \"\"\n",
        "\n",
        "    return vertexai, project, location, api_key\n",
        "\n",
        "\n",
        "def check_define_env_vars(\n",
        "    vertexai: bool,\n",
        "    project: str,\n",
        "    location: str,\n",
        "    api_key: str,\n",
        ") -> bool:\n",
        "    match (vertexai, bool(project), bool(location), bool(api_key)):\n",
        "        case (True, True, _, _):\n",
        "            # Vertex AI - Google Cloud project [+location]\n",
        "            location = location or \"global\"\n",
        "            define_env_vars(vertexai, project, location, \"\")\n",
        "        case (True, False, _, True):\n",
        "            # Vertex AI - API key\n",
        "            define_env_vars(vertexai, \"\", \"\", api_key)\n",
        "        case (False, _, _, True):\n",
        "            # Google AI Studio - API key\n",
        "            define_env_vars(vertexai, \"\", \"\", api_key)\n",
        "        case _:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def define_env_vars(vertexai: bool, project: str, location: str, api_key: str) -> None:\n",
        "    os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = str(vertexai)\n",
        "    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project\n",
        "    os.environ[\"GOOGLE_CLOUD_LOCATION\"] = location\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "def check_configuration(client: genai.Client) -> None:\n",
        "    service = \"Vertex AI\" if client.vertexai else \"Google AI Studio\"\n",
        "    print(f\"‚úÖ Using the {service} API\", end=\"\")\n",
        "\n",
        "    if client._api_client.project:\n",
        "        print(f' with project \"{client._api_client.project[:7]}‚Ä¶\"', end=\"\")\n",
        "        print(f' in location \"{client._api_client.location}\"')\n",
        "    elif client._api_client.api_key:\n",
        "        api_key = client._api_client.api_key\n",
        "        print(f' with API key \"{api_key[:5]}‚Ä¶{api_key[-5:]}\"', end=\"\")\n",
        "        print(f\" (in case of error, make sure it was created for {service})\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Environment functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc15c20f003e"
      },
      "source": [
        "### ü§ñ Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c8f008b2147"
      },
      "source": [
        "To send Gemini requests, create a `google.genai` client:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecc4c9a110e2"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "check_environment()\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "check_configuration(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc8127c2da50"
      },
      "source": [
        "### üñºÔ∏è Image test suite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311fc6f58d0d"
      },
      "source": [
        "Let's define a list of images for our tests:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58adc81b7d8c"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "from dataclasses import dataclass\n",
        "from enum import StrEnum\n",
        "from typing import TypeAlias\n",
        "\n",
        "Url: TypeAlias = str\n",
        "\n",
        "\n",
        "class Source(StrEnum):\n",
        "    incunable = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2014:2014rosen0487:0165/full/pct:25/0/default.jpg\"\n",
        "    engravings = \"https://tile.loc.gov/image-services/iiif/service:gdc:gdcscd:00:34:07:66:92:1:00340766921:0121/full/pct:50/0/default.jpg\"\n",
        "    museum_guidebook = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2014:2014gen34181:0033/full/pct:75/0/default.jpg\"\n",
        "    denver_illustrated = \"https://tile.loc.gov/image-services/iiif/service:gdc:gdclccn:rc:01:00:04:94:rc01000494:0051/full/pct:50/0/default.jpg\"\n",
        "    physics_textbook = \"https://tile.loc.gov/image-services/iiif/service:gdc:gdcscd:00:03:64:87:31:8:00036487318:0103/full/pct:50/0/default.jpg\"\n",
        "    portrait_miniatures = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2024:2024rosen013592v02:0249/full/pct:50/0/default.jpg\"\n",
        "    wizard_of_oz_drawings = \"https://tile.loc.gov/image-services/iiif/service:rbc:rbc0001:2006:2006gen32405:0048/full/pct:25/0/default.jpg\"\n",
        "    paintings = \"https://images.unsplash.com/photo-1714146681164-f26fed839692?h=1440\"\n",
        "    alice_drawing = \"https://images.unsplash.com/photo-1630595011903-689853b04ee2?h=800\"\n",
        "    book = \"https://images.unsplash.com/photo-1643451533573-ee364ba6e330?h=800\"\n",
        "    manual = \"https://images.unsplash.com/photo-1623666936367-a100f62ba9b7?h=800\"\n",
        "    electronics = \"https://images.unsplash.com/photo-1757397584789-8b2c5bfcdbc3?h=1440\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SourceMetadata:\n",
        "    title: str\n",
        "    webpage_url: Url\n",
        "    credit_line: str\n",
        "\n",
        "\n",
        "LOC = \"Library of Congress\"\n",
        "LOC_RARE_BOOKS = \"Library of Congress, Rare Book and Special Collections Division\"\n",
        "LOC_MEETING_FRONTIERS = \"Library of Congress, Meeting of Frontiers\"\n",
        "\n",
        "metadata_by_source: dict[Source, SourceMetadata] = {\n",
        "    Source.incunable: SourceMetadata(\n",
        "        \"Vergaderinge der historien van Troy (1485)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2014rosen0487/?sp=165\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.engravings: SourceMetadata(\n",
        "        \"Harper's illustrated catalogue (1847)\",\n",
        "        \"https://www.loc.gov/resource/gdcscd.00340766921/?sp=121\",\n",
        "        LOC,\n",
        "    ),\n",
        "    Source.museum_guidebook: SourceMetadata(\n",
        "        \"Barnum's American Museum illustrated (1850)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2014gen34181/?sp=33\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.denver_illustrated: SourceMetadata(\n",
        "        \"Denver illustrated (1893)\",\n",
        "        \"https://www.loc.gov/resource/gdclccn.rc01000494/?sp=51\",\n",
        "        LOC_MEETING_FRONTIERS,\n",
        "    ),\n",
        "    Source.physics_textbook: SourceMetadata(\n",
        "        \"Lessons in physics (1916)\",\n",
        "        \"https://www.loc.gov/resource/gdcscd.00036487318/?sp=103\",\n",
        "        LOC,\n",
        "    ),\n",
        "    Source.portrait_miniatures: SourceMetadata(\n",
        "        \"The history of portrait miniatures (1904)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2024rosen013592v02/?sp=249\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.wizard_of_oz_drawings: SourceMetadata(\n",
        "        \"The wonderful Wizard of Oz (1899)\",\n",
        "        \"https://www.loc.gov/resource/rbc0001.2006gen32405/?sp=48\",\n",
        "        LOC_RARE_BOOKS,\n",
        "    ),\n",
        "    Source.paintings: SourceMetadata(\n",
        "        \"Open book showing paintings by Vincent van Gogh\",\n",
        "        \"https://unsplash.com/photos/9hD7qrxICag\",\n",
        "        \"Photo by Trung Manh cong on Unsplash\",\n",
        "    ),\n",
        "    Source.alice_drawing: SourceMetadata(\n",
        "        \"Open book showing an illustration and text from Alice's Adventures in Wonderland\",\n",
        "        \"https://unsplash.com/photos/bewzr_Q9u2o\",\n",
        "        \"Photo by Brett Jordan on Unsplash\",\n",
        "    ),\n",
        "    Source.book: SourceMetadata(\n",
        "        \"Open book showing two botanical illustrations\",\n",
        "        \"https://unsplash.com/photos/4IDqcNj827I\",\n",
        "        \"Photo by Ranurte on Unsplash\",\n",
        "    ),\n",
        "    Source.manual: SourceMetadata(\n",
        "        \"Open user manual for vintage camera\",\n",
        "        \"https://unsplash.com/photos/aaFU96eYASk\",\n",
        "        \"Photo by Annie Spratt on Unsplash\",\n",
        "    ),\n",
        "    Source.electronics: SourceMetadata(\n",
        "        \"Circuit board with electronic components\",\n",
        "        \"https://unsplash.com/photos/Aqa1pHQ57pw\",\n",
        "        \"Photo by Albert Stoynov on Unsplash\",\n",
        "    ),\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Test images defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098070863122"
      },
      "source": [
        "### üß† Gemini models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7511ef9fea4"
      },
      "source": [
        "Gemini comes in different [versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models). We can currently use the following models:\n",
        "\n",
        "- For object detection: Gemini 2.5 or Gemini 3, each available in Flash or Pro versions.\n",
        "- For object editing: Gemini 2.5 Flash Image or Gemini 3 Pro Image, also known as Nano Banana and Nano Banana Pro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c81ec34039b"
      },
      "source": [
        "### üõ†Ô∏è Helpers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9b3bc58842"
      },
      "source": [
        "Now, let's add core helper classes and functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1008e1f9132b"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "from enum import Enum, auto\n",
        "from pathlib import Path\n",
        "from typing import Any, cast\n",
        "\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import pydantic\n",
        "import tenacity\n",
        "from google.genai.errors import ClientError\n",
        "from google.genai.types import (\n",
        "    FinishReason,\n",
        "    GenerateContentConfig,\n",
        "    GenerateContentResponse,\n",
        "    PIL_Image,\n",
        "    ThinkingConfig,\n",
        "    ThinkingLevel,\n",
        ")\n",
        "\n",
        "class Model(Enum):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Multimodal models with spatial understanding and structured outputs\n",
        "class MultimodalModel(Model):\n",
        "    # Generally Available (GA)\n",
        "    GEMINI_2_5_FLASH = \"gemini-2.5-flash\"\n",
        "    GEMINI_2_5_PRO = \"gemini-2.5-pro\"\n",
        "    # Preview\n",
        "    GEMINI_3_FLASH_PREVIEW = \"gemini-3-flash-preview\"\n",
        "    GEMINI_3_PRO_PREVIEW = \"gemini-3.1-pro-preview\"\n",
        "    # Default model used for object detection\n",
        "    DEFAULT = GEMINI_3_FLASH_PREVIEW\n",
        "\n",
        "\n",
        "# Image generation and editing models\n",
        "class ImageModel(Model):\n",
        "    # Generally Available (GA)\n",
        "    GEMINI_2_5_FLASH_IMAGE = \"gemini-2.5-flash-image\"  # Nano Banana üçå\n",
        "    # Preview\n",
        "    GEMINI_3_PRO_IMAGE_PREVIEW = \"gemini-3-pro-image-preview\"  # Nano Banana Pro üçå\n",
        "    # Default model used for image editing\n",
        "    DEFAULT = GEMINI_2_5_FLASH_IMAGE\n",
        "\n",
        "\n",
        "def generate_content(\n",
        "    contents: list[Any],\n",
        "    model: Model,\n",
        "    config: GenerateContentConfig | None,\n",
        "    should_display_response_info: bool = False,\n",
        ") -> GenerateContentResponse | None:\n",
        "    response = None\n",
        "    client = check_client_for_model(model)\n",
        "\n",
        "    for attempt in get_retrier():\n",
        "        with attempt:\n",
        "            response = client.models.generate_content(\n",
        "                model=model.value,\n",
        "                contents=contents,\n",
        "                config=config,\n",
        "            )\n",
        "    if should_display_response_info:\n",
        "        display_response_info(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def check_client_for_model(model: Model) -> genai.Client:\n",
        "    if (\n",
        "        model.value.endswith(\"-preview\")\n",
        "        and client.vertexai\n",
        "        and client._api_client.location != \"global\"\n",
        "    ):\n",
        "        # Preview models are only available on the \"global\" location\n",
        "        return genai.Client(location=\"global\")\n",
        "\n",
        "    return client\n",
        "\n",
        "\n",
        "def display_response_info(response: GenerateContentResponse | None) -> None:\n",
        "    if response is None:\n",
        "        print(\"‚ùå No response\")\n",
        "        return\n",
        "\n",
        "    if usage_metadata := response.usage_metadata:\n",
        "        if usage_metadata.prompt_token_count:\n",
        "            print(f\"Input tokens   : {usage_metadata.prompt_token_count:9,d}\")\n",
        "        if usage_metadata.candidates_token_count:\n",
        "            print(f\"Output tokens  : {usage_metadata.candidates_token_count:9,d}\")\n",
        "        if usage_metadata.thoughts_token_count:\n",
        "            print(f\"Thoughts tokens: {usage_metadata.thoughts_token_count:9,d}\")\n",
        "\n",
        "    if response.parsed is None:\n",
        "        print(\"‚ùå Could not parse the JSON response\")\n",
        "        return\n",
        "    if not response.candidates:\n",
        "        print(\"‚ùå No `response.candidates`\")\n",
        "        return\n",
        "    if (finish_reason := response.candidates[0].finish_reason) != FinishReason.STOP:\n",
        "        print(f\"‚ùå {finish_reason = }\")\n",
        "    if not response.text:\n",
        "        print(\"‚ùå No `response.text`\")\n",
        "        return\n",
        "\n",
        "\n",
        "def generate_image(\n",
        "    sources: list[PIL_Image],\n",
        "    prompt: str,\n",
        "    model: ImageModel,\n",
        "    config: GenerateContentConfig | None = None,\n",
        ") -> PIL_Image | None:\n",
        "    contents = [*sources, prompt.strip()]\n",
        "\n",
        "    response = generate_content(contents, model, config)\n",
        "\n",
        "    return check_get_output_image_from_response(response)\n",
        "\n",
        "\n",
        "def check_get_output_image_from_response(\n",
        "    response: GenerateContentResponse | None,\n",
        ") -> PIL_Image | None:\n",
        "    if response is None:\n",
        "        print(\"‚ùå No `response`\")\n",
        "        return None\n",
        "    if not response.candidates:\n",
        "        print(\"‚ùå No `response.candidates`\")\n",
        "        if response.prompt_feedback:\n",
        "            if block_reason := response.prompt_feedback.block_reason:\n",
        "                print(f\"{block_reason = :s}\")\n",
        "            if block_reason_message := response.prompt_feedback.block_reason_message:\n",
        "                print(f\"{block_reason_message = }\")\n",
        "        return None\n",
        "    if not (content := response.candidates[0].content):\n",
        "        print(\"‚ùå No `response.candidates[0].content`\")\n",
        "        return None\n",
        "    if not (parts := content.parts):\n",
        "        print(\"‚ùå No `response.candidates[0].content.parts`\")\n",
        "        return None\n",
        "\n",
        "    output_image: PIL_Image | None = None\n",
        "    for part in parts:\n",
        "        if part.text:\n",
        "            display_markdown(part.text)\n",
        "            continue\n",
        "        sdk_image = part.as_image()\n",
        "        assert sdk_image is not None\n",
        "        output_image = sdk_image._pil_image\n",
        "        assert output_image is not None\n",
        "        break  # There should be a single image\n",
        "\n",
        "    return output_image\n",
        "\n",
        "\n",
        "def get_thinking_config(model: Model) -> ThinkingConfig | None:\n",
        "    match model:\n",
        "        case MultimodalModel.GEMINI_2_5_FLASH:\n",
        "            return ThinkingConfig(thinking_budget=0)\n",
        "        case MultimodalModel.GEMINI_2_5_PRO:\n",
        "            return ThinkingConfig(thinking_budget=128, include_thoughts=False)\n",
        "        case MultimodalModel.GEMINI_3_FLASH_PREVIEW:\n",
        "            return ThinkingConfig(thinking_level=ThinkingLevel.MINIMAL)\n",
        "        case MultimodalModel.GEMINI_3_PRO_PREVIEW:\n",
        "            return ThinkingConfig(thinking_level=ThinkingLevel.LOW)\n",
        "        case _:\n",
        "            return None  # Default\n",
        "\n",
        "\n",
        "def display_markdown(markdown: str) -> None:\n",
        "    IPython.display.display(IPython.display.Markdown(markdown))\n",
        "\n",
        "\n",
        "def display_image(image: PIL_Image) -> None:\n",
        "    IPython.display.display(image)\n",
        "\n",
        "\n",
        "def get_retrier() -> tenacity.Retrying:\n",
        "    return tenacity.Retrying(\n",
        "        stop=tenacity.stop_after_attempt(7),\n",
        "        wait=tenacity.wait_incrementing(start=10, increment=1),\n",
        "        retry=should_retry_request,\n",
        "        reraise=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def should_retry_request(retry_state: tenacity.RetryCallState) -> bool:\n",
        "    if not retry_state.outcome:\n",
        "        return False\n",
        "    err = retry_state.outcome.exception()\n",
        "    if not isinstance(err, ClientError):\n",
        "        return False\n",
        "    print(f\"‚ùå ClientError {err.code}: {err.message}\")\n",
        "\n",
        "    retry = False\n",
        "    match err.code:\n",
        "        case 400 if err.message is not None and \" try again \" in err.message:\n",
        "            # Workshop: first time access to Cloud Storage (service agent provisioning)\n",
        "            retry = True\n",
        "        case 429:\n",
        "            # Workshop: temporary project with 1 QPM quota\n",
        "            retry = True\n",
        "    print(f\"üîÑ Retry: {retry}\")\n",
        "\n",
        "    return retry\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helpers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78e5c2adcda"
      },
      "source": [
        "---\n",
        "\n",
        "## üîç Detecting visual objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ae4a031de1b"
      },
      "source": [
        "To perform visual object detection, craft the prompt to indicate what you'd like to detect and how results should be returned. In the same request, it's possible to also extract additional information about each detected object. This can be virtually anything, from labels such as \"furniture\", \"table\", or \"chair\", to more precise classifications like \"mammals\" or \"reptiles\", or to contextual data such as captions, colors, shapes, etc.\n",
        "\n",
        "For the next tests, we'll experiment with detecting illustrations within book photos. Here's a possible prompt:\n",
        "\n",
        "```python\n",
        "OBJECT_DETECTION_PROMPT = \"\"\"\n",
        "Detect every illustration within the book photo and extract the following data for each:\n",
        "- `box_2d`: Bounding box coordinates of the illustration only (ignoring any caption).\n",
        "- `caption`: Verbatim caption or legend such as \"Figure 1\". Use \"\" if not found.\n",
        "- `label`: Single-word label describing the illustration. Use \"\" if not found.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Bounding boxes are very useful for locating or extracting the detected objects.\n",
        "- Typically, for Gemini models, a `box_2d` bounding box represents coordinates normalized to a `(0, 0, 1000, 1000)` space for a `(0, 0, width, height)` input image.\n",
        "- We're also requesting to extract captions (metadata often present in reference books) and labels (dynamic metadata).\n",
        "\n",
        "To automate response processing, it's convenient to define a Pydantic class that matches the prompt, such as:\n",
        "\n",
        "```python\n",
        "class DetectedObject(pydantic.BaseModel):\n",
        "    box_2d: list[int]\n",
        "    caption: str\n",
        "    label: str\n",
        "\n",
        "DetectedObjects: TypeAlias = list[DetectedObject]\n",
        "```\n",
        "\n",
        "Then, request a structured output with config fields `response_mime_type` and `response_schema`:\n",
        "\n",
        "```python\n",
        "config = GenerateContentConfig(\n",
        "    # ‚Ä¶,\n",
        "    response_mime_type=\"application/json\",\n",
        "    response_schema=DetectedObjects,\n",
        "    # ‚Ä¶,\n",
        ")\n",
        "```\n",
        "\n",
        "This will generate a JSON response which the SDK can parse automatically, letting us directly use object instances:\n",
        "\n",
        "```python\n",
        "detected_objects = cast(DetectedObjects, response.parsed)\n",
        "```\n",
        "\n",
        "Let's add a few object-detection-specific classes and functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d6f8582c518"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "import io\n",
        "import urllib.request\n",
        "from dataclasses import field\n",
        "from datetime import datetime\n",
        "\n",
        "import PIL.Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "from google.genai.types import Part, PartMediaResolutionLevel\n",
        "\n",
        "OBJECT_DETECTION_PROMPT = \"\"\"\n",
        "Detect every illustration within the book photo and extract the following data for each:\n",
        "- `box_2d`: Bounding box coordinates of the illustration only (ignoring any caption).\n",
        "- `caption`: Verbatim caption or legend such as \"Figure 1\". Use \"\" if not found.\n",
        "- `label`: Single-word label describing the illustration. Use \"\" if not found.\n",
        "\"\"\"\n",
        "\n",
        "# Margin added to detected/cropped objects, giving more context for a better understanding of spatial distortions\n",
        "CROP_MARGIN_PX = 10\n",
        "\n",
        "# Set to True to save each generated image\n",
        "SAVE_GENERATED_IMAGES = False\n",
        "OUTPUT_IMAGES_PATH = Path(\"./object_detection_and_editing\")\n",
        "\n",
        "\n",
        "# Matching class for structured output generation\n",
        "class DetectedObject(pydantic.BaseModel):\n",
        "    box_2d: list[int]\n",
        "    caption: str\n",
        "    label: str\n",
        "\n",
        "\n",
        "# Misc data classes\n",
        "InputImage: TypeAlias = Path | Url\n",
        "DetectedObjects: TypeAlias = list[DetectedObject]\n",
        "WorkflowStepImages: TypeAlias = list[PIL_Image]\n",
        "\n",
        "\n",
        "class WorkflowStep(StrEnum):\n",
        "    SOURCE = auto()\n",
        "    CROPPED = auto()\n",
        "    RESTORED = auto()\n",
        "    COLORIZED = auto()\n",
        "    CINEMATIZED = auto()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VisualObjectWorkflow:\n",
        "    source_image: PIL_Image\n",
        "    detected_objects: DetectedObjects\n",
        "    images_by_step: dict[WorkflowStep, WorkflowStepImages] = field(default_factory=dict)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        denormalize_bounding_boxes(self)\n",
        "\n",
        "\n",
        "workflow_by_image: dict[InputImage, VisualObjectWorkflow] = dict()\n",
        "\n",
        "\n",
        "def denormalize_bounding_boxes(self: VisualObjectWorkflow) -> None:\n",
        "    \"\"\"Convert the box_2d coordinates.\n",
        "    - Before: [y1, x1, y2, x2] normalized to 0-1000, as returned by Gemini\n",
        "    - After:  [x1, y1, x2, y2] in source_image coordinates, as used in Pillow\n",
        "    \"\"\"\n",
        "\n",
        "    def to_image_coord(coord: int, dim: int) -> int:\n",
        "        return int(coord * dim / 1000 + 0.5)\n",
        "\n",
        "    w, h = self.source_image.size\n",
        "    for obj in self.detected_objects:\n",
        "        y1, x1, y2, x2 = obj.box_2d\n",
        "        x1, x2 = to_image_coord(x1, w), to_image_coord(x2, w)\n",
        "        y1, y2 = to_image_coord(y1, h), to_image_coord(y2, h)\n",
        "        obj.box_2d = [x1, y1, x2, y2]\n",
        "\n",
        "\n",
        "def detect_objects(\n",
        "    image: InputImage,\n",
        "    prompt: str = OBJECT_DETECTION_PROMPT,\n",
        "    model: MultimodalModel = MultimodalModel.DEFAULT,\n",
        "    config: GenerateContentConfig | None = None,\n",
        "    media_resolution: PartMediaResolutionLevel | None = None,\n",
        "    display_results: bool = True,\n",
        ") -> None:\n",
        "    display_image_source_info(image)\n",
        "    pil_image, content_part = get_pil_image_and_part(image, model, media_resolution)\n",
        "    prompt = prompt.strip()\n",
        "    contents = [content_part, prompt]\n",
        "    config = config or get_object_detection_config(model)\n",
        "\n",
        "    response = generate_content(contents, model, config)\n",
        "\n",
        "    if response is not None and response.parsed is not None:\n",
        "        detected_objects = cast(\"DetectedObjects\", response.parsed)\n",
        "    else:\n",
        "        detected_objects = DetectedObjects()\n",
        "\n",
        "    workflow = VisualObjectWorkflow(pil_image, detected_objects)\n",
        "    workflow_by_image[image] = workflow\n",
        "    add_cropped_objects(workflow, image, prompt)\n",
        "\n",
        "    if display_results:\n",
        "        display_detected_objects(workflow)\n",
        "\n",
        "\n",
        "def get_pil_image_and_part(\n",
        "    image: InputImage,\n",
        "    model: MultimodalModel,\n",
        "    media_resolution: PartMediaResolutionLevel | None,\n",
        ") -> tuple[PIL_Image, Part]:\n",
        "    if isinstance(image, Path):\n",
        "        image_bytes = image.read_bytes()\n",
        "    else:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        req = urllib.request.Request(image, headers=headers)\n",
        "        with urllib.request.urlopen(req, timeout=10) as response:\n",
        "            image_bytes = response.read()\n",
        "\n",
        "    pil_image = PIL.Image.open(io.BytesIO(image_bytes))\n",
        "    content_part = Part.from_bytes(\n",
        "        data=image_bytes,\n",
        "        mime_type=\"image/*\",\n",
        "        media_resolution=media_resolution,\n",
        "    )\n",
        "\n",
        "    return pil_image, content_part\n",
        "\n",
        "\n",
        "def get_object_detection_config(model: Model) -> GenerateContentConfig:\n",
        "    # Low randomness for more determinism\n",
        "    return GenerateContentConfig(\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        seed=42,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=DetectedObjects,\n",
        "        thinking_config=get_thinking_config(model),\n",
        "    )\n",
        "\n",
        "\n",
        "def add_cropped_objects(\n",
        "    workflow: VisualObjectWorkflow,\n",
        "    input: InputImage,\n",
        "    prompt: str,\n",
        "    crop_margin: int = CROP_MARGIN_PX,\n",
        ") -> None:\n",
        "    cropped_images: list[PIL_Image] = []\n",
        "    obj_count = len(workflow.detected_objects)\n",
        "    for obj_order, obj in enumerate(workflow.detected_objects, 1):\n",
        "        cropped_image, _ = extract_object_image(workflow.source_image, obj, crop_margin)\n",
        "        cropped_images.append(cropped_image)\n",
        "        save_workflow_image(\n",
        "            WorkflowStep.SOURCE,\n",
        "            WorkflowStep.CROPPED,\n",
        "            input,\n",
        "            obj_order,\n",
        "            obj_count,\n",
        "            cropped_image,\n",
        "            dict(prompt=prompt, crop_margin=str(crop_margin)),\n",
        "        )\n",
        "    workflow.images_by_step[WorkflowStep.CROPPED] = cropped_images\n",
        "\n",
        "\n",
        "def extract_object_image(\n",
        "    image: PIL_Image,\n",
        "    obj: DetectedObject,\n",
        "    margin: int = 0,\n",
        ") -> tuple[PIL_Image, tuple[int, int, int, int]]:\n",
        "    def clamp(coord: int, dim: int) -> int:\n",
        "        return min(max(coord, 0), dim)\n",
        "\n",
        "    x1, y1, x2, y2 = obj.box_2d\n",
        "    w, h = image.size\n",
        "    if margin != 0:\n",
        "        x1, x2 = clamp(x1 - margin, w), clamp(x2 + margin, w)\n",
        "        y1, y2 = clamp(y1 - margin, h), clamp(y2 + margin, h)\n",
        "\n",
        "    box = (x1, y1, x2, y2)\n",
        "    object_image = image.crop(box)\n",
        "\n",
        "    return object_image, box\n",
        "\n",
        "\n",
        "def save_workflow_image(\n",
        "    source_step: WorkflowStep,\n",
        "    target_step: WorkflowStep,\n",
        "    input_image: InputImage,\n",
        "    obj_order: int,\n",
        "    obj_count: int,\n",
        "    target_image: PIL_Image | None,\n",
        "    image_info: dict[str, str] | None = None,\n",
        ") -> None:\n",
        "    if not SAVE_GENERATED_IMAGES or target_image is None:\n",
        "        return\n",
        "    if not OUTPUT_IMAGES_PATH.is_dir():\n",
        "        OUTPUT_IMAGES_PATH.mkdir(parents=True)\n",
        "    time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    try:\n",
        "        filename = f\"{Source(input_image).name}_\"\n",
        "    except ValueError:\n",
        "        filename = \"\"\n",
        "    filename += f\"{obj_order}o{obj_count}_{source_step}_{target_step}_{time_str}.png\"\n",
        "    image_path = OUTPUT_IMAGES_PATH.joinpath(filename)\n",
        "    params = dict()\n",
        "    if image_info:\n",
        "        png_info = PngInfo()\n",
        "        for k, v in image_info.items():\n",
        "            png_info.add_text(k, v)\n",
        "        params.update(pnginfo=png_info)\n",
        "    target_image.save(image_path, **params)\n",
        "\n",
        "\n",
        "# Matplotlib\n",
        "FIGURE_FG_COLOR = \"#F1F3F4\"\n",
        "FIGURE_BG_COLOR = \"#202124\"\n",
        "EDGE_COLOR = \"#80868B\"\n",
        "rcParams = {\n",
        "    \"figure.dpi\": 300,\n",
        "    \"text.color\": FIGURE_FG_COLOR,\n",
        "    \"figure.edgecolor\": FIGURE_FG_COLOR,\n",
        "    \"axes.titlecolor\": FIGURE_FG_COLOR,\n",
        "    \"axes.edgecolor\": FIGURE_FG_COLOR,\n",
        "    \"xtick.color\": FIGURE_FG_COLOR,\n",
        "    \"ytick.color\": FIGURE_FG_COLOR,\n",
        "    \"figure.facecolor\": FIGURE_BG_COLOR,\n",
        "    \"axes.edgecolor\": EDGE_COLOR,\n",
        "    \"xtick.bottom\": False,\n",
        "    \"xtick.top\": False,\n",
        "    \"ytick.left\": False,\n",
        "    \"ytick.right\": False,\n",
        "    \"xtick.labelbottom\": False,\n",
        "    \"ytick.labelleft\": False,\n",
        "}\n",
        "plt.rcParams.update(rcParams)\n",
        "\n",
        "\n",
        "def display_image_source_info(image: InputImage) -> None:\n",
        "    def markdown_for_image() -> str:\n",
        "        if image not in Source:\n",
        "            return f\"[[Source Image]({image})]\"\n",
        "        source = Source(image)\n",
        "        metadata = metadata_by_source.get(source)\n",
        "        if not metadata:\n",
        "            return f\"[[Source Image]({source.value})]\"\n",
        "        parts = [\n",
        "            f\"[Source Image]({source.value})\",\n",
        "            f\"[Source Page]({metadata.webpage_url})\",\n",
        "            metadata.title,\n",
        "            metadata.credit_line,\n",
        "        ]\n",
        "        separator = \"‚Ä¢\"\n",
        "        inner_info = f\" {separator} \".join(parts)\n",
        "        return f\"{separator} {inner_info} {separator}\"\n",
        "\n",
        "    horizontal_line = \"---\"\n",
        "    markdown = \"\\n\\n\".join([horizontal_line, markdown_for_image(), horizontal_line])\n",
        "    display_markdown(markdown)\n",
        "\n",
        "\n",
        "def display_detected_objects(workflow: VisualObjectWorkflow) -> None:\n",
        "    source_image = workflow.source_image\n",
        "    detected_objects = PIL.Image.new(\"RGB\", source_image.size, \"white\")\n",
        "    for obj in workflow.detected_objects:\n",
        "        obj_image, box = extract_object_image(source_image, obj)\n",
        "        detected_objects.paste(obj_image, (box[0], box[1]))\n",
        "\n",
        "    fig = plt.figure(layout=\"compressed\")\n",
        "    horizontal = True\n",
        "    rows, cols = (1, 2) if horizontal else (2, 1)\n",
        "    gs = fig.add_gridspec(rows, cols)\n",
        "    ax = fig.add_subplot(gs[0, 0])\n",
        "    ax.imshow(source_image)\n",
        "    ax = fig.add_subplot(gs[rows - 1, cols - 1])\n",
        "    ax.imshow(detected_objects)\n",
        "\n",
        "    disable_colab_cell_scrollbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Object detection helpers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fc42a298b2"
      },
      "source": [
        "üß™ Let's start simple: can we detect the single illustration in this incunable from 1485?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03d35b04140b"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.incunable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83951310b8d"
      },
      "source": [
        "> üí° This works nicely. The bounding box is very precise, enclosing the hand-colored woodcut illustration very tightly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40d0697d9e98"
      },
      "source": [
        "üß™ Now, let's check the detection of the multiple visuals in this museum guidebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90e1a6819736"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c658983d8a66"
      },
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - The bounding boxes are again very precise.\n",
        "> - The results are perfect: there are no false positives and no false negatives.\n",
        "> - The captions below the visuals are not enclosed within the bounding boxes, which was specifically requested. The bounding box granularity can be controlled by changing the prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f5ffb27cc5f"
      },
      "source": [
        "üß™ What about slightly warped visuals?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f929f53dcd7c"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.paintings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce9a5c7f9060"
      },
      "source": [
        "> üí° This doesn't make a difference. Notice how the bottom-right painting is partially covered by the orange bookmark. We'll try to fix that in the restoration step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4869f7509aaa"
      },
      "source": [
        "üß™ What about the tilted visuals in this book about the architecture in Denver?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c5807d309e5"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.denver_illustrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0057364a2b84"
      },
      "source": [
        "> üí° Each visual is perfectly detected: spatial understanding covers tilted objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85c320bd8134"
      },
      "source": [
        "üß™ Finally, let's check the detection on this significantly warped book page from Alice's Adventures in Wonderland:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c55df2cb992"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.alice_drawing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "283221cf3524"
      },
      "source": [
        "> üí° Page curvature and other distortions don't prevent non-rectangular objects from being detected. In fact, spatial understanding works at the pixel level, which explains this precision for warped objects. If you'd like to work at a lower level, you can also ask for a \"segmentation mask\" in the prompt and you'll get a base64-encoded PNG (each pixel giving the 0-255 probability it belongs to the object within the bounding box). See the [segmentation doc](https://ai.google.dev/gemini-api/docs/image-understanding#segmentation) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19043f250ef8"
      },
      "source": [
        "---\n",
        "\n",
        "## üè∑Ô∏è Text extraction and dynamic labeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7e11761922"
      },
      "source": [
        "On top of localizing each object with its bounding box, our prompt requested to extract a verbatim `caption` and to assign a single-word `label`, when possible.\n",
        "\n",
        "Let's add a simple function to display the detection data in a table:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b2616a8d96f"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from collections.abc import Iterator\n",
        "\n",
        "\n",
        "def display_detection_data(source: Source, show_consolidated: bool = False) -> None:\n",
        "    def string_with_visible_linebreaks(s: str) -> str:\n",
        "        return f'''\"{s.replace('\n",
        "', '‚Ü©Ô∏è')}\"'''\n",
        "\n",
        "    def yield_md_rows_consolidated(workflow: VisualObjectWorkflow) -> Iterator[str]:\n",
        "        yield \"| label | count | captions |\"\n",
        "        yield \"| :--- | ---: | :--- |\"\n",
        "        stats = defaultdict(list)\n",
        "        for obj in workflow.detected_objects:\n",
        "            stats[obj.label].append(string_with_visible_linebreaks(obj.caption))\n",
        "        for label, captions in stats.items():\n",
        "            count = len(captions)\n",
        "            label_captions = \" ‚Ä¢ \".join(sorted(captions))\n",
        "            yield f\"| {label} | {count} | {label_captions} |\"\n",
        "\n",
        "    def yield_md_rows_with_bbox(workflow: VisualObjectWorkflow) -> Iterator[str]:\n",
        "        yield \"| box_2d | label | caption |\"\n",
        "        yield \"| :--- | :--- | :--- |\"\n",
        "        for obj in workflow.detected_objects:\n",
        "            yield f\"| {obj.box_2d} | {obj.label} | {string_with_visible_linebreaks(obj.caption)} |\"\n",
        "\n",
        "    workflow = workflow_by_image.get(source)\n",
        "    if workflow is None:\n",
        "        print(f'‚ùå No detection for source \"{source.name}\"')\n",
        "        return\n",
        "    md_rows = list(\n",
        "        yield_md_rows_consolidated(workflow)\n",
        "        if show_consolidated\n",
        "        else yield_md_rows_with_bbox(workflow)\n",
        "    )\n",
        "    display_image_source_info(source)\n",
        "    display_markdown(\"\n",
        "\".join(md_rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cbc8a954164"
      },
      "source": [
        "In the museum guidebook, the dynamic labeling is precise according to the context, and the captions below each illustration are perfectly extracted:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcdc70f378a6"
      },
      "outputs": [],
      "source": [
        "display_detection_data(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2b6727eea3a"
      },
      "source": [
        "In the book photo showing four paintings, this is perfect too:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce778e496ff7"
      },
      "outputs": [],
      "source": [
        "display_detection_data(Source.paintings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692fb5520974"
      },
      "source": [
        "In the Denver architecture book, the four captions are assigned to the correct illustrations, which was not an obvious task:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ff771a1d61"
      },
      "outputs": [],
      "source": [
        "display_detection_data(Source.denver_illustrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d85b6b7333ae"
      },
      "source": [
        "> üí° If you have a closer look at the input image, it's hard to tell which caption belongs to which illustration at a glance. Most of us would need to think about it (and may be wrong). Asking Gemini indicates that the results are intentional and not pure luck: _Deciphering vintage layouts can feel a bit like a puzzle, but there is usually a \"reading-order\" logic at play. In this specific case, the captions are arranged to correspond with the images in a clockwise or Z-pattern starting from the top left._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a77324a98c6c"
      },
      "source": [
        "In the \"Alice's Adventures in Wonderland\" book page, there was a single illustration accompanying the story text. As expected, the caption is empty (i.e., no false positive):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c717ebadd61"
      },
      "outputs": [],
      "source": [
        "display_detection_data(Source.alice_drawing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "413e97ed78b4"
      },
      "source": [
        "---\n",
        "\n",
        "## üî≠ Generalizing object detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b2b8297acb9"
      },
      "source": [
        "We can use the same principles for other object types. We'll generally keep requesting bounding boxes to identify object positions within images. Without changing our current output structure (i.e., no code change), we can use captions and labels to extract different object metadata depending on the input type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3b6f3112e7e"
      },
      "source": [
        "üß™ See how we can detect electronic components by adapting the prompt while keeping the exact same code and output structure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "127175ee4ca1"
      },
      "outputs": [],
      "source": [
        "ELECTRONIC_COMPONENT_DETECTION_PROMPT = \"\"\"\n",
        "Exhaustively detect all the individual electronic components in the image and provide the following data for each:\n",
        "- `box_2d`: bounding box coordinates.\n",
        "- `caption`: Verbatim alphanumeric text visible on the component (including original line breaks), or \"\" if no text is present.\n",
        "- `label`: Specific type of component.\n",
        "\"\"\"\n",
        "\n",
        "detect_objects(\n",
        "    Source.electronics,\n",
        "    ELECTRONIC_COMPONENT_DETECTION_PROMPT,\n",
        "    media_resolution=PartMediaResolutionLevel.MEDIA_RESOLUTION_ULTRA_HIGH,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltolrjsxn3cd"
      },
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - Large and tiny components are detected, thanks to the specific instruction \"exhaustively detect‚Ä¶\".\n",
        "> - By using the ultra-high media resolution, we ensure more details are tokenized and the \"P\" component (a visual outlier) gets detected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe10f94c263"
      },
      "source": [
        "Here's a consolidated view of the detected components:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19946a978745"
      },
      "outputs": [],
      "source": [
        "display_detection_data(Source.electronics, show_consolidated=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": ""
      },
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - Components are detected along with their text markings, despite the three different text orientations (upright, sideways, and upside down), the blur, and the photo noise.\n",
        "> - We removed the degree of freedom for multi-line text by specifying to include \"original line breaks\" in the prompt: responses now consistently include the line breaks for the three integrated circuits (displayed with the ‚Ü©Ô∏è emoji for better visibility).\n",
        "> - The last degree of freedom lies in the labeling. While most components have been properly labeled, it is unclear whether the \"P\" component is a diode, a resistor, or a fuse. Making the instructions more specific (e.g., listing the possible labels, using an enum for the `label` field in the Pydantic class, or providing guidelines and more details about the expected circuit boards) will make the prompt more \"closed\" and the results more deterministic and accurate.\n",
        ">   It's also possible to enable/update the `thinking_config` configuration, which will trigger a chain of thought before generating the final answer. In all the detections performed, our code used `ThinkingLevel.MINIMAL`, which didn't consume any thought tokens (with Gemini 3 Flash). Updating the parameter to `ThinkingLevel.LOW`, `ThinkingLevel.MEDIUM`, or `ThinkingLevel.HIGH` will use thought tokens and can lead to better outputs in complex cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44c5f5f21a8f"
      },
      "source": [
        "This demonstrates the versatility of the approach. Without retraining a model, we switched from detecting 15th-century woodcuts and illustrations with vintage layouts to identifying modern electronics just by changing the prompt. Such detections, including caption and label metadata, could be used to auto-crop components for a parts catalog, verify assembly lines, or create interactive schematics‚Ä¶ all without a single labeled training image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690d1bd67282"
      },
      "source": [
        "---\n",
        "\n",
        "## ü™Ñ Editing visual objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd4061f73fa7"
      },
      "source": [
        "Now that we can detect visual objects, we can envision an automation workflow to extract and reuse them. For this, we'll use Gemini 2.5 Flash Image (also known as Nano Banana üçå) by default, a state-of-the-art image generation and editing model.\n",
        "\n",
        "Our object editing functions will follow the same template, taking one step as input and generating an edited image for the output step. Let's define core helpers for this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7397425db1f0"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "from typing import Protocol\n",
        "\n",
        "class ObjectEditingFunction(Protocol):\n",
        "    def __call__(\n",
        "        self,\n",
        "        image: InputImage,\n",
        "        prompt: str | None = None,\n",
        "        model: ImageModel | None = None,\n",
        "        config: GenerateContentConfig | None = None,\n",
        "        display_results: bool = True,\n",
        "    ) -> None: ...\n",
        "\n",
        "\n",
        "SourceTargetSteps = tuple[WorkflowStep, WorkflowStep]\n",
        "registered_functions: dict[SourceTargetSteps, ObjectEditingFunction] = dict()\n",
        "\n",
        "DEFAULT_EDITING_CONFIG = GenerateContentConfig(response_modalities=[\"IMAGE\"])\n",
        "EMPTY_IMAGE = PIL.Image.new(\"1\", (1, 1), \"white\")\n",
        "\n",
        "\n",
        "def object_editing_function(\n",
        "    default_prompt: str,\n",
        "    source_step: WorkflowStep,\n",
        "    target_step: WorkflowStep,\n",
        "    default_model: ImageModel = ImageModel.DEFAULT,\n",
        "    default_config: GenerateContentConfig = DEFAULT_EDITING_CONFIG,\n",
        ") -> ObjectEditingFunction:\n",
        "    def editing_function(\n",
        "        image: InputImage,\n",
        "        prompt: str | None = default_prompt,\n",
        "        model: ImageModel | None = default_model,\n",
        "        config: GenerateContentConfig | None = default_config,\n",
        "        display_results: bool = True,\n",
        "    ) -> None:\n",
        "        workflow, source_images = get_workflow_and_step_images(image, source_step)\n",
        "        if prompt is None:\n",
        "            prompt = default_prompt\n",
        "        prompt = prompt.strip()\n",
        "        if model is None:\n",
        "            model = default_model\n",
        "        # Note: \"config is None\" is valid and will use the model endpoint default config\n",
        "\n",
        "        target_images: list[PIL_Image] = []\n",
        "        display_image_source_info(image)\n",
        "        obj_count = len(source_images)\n",
        "        for obj_order, source_image in enumerate(source_images, 1):\n",
        "            target_image = generate_image([source_image], prompt, model, config)\n",
        "            save_workflow_image(\n",
        "                source_step,\n",
        "                target_step,\n",
        "                image,\n",
        "                obj_order,\n",
        "                obj_count,\n",
        "                target_image,\n",
        "                dict(prompt=prompt),\n",
        "            )\n",
        "            target_images.append(target_image if target_image else EMPTY_IMAGE)\n",
        "\n",
        "        workflow.images_by_step[target_step] = target_images\n",
        "        if display_results:\n",
        "            display_sources_and_targets(workflow, source_step, target_step)\n",
        "\n",
        "    registered_functions[(source_step, target_step)] = editing_function\n",
        "\n",
        "    return editing_function\n",
        "\n",
        "\n",
        "def get_workflow_and_step_images(\n",
        "    image: InputImage,\n",
        "    step: WorkflowStep,\n",
        ") -> tuple[VisualObjectWorkflow, list[PIL_Image]]:\n",
        "    # Objects detected?\n",
        "    if image not in workflow_by_image:\n",
        "        detect_objects(image, display_results=False)\n",
        "    workflow = workflow_by_image.get(image)\n",
        "    assert workflow is not None\n",
        "\n",
        "    # Workflow step objects? (single level, could be extended to a dynamical graph)\n",
        "    operation = (WorkflowStep.CROPPED, step)\n",
        "    if step not in workflow.images_by_step and operation in registered_functions:\n",
        "        source_function = registered_functions[operation]\n",
        "        source_function(image, display_results=False)\n",
        "\n",
        "    # Source images\n",
        "    source_images = workflow.images_by_step.get(step)\n",
        "    assert source_images is not None\n",
        "\n",
        "    return workflow, source_images\n",
        "\n",
        "\n",
        "def display_sources_and_targets(\n",
        "    workflow: VisualObjectWorkflow,\n",
        "    source_step: WorkflowStep,\n",
        "    target_step: WorkflowStep,\n",
        ") -> None:\n",
        "    source_images = workflow.images_by_step[source_step]\n",
        "    target_images = workflow.images_by_step[target_step]\n",
        "    assert len(source_images) == len(target_images)\n",
        "\n",
        "    fig = plt.figure(layout=\"compressed\")\n",
        "    if horizontal := (len(source_images) >= 2):\n",
        "        rows, cols = 2, len(source_images)\n",
        "    else:\n",
        "        rows, cols = len(source_images), 2\n",
        "    gs = fig.add_gridspec(rows, cols)\n",
        "    source_image = workflow.source_image\n",
        "\n",
        "    for i, (source_image, target_image) in enumerate(\n",
        "        zip(source_images, target_images, strict=False)\n",
        "    ):\n",
        "        for dim in [0, 1]:\n",
        "            image = source_image if dim == 0 else target_image\n",
        "            grid_spec = gs[dim, i] if horizontal else gs[i, dim]\n",
        "            ax = fig.add_subplot(grid_spec)\n",
        "            ax.set_axis_off()\n",
        "            ax.imshow(image)\n",
        "\n",
        "    disable_colab_cell_scrollbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Object editing helpers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16015359d708"
      },
      "source": [
        "Now, let's define a first editing step, to restore the detected objects that can contain many real-life artifacts‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0190a6c6d65"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚ú® Restoring visual objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a81992d9178"
      },
      "source": [
        "For this restoration step, we need to craft a prompt that is generic enough (to cover most use cases) but also specific enough (to take into account restoration needs).\n",
        "\n",
        "An image editing prompt is based on natural language, typically using imperative or declarative instructions. With an imperative prompt, you describe the actions to perform on the input, while with a declarative prompt, you describe the expected output. Both are possible and will provide equivalent results. Your choice is really a matter of preference, as long as the prompt makes sense.\n",
        "\n",
        "Our test suite is mostly composed of book photos, which can contain various photographic and paper artifacts. The Nano Banana models understand these subtleties and can edit images accordingly, which simplifies the prompt.\n",
        "\n",
        "Here is a possible restoration function using an imperative prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "610b4adf856f"
      },
      "outputs": [],
      "source": [
        "RESTORATION_PROMPT = \"\"\"\n",
        "- Isolate and straighten the visual on a pure white background, excluding any surrounding text.\n",
        "- Clean up all physical artifacts and noise while preserving every original detail.\n",
        "- Center the result and scale it to fit the canvas with minimal, symmetrical margins, ensuring no distortion or cropping.\n",
        "\"\"\"\n",
        "\n",
        "# Default config with low randomness for more deterministic restoration outputs\n",
        "RESTORATION_CONFIG = GenerateContentConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=0.0,\n",
        "    seed=42,\n",
        "    response_modalities=[\"IMAGE\"],\n",
        ")\n",
        "\n",
        "restore_objects = object_editing_function(\n",
        "    RESTORATION_PROMPT,\n",
        "    WorkflowStep.CROPPED,\n",
        "    WorkflowStep.RESTORED,\n",
        "    default_config=RESTORATION_CONFIG,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Restoration function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c0f0e250938"
      },
      "source": [
        "üß™ Let's try to restore the illustration from the 1485 incunable:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fffdb14f08fe"
      },
      "outputs": [],
      "source": [
        "restore_objects(Source.incunable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da022e0682b5"
      },
      "source": [
        "> üí° We now have a nice restoration of the hand-colored woodcut illustration. Note that our prompt is generic (_\"clean up all physical artifacts\"_) and could be made more specific to remove more or fewer artifacts. In this example, there are remaining artifacts, such as the paper discoloration in the sword or the bleeding ink in the armor. We'll see if we can fix these in the colorization step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721724969f5f"
      },
      "source": [
        "üß™ What about the illustrations from the museum guidebook?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8482022c64df"
      },
      "outputs": [],
      "source": [
        "restore_objects(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4a9d196e369"
      },
      "source": [
        "> üí° All good!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8876b7f2b8d0"
      },
      "source": [
        "üß™ What about the slightly warped visuals?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d38123edf7b1"
      },
      "outputs": [],
      "source": [
        "restore_objects(Source.paintings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bcc51508359"
      },
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - Notice how, on the last painting, the orange bookmark is properly removed and the hidden part inpainted to complete the painting.\n",
        "> - We requested to \"fill the canvas with minimal uniform margins, without distortion or cropping\". Depending on the aspect ratio and type of the visual, this degree of freedom can result in different white margins.\n",
        "> - This example shows famous paintings by Vincent Van Gogh. Nano Banana does not fetch any reference images and only uses the provided input. If these were photos of private paintings, they would be restored in the same way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94155ba7de4a"
      },
      "source": [
        "In the Denver architecture book, the illustrations can be tilted, which our generic prompt does not fully take into account. When several geometric transformations are involved, it can be challenging to craft an imperative prompt that details all the operations to perform. Instead, a descriptive prompt can be more straightforward by directly describing the expected output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d405847bbe8"
      },
      "source": [
        "üß™ Here's an example of a descriptive prompt focusing on the restoration of tilted visuals:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19a5fe654818"
      },
      "outputs": [],
      "source": [
        "tilted_visual_prompt = \"\"\"\n",
        "An upright, high-fidelity rendition of the visual isolated against a pure white background, filling the canvas with minimal uniform margins. The output is clean, sharp, and free of physical artifacts.\n",
        "\"\"\"\n",
        "\n",
        "restore_objects(Source.denver_illustrated, tilted_visual_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c60b58013cdc"
      },
      "source": [
        "> üí° Remarks:\n",
        ">\n",
        "> - To get these results, the prompt focuses on requesting an \"upright\" visual \"filling the canvas\", which proves more straightforward to write than trying to account for all possible geometric corrections.\n",
        "> - The native visual understanding automatically identifies the content type (photo, illustration, etc.), the different artifacts (photographic, paper, printing, scanning‚Ä¶), allowing for precise restorations out of the box.\n",
        "> - Notice how the consistency is preserved: the last visual is restored as an illustration, while the first visuals maintain their photographic style.\n",
        "> - The results, with this rather generic prompt, are impressive. It is, of course, possible to be more specific and request particular lighting, styles, colors‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974f1e6d0489"
      },
      "source": [
        "In this last test, the input visual has distortions not only from the page curvature but also from the photo perspective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2329e1766dd"
      },
      "source": [
        "üß™ Here's an example of a descriptive prompt focusing on restoring warped illustrations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6a652d30523"
      },
      "outputs": [],
      "source": [
        "warped_visual_prompt = \"\"\"\n",
        "An edge-to-edge digital extraction of the illustration from the provided book photo, excluding any peripheral text. All page curvature and perspective distortions are corrected, resulting in an image framed in a perfect rectangle, on a pure white canvas with minimal margins.\n",
        "\"\"\"\n",
        "\n",
        "restore_objects(Source.alice_drawing, warped_visual_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60d8067698d6"
      },
      "source": [
        "> üí° It is really impressive that such a restoration can be performed in a single step. If you have complex transformations, test descriptive prompts iteratively, using precise and concise instructions, and you might be pleasantly surprised. In the worst case, it's also possible to process the transformations in successive, easier steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9609062b272"
      },
      "source": [
        "Now, let's add a colorization step‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8dc713f96a"
      },
      "source": [
        "---\n",
        "\n",
        "## üé® Colorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc8173bda257"
      },
      "source": [
        "Our restoration step respected the original styles of the input images. Recent image editing models excel at transforming image styles, starting with colors. This can generally be performed directly with a simple, precise instruction.\n",
        "\n",
        "Here is a possible colorization function using an imperative prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ce94660f7a3"
      },
      "outputs": [],
      "source": [
        "COLORIZATION_PROMPT = \"\"\"\n",
        "Colorize this image in a modern book illustration style, maintaining all original details without any additions.\n",
        "\"\"\"\n",
        "\n",
        "colorize = object_editing_function(\n",
        "    COLORIZATION_PROMPT,\n",
        "    WorkflowStep.RESTORED,\n",
        "    WorkflowStep.COLORIZED,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Colorization function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5efdf9f20189"
      },
      "source": [
        "üß™ Let's modernize our 1485 illustration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f75a8e7b3063"
      },
      "outputs": [],
      "source": [
        "colorize(Source.incunable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3090c86bdfbc"
      },
      "source": [
        "> üí° All details are preserved, as requested in the prompt. Notice how the colorization can naturally fix some remaining artifacts (e.g., the paper discoloration in the sword or the bleeding ink in the armor).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c497f1725583"
      },
      "source": [
        "üß™ Let's colorize our museum guidebook illustrations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ffe21fb7f45"
      },
      "outputs": [],
      "source": [
        "colorize(Source.museum_guidebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "042920286499"
      },
      "source": [
        "> üí° Our prompt is very open as it only specifies \"modern book illustration style\". This can generate very creative colorizations, but they all seem to make perfect sense.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf46a879527d"
      },
      "source": [
        "üß™ What about our Denver buildings?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55cc9b7017ce"
      },
      "outputs": [],
      "source": [
        "colorize(Source.denver_illustrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d019e2469f"
      },
      "source": [
        "> üí° As requested, they all look like modern illustrations, including the first visuals (originating from noisy photos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "509ec83149b6"
      },
      "source": [
        "It's possible to go further by not only \"colorizing\" but also \"transforming\" the image into a significantly different one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3b10db2267e"
      },
      "source": [
        "üß™ Let's make our \"Alice's Adventures in Wonderland\" drawing into a watercolor painting:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6538dd6e8a62"
      },
      "outputs": [],
      "source": [
        "watercolor_prompt = \"\"\"\n",
        "Transform this visual into a warm, watercolor painting.\n",
        "\"\"\"\n",
        "\n",
        "colorize(Source.alice_drawing, watercolor_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25bf687052ad"
      },
      "source": [
        "üß™ What about making it a traditional painting?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb1623aeec8b"
      },
      "outputs": [],
      "source": [
        "painting_prompt = \"\"\"\n",
        "Transform this visual into a traditional painting.\n",
        "\"\"\"\n",
        "\n",
        "colorize(Source.alice_drawing, painting_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66405678af28"
      },
      "source": [
        "We can also change image compositions. Depending on the context, some compositions are more or less implied by default. For example, illustrations often have margins while photos generally have edge-to-edge (full-bleed in the printing world) compositions. When possible, it's interesting to refer to a type of visual (which intrinsically brings a lot of semantics to the context) and adjust the instructions accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798e373a9cc7"
      },
      "source": [
        "üß™ Let's see how we can detect engravings in this 1847 book, restore them, and transform them into modern digital graphics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "827ae0c02cc0"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.engravings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dba84aa199cd"
      },
      "outputs": [],
      "source": [
        "restore_objects(Source.engravings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d384a569a625"
      },
      "outputs": [],
      "source": [
        "visual_to_digital_graphic_prompt = \"\"\"\n",
        "Transform this visual into a full-color, flat digital graphic, extending the content for a full-bleed effect.\n",
        "\"\"\"\n",
        "\n",
        "colorize(Source.engravings, visual_to_digital_graphic_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3062637f7caa"
      },
      "source": [
        "üß™ We can also transform the same engravings into photos with a very simple prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "defdf46cb9ac"
      },
      "outputs": [],
      "source": [
        "visual_to_photo_prompt = \"\"\"\n",
        "Transform this visual into a high-end, modern camera photograph.\n",
        "\"\"\"\n",
        "\n",
        "colorize(Source.engravings, visual_to_photo_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCSF2NwZn3cl"
      },
      "source": [
        "> üí° As photos are generally full-bleed, the prompt does not need to specify a composition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b794af370694"
      },
      "source": [
        "It's really up to our imagination, as Nano Banana seems to grasp every aspect of the visual semantics.\n",
        "\n",
        "Let's add a final step to see how far we can go, reimagining images as cinematic movie stills‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f8a44cb0b2e"
      },
      "source": [
        "---\n",
        "\n",
        "## üéûÔ∏è Cinematization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2081978e2401"
      },
      "source": [
        "We've used rather \"closed\" prompts so far, crafting specific instructions and constraints to control the outputs. It's possible to go even further with \"open\" prompts and generate images in full creative mode. Notably, it can be interesting to refer to photographic or cinematographic terminology as it encompasses many visual techniques.\n",
        "\n",
        "Here is a possible generic cinematization function to reimagine images as movie stills:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a221f656d756"
      },
      "outputs": [],
      "source": [
        "CINEMATIZATION_PROMPT = \"\"\"\n",
        "Reimagine this image as a joyful, modern live-action cinematic movie still featuring professional lighting and composition.\n",
        "\"\"\"\n",
        "\n",
        "cinematize = object_editing_function(\n",
        "    CINEMATIZATION_PROMPT,\n",
        "    WorkflowStep.RESTORED,\n",
        "    WorkflowStep.CINEMATIZED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b1e17985958"
      },
      "source": [
        "üß™ Let's cinematize the \"Alice's Adventures in Wonderland\" drawing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3084d59f7478"
      },
      "outputs": [],
      "source": [
        "cinematize(Source.alice_drawing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8afd6fd0ac"
      },
      "source": [
        "> üí° This looks like a high-budget movie still. There are lots of degrees of freedom in the prompt, but you're likely to get foreground figures in sharp focus, a gradual background blur, \"golden hour\" lighting (a magical ingredient for many cinematographers), and detailed textures. Such compositions really evoke different atmospheres compared to the photos generated in the previous test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f39c737673f7"
      },
      "source": [
        "üß™ Let's test the workflow on a page from the Wonderful Wizard of Oz containing three drawings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a9c211c6c4c"
      },
      "outputs": [],
      "source": [
        "detect_objects(Source.wizard_of_oz_drawings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dc600c03d88"
      },
      "outputs": [],
      "source": [
        "restore_objects(Source.wizard_of_oz_drawings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07653703d4a3"
      },
      "outputs": [],
      "source": [
        "cinematize(Source.wizard_of_oz_drawings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69797fb2dd6f"
      },
      "source": [
        "> üí° The cast for a new movie is ready ;)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01e80e72322c"
      },
      "source": [
        "Cinematic images have various use cases:\n",
        "\n",
        "- These cinematized stills can be perfect \"reference images\" for video generation models like Veo. See [Generate Veo videos from reference images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/use-reference-images-to-guide-video-generation#googlegenaisdk_videogen_with_image-python_genai_sdk).\n",
        "- As they are photorealistic representations, they can also be a source for generating 2D or 3D visuals, in any style, with realistic figures, perfect proportions, advanced lighting, enhanced compositions‚Ä¶\n",
        "- You can use them in many professional contexts or for high-end products: presentations, magazines, posters, storyboards, brainstorming sessions‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6f5cb8377a"
      },
      "source": [
        "---\n",
        "\n",
        "## üèÅ Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb910dd535b9"
      },
      "source": [
        "- Gemini's native spatial understanding enables the detection of specific visual objects based on a single prompt in natural language.\n",
        "- We tested the detection of illustrations in book photos, which traditional machine learning (ML) models usually miss, as they are typically trained to detect people, animals, vehicles, food, and a finite set of physical object classes.\n",
        "- We tested the detection of straight, tilted, and even significantly warped illustrations, and they were always precisely identified.\n",
        "- The core implementation was straightforward, requiring minimal code using the Python SDK and customized prompts. By comparison, fine-tuning a traditional object detection model is time-consuming: it involves assembling an image dataset, labeling objects, and managing training jobs.\n",
        "- This solution is very flexible: we could switch from detecting illustrations to electronic components, by adapting the prompt, while keeping the code unchanged.\n",
        "- Using structured outputs (with a JSON schema or Pydantic classes, and the Python SDK) makes the code both easy to implement and ready to deploy to production.\n",
        "- Then, Nano Banana allows editing these visual objects in virtually any way imaginable.\n",
        "- We tested a workflow with restoration, colorization, and even cinematization steps, using imperative and descriptive prompts.\n",
        "- The possibilities seem really endless, and the principles in this exploration can be reused in different contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2637033c091"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚ûï More!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "625892c50cfb"
      },
      "source": [
        "- Run this other Nano Banana notebook about [Generating Consistent Imagery](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/media-generation/consistent_imagery_generation.ipynb)\n",
        "- Check out the documentation page about [Image understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding)\n",
        "- Explore additional use cases in the [Vertex AI Prompt Gallery](https://console.cloud.google.com/vertex-ai/studio/prompt-gallery)\n",
        "- Stay updated by following the [Vertex AI Release Notes](https://cloud.google.com/vertex-ai/generative-ai/docs/release-notes)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "object_detection_and_editing.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
