{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Production & Terabyte Scale RAG Pipeline Using BigFrames\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fnotebook_template.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/notebook_template.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery Studio logo\"><br> Open in BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Authors | [Lorenzo Spataro](https://github.com/lspataroG)  [Elia Secchi](https://github.com/eliasecchig)  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook demonstrates how to use BigFrames and LangChain to build a RAG (Retrieval Augmented Generation) pipeline using Vertex AI. You will learn how to:\n",
        "- Load data from BigQuery into BigFrames\n",
        "- Create embeddings using Vertex AI models\n",
        "- Build a vector store using BigQuery\n",
        "- Create a RAG pipeline using LangChain\n",
        "- Query your data using natural language\n",
        "\n",
        "## What is BigQuery DataFrames?\n",
        "BigQuery DataFrames also called BigFrames lets you process data in BigQuery using familiar Python APIs like pandas and scikit-learn. It works by converting Python code into optimized SQL that runs directly in BigQuery.\n",
        "Key benefits:\n",
        "- Process terabytes of data using Python without moving it out of BigQuery\n",
        "- Train ML models directly in BigQuery using scikit-learn syntax\n",
        "- 750+ pandas/scikit-learn APIs available through SQL conversion\n",
        "- Lazy evaluation for better performance\n",
        "- Custom Python functions run as BigQuery remote functions\n",
        "- Vertex AI integration for Gemini model access\n",
        "\n",
        "The following diagram describes the workflow of BigQuery DataFrames:\n",
        "![BigQuery DataFrames Workflow](https://cloud.google.com/static/bigquery/images/dataframes-workflow.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "# Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "## Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform  \"bigframes\" langchain markdownify swifter \"langchain-google-community[featurestore]\" langchain-google-vertexai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "## Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. In Colab or Colab Enterprise, you might see an error message that says \"Your session crashed for an unknown reason.\" This is expected. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "from google.cloud import bigquery\n",
        "import bigframes.ml.llm as llm\n",
        "import bigframes.pandas as bpd\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "# GOOGLE_CLOUD_REGION must be in a US region because the source dataset is in US\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "bq_client = bigquery.Client(project=PROJECT_ID, location=\"US\")\n",
        "bpd.options.bigquery.project = PROJECT_ID\n",
        "bpd.options.bigquery.location = \"US\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6",
        "tags": []
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myuk3g_Jrvob",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Third-party imports\n",
        "import swifter\n",
        "from google.cloud import bigquery\n",
        "from langchain_google_community import BigQueryVectorStore\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from markdownify import markdownify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QNXi4qKrvob"
      },
      "source": [
        "### Variables definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx97BvG-qn1E"
      },
      "source": [
        "As we're building a pipeline intended to run regularly on a schedule, we're going to set up some time-dependent variables:\n",
        "\n",
        "*   `RUN_DATE`: The date the process runs\n",
        "*   `IS_INCREMENTAL`: If `True` only query recent data; otherwise, query the whole dataset\n",
        "*   `LOOK_BACK_DAYS`: If `IS_INCREMENTAL=True`, defines how many days in the past to query\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB7Zf9WCrvob",
        "tags": []
      },
      "outputs": [],
      "source": [
        "IS_INCREMENTAL = True  # Flag to enable incremental processing\n",
        "RUN_DATE = datetime.strptime(\"2022-09-26\", '%Y-%m-%d').date()  # Set as the last date of the dataset for demonstration purposes (ie. there is no data after that)\n",
        "LOOK_BACK_DAYS = 1  # Number of days to look back from RUN_DATE\n",
        "START_DATE = str(RUN_DATE - timedelta(days=LOOK_BACK_DAYS))  # Start date for data processing window\n",
        "END_DATE = str(RUN_DATE)  # End date for data processing window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZRyBMJprvob"
      },
      "source": [
        "## 1. Data Loading and Initial Analysis\n",
        "This section retrieves and examines StackOverflow Python Q&A data from the public BigQuery table `production-ai-template.stackoverflow_qa.stackoverflow_python_questions_and_answers`. The data comes from the official [Stack Overflow public dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow) and contains a sample of Python-related questions and their corresponding answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70vCCTMmrvob",
        "tags": []
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "    SELECT\n",
        "        creation_date,\n",
        "        last_edit_date,\n",
        "        question_id,\n",
        "        question_title,\n",
        "        question_body AS question_text,\n",
        "        answers\n",
        "    FROM `production-ai-template.stackoverflow_qa.stackoverflow_python_questions_and_answers`\n",
        "    WHERE TRUE\n",
        "        # If IS_INCREMENTAL is True, filter records between START_DATE and END_DATE\n",
        "        # Otherwise, include all records without date filtering\n",
        "        {f'AND TIMESTAMP_TRUNC(creation_date, DAY) BETWEEN TIMESTAMP(\"{START_DATE}\") AND TIMESTAMP(\"{END_DATE}\")' if IS_INCREMENTAL else ''}\n",
        "\"\"\"\n",
        "df = bpd.read_gbq(query)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xpXwWB7rvoc"
      },
      "source": [
        "## 2. Data Cleaning and Markdown Conversion\n",
        "In this step, we clean the raw data by converting HTML content to Markdown format for better readability and processing.\n",
        "We transform both questions and answers from HTML to Markdown, structure the content with proper headings,\n",
        "and combine them into a unified text format that will be easier to work with in subsequent steps.\n",
        "\n",
        "> Note: In this case, we are leveraging BigFrames' capability to pull data into memory, where the processing happens. This allows us to efficiently transform and clean the data using pandas-like operations. Later, we will demonstrate how to scale this data processing using Remote Functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjauyZUrvoc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def convert_html_to_markdown(html: str) -> str:\n",
        "    \"\"\"Convert HTML into Markdown for easier parsing and rendering after LLM response.\"\"\"\n",
        "    return markdownify(html).strip()\n",
        "\n",
        "def create_answers_markdown(answers: list[dict]) -> str:\n",
        "    \"\"\"Convert each answer's HTML to markdown and concatenate into a single markdown text.\"\"\"\n",
        "    answers_md = \"\"\n",
        "    for index, answer_record in enumerate(answers):\n",
        "        answers_md += f\"\\n\\n## Answer {index + 1}:\\n\" # Answer number is H2 heading size\n",
        "        answers_md += convert_html_to_markdown(answer_record[\"body\"])\n",
        "    return answers_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHuxypiPrvoc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Sort, deduplicate and reset index in one operation\n",
        "df = df.sort_values(\"last_edit_date\", ascending=False).drop_duplicates(\"question_id\").reset_index(drop=True)\n",
        "\n",
        "# Create markdown fields efficiently\n",
        "df[\"question_title_md\"] = \"# \" + df[\"question_title\"] + \"\\n\"  # Title is H1 heading size\n",
        "df[\"question_text_md\"] = df[\"question_text\"].to_pandas().apply(convert_html_to_markdown)\n",
        "df[\"answers_md\"] = df[\"answers\"].to_pandas().apply(create_answers_markdown)\n",
        "\n",
        "# Load data back in BigQuery\n",
        "df[\"full_text_md\"] = df[\"question_title_md\"] + df[\"question_text_md\"] + df[\"answers_md\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f06xaGIRrvoc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Select final columns for the cleaned dataset\n",
        "final_cols = [\"last_edit_date\", \"question_id\", \"question_text\", \"full_text_md\"]\n",
        "df = df[final_cols]\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilb9nQzLrvoc"
      },
      "source": [
        "## 2. Text Chunking\n",
        "The text data is in Markdown format, requiring a thoughtful chunking approach. While we currently use a basic character-based splitter, production systems typically employ more sophisticated techniques:\n",
        "\n",
        "- Preserve semantic units like paragraphs and sections\n",
        "- Maintain markdown structure and hierarchy\n",
        "- Keep related content together (e.g., questions with their answers)\n",
        "- Use overlapping chunks to maintain context across boundaries\n",
        "- Consider special markdown elements like code blocks and lists\n",
        "\n",
        "This helps ensure chunks remain coherent and meaningful for downstream tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z0mITgYrvoc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lslvpk7Qrvoc"
      },
      "source": [
        "Apply text chunking to each document locally using pandas and swifter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEUabTuTrvoc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df[\"text_chunk\"] = df[\"full_text_md\"].to_pandas().astype(object).swifter.apply(text_splitter.split_text)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "P7aiNG-cFC8y"
      },
      "outputs": [],
      "source": [
        "# compute the sequential index of a chunk within the list of chunks for each question\n",
        "chunk_ids = [str(idx) for text_chunk in df[\"text_chunk\"] for idx in range(len(text_chunk))]\n",
        "# Explode the chunk list so that we get a row per chunk\n",
        "df = df.explode(\"text_chunk\").reset_index(drop=True)\n",
        "# assigning the chunk_id as question_id + sequential index of the chunk\n",
        "df[\"chunk_id\"] = df[\"question_id\"].astype(\"string\") + \"__\" + chunk_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q046iLWsrvod",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeOhgIdzrvod"
      },
      "source": [
        "## 3. Embedding\n",
        "\n",
        "To generate embeddings, we leverage the seamless integration between BigFrames, BigQuery, and Vertex AI.\n",
        "This integration allows us to efficiently generate embeddings through Vertex AI's batch scoring process.\n",
        "The text-embedding-005 model converts each text chunk into a high-dimensional vector representation,\n",
        "enabling semantic search and similarity analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7UZ2aXervod",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize the embedding model\n",
        "embedder = llm.TextEmbeddingGenerator(model_name=\"text-embedding-005\")\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings_df = embedder.predict(df[\"text_chunk\"])\n",
        "df = df.assign(\n",
        "    embedding_result=embeddings_df[\"ml_generate_embedding_result\"],\n",
        "    embedding_statistics=embeddings_df[\"ml_generate_embedding_statistics\"],\n",
        "    embedding_status=embeddings_df[\"ml_generate_embedding_status\"]\n",
        ")\n",
        "current_timestamp = datetime.now()\n",
        "df[\"creation_timestamp\"] = current_timestamp\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhGB484Nrvod"
      },
      "source": [
        "We can now notice 4 new columns added to our DataFrame!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7NwgaQmrvod"
      },
      "source": [
        "## Saving results\n",
        "\n",
        "We are now ready to save the results of the processing to a BigQuery table, for consumption by the different Vector DBs we might want to use.\n",
        "The incremental writing strategy allows us to efficiently update our embeddings table by:\n",
        "1. Only processing new/modified questions since last run (controlled by `IS_INCREMENTAL` flag)\n",
        "2. Appending new embeddings to the existing table when `IS_INCREMENTAL=True`\n",
        "3. Replacing the entire table when `IS_INCREMENTAL=False`\n",
        "\n",
        "Since we may end up with duplicate entries when doing incremental updates\n",
        "(e.g. if a question was modified multiple times), we'll need to deduplicate\n",
        "the table afterwards to keep only the latest version of each question.\n",
        "The deduplication will be done based on question_id, keeping the row with the most recent `creation_timestamp`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZa-KlYrvod",
        "tags": []
      },
      "outputs": [],
      "source": [
        "DESTINATION_DATASET_ID = \"stackoverflow_data\"\n",
        "DESTINATION_TABLE_ID = \"incremental_questions_embeddings\"\n",
        "PARTITION_DATE_COLUMN = \"creation_timestamp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rX7Ckehrvod"
      },
      "source": [
        "### If it doesn't exist, let's create an empty table with partitioning and the right schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xdWlDzgAFC8y"
      },
      "outputs": [],
      "source": [
        "def create_table_if_not_exist(df, project_id, dataset_id, table_id, partition_column, location=\"US\"):\n",
        "    table_schema = bq_client.get_table(df.head(0).to_gbq()).schema\n",
        "\n",
        "    # Create table schema with partitioning\n",
        "    table = bigquery.Table(f\"{project_id}.{dataset_id}.{table_id}\", schema=table_schema)\n",
        "    table.time_partitioning = bigquery.TimePartitioning(\n",
        "        type_=bigquery.TimePartitioningType.DAY,\n",
        "        field=partition_column\n",
        "    )\n",
        "\n",
        "    dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
        "    dataset.location = location\n",
        "    bq_client.create_dataset(dataset, exists_ok=True)\n",
        "    table = bq_client.create_table(table=table, exists_ok=True)\n",
        "\n",
        "create_table_if_not_exist(\n",
        "    df=df,\n",
        "    project_id=PROJECT_ID,\n",
        "    dataset_id=DESTINATION_DATASET_ID,\n",
        "    table_id=DESTINATION_TABLE_ID,\n",
        "    partition_column=PARTITION_DATE_COLUMN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAyS5F6trvod",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# If IS_INCREMENTAL is True, append new data to existing table\n",
        "# If IS_INCREMENTAL is False, replace entire table with new data\n",
        "if_exists_mode = \"append\" if IS_INCREMENTAL else \"replace\"\n",
        "\n",
        "incremental_table_id = df.to_gbq(\n",
        "    destination_table=f\"{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}\",\n",
        "    if_exists=if_exists_mode\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIOzoRNHrvoe"
      },
      "source": [
        "## Create a new Dedup table (Optional)\n",
        "\n",
        "If necessary, we can create a deduplication table to address duplicate questions that may appear in the dataset across different dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3_6VklCMFC83"
      },
      "outputs": [],
      "source": [
        "df_questions = bpd.read_gbq(f\"{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}\", use_cache=False)\n",
        "max_date_df = df_questions.groupby(\"question_id\")[\"creation_timestamp\"].max().reset_index()\n",
        "df_questions_dedup = max_date_df.merge(df_questions, how=\"inner\", on=[\"question_id\", \"creation_timestamp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gazEjsfVFC83"
      },
      "outputs": [],
      "source": [
        "DESTINATION_DEDUPED_QUESTIONS_TABLE_ID = \"questions_embeddings\"\n",
        "create_table_if_not_exist(\n",
        "    df=df_questions_dedup,\n",
        "    project_id=PROJECT_ID,\n",
        "    dataset_id=DESTINATION_DATASET_ID,\n",
        "    table_id=DESTINATION_DEDUPED_QUESTIONS_TABLE_ID,\n",
        "    partition_column=PARTITION_DATE_COLUMN\n",
        ")\n",
        "\n",
        "deduped_table_id = df_questions_dedup.to_gbq(\n",
        "    destination_table=f\"{DESTINATION_DATASET_ID}.{DESTINATION_DEDUPED_QUESTIONS_TABLE_ID}\",\n",
        "    if_exists=\"replace\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CwdVwyQrvoj"
      },
      "source": [
        "## Testing retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VQiUuGkrvoj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "embedding_model = VertexAIEmbeddings(\n",
        "    model_name=\"text-embedding-005\", project=PROJECT_ID\n",
        ")\n",
        "bq_store = BigQueryVectorStore(\n",
        "    project_id=PROJECT_ID,\n",
        "    location=\"US\",\n",
        "    dataset_name=DESTINATION_DATASET_ID,\n",
        "    table_name=DESTINATION_DEDUPED_QUESTIONS_TABLE_ID,\n",
        "    embedding=embedding_model,\n",
        "    embedding_field=\"embedding_result\",\n",
        "    content_field=\"text_chunk\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0fvQQwKrvoj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "bq_store.similarity_search(\"how to pull csv files?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2o86ME-rvoj"
      },
      "source": [
        "## Scaling to Terabytes: BigFrame Remote Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unwXv4-Urvoj"
      },
      "source": [
        "Sometimes data is too large to run local process when running custom python functions.\n",
        "In fact, everytime we convert a series or a dataframe to pandas using `to_pandas()`, the data is loaded into memory.\n",
        "\n",
        "To be able to run large datasets processes remotely, we can define remote <b>UDF functions</b>. Let's see an example of that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ap_IicZFC84"
      },
      "outputs": [],
      "source": [
        "import bigframes.pandas\n",
        "import bigframes.bigquery as bbq\n",
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=10,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "# Create UDF for chunking\n",
        "# Behind the scenes, BigFrames will automatically create a connection for you but you can also create a dedicated connection.\n",
        "# See here: https://cloud.google.com/bigquery/docs/remote-functions#create_a_connection.\n",
        "@bigframes.pandas.remote_function(packages=[\"langchain\"], reuse=True)\n",
        "def chunk_text_udf(text: str) -> str:\n",
        "    return json.dumps([chunk.page_content for chunk in text_splitter.create_documents([text])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWTS6Oz5FC84"
      },
      "source": [
        "We are using the UDF to chunk the data and return a list of text chunks. Since BigFrames UDFs expect in input and in output simple types, we are going to convert the list of chunks to a json string inside the UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzqyoMtXrvoj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_udf = bpd.read_gbq(f\"{DESTINATION_DATASET_ID}.{DESTINATION_DEDUPED_QUESTIONS_TABLE_ID}\", use_cache=False)\n",
        "df_udf[\"question_text\"] = df_udf[\"question_text\"].apply(chunk_text_udf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0McPCZQFC84"
      },
      "source": [
        "As we can see, the data type is now string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPSE7IR-rvok",
        "tags": []
      },
      "outputs": [],
      "source": [
        "first_row_question = df_udf[\"question_text\"].iloc[0]\n",
        "print(type(first_row_question))\n",
        "first_row_question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rusk8ojbtEl0"
      },
      "source": [
        "To fix that, we can now use the BigFrames BQ `json_extract_string_array` method to convert the json string back to a list of string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npMj9jbhrvok",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_udf[\"question_text\"] = bbq.json_extract_string_array(df_udf[\"question_text\"])\n",
        "\n",
        "first_row_question = df_udf[\"question_text\"].iloc[0]\n",
        "print(type(first_row_question))\n",
        "first_row_question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2E5pSvtrvok"
      },
      "source": [
        "Let's print out the result. The remote function will be only executed now we ask to print it following a lazy execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad",
        "tags": []
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "Clean up resources created in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3I3SqYIuFC84"
      },
      "outputs": [],
      "source": [
        "dataset = f\"{PROJECT_ID}.{DESTINATION_DATASET_ID}\"\n",
        "dataset_object = bigquery.Dataset(dataset)\n",
        "bq_client.delete_dataset(dataset_object, delete_contents=True, not_found_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook showcased the power of **BigFrames** for building production-ready RAG pipelines on Google Cloud. We leveraged BigFrames' seamless integration with BigQuery and Vertex AI to efficiently process and embed large text datasets.\n",
        "\n",
        "Key takeaways highlighting BigFrames' capabilities include:\n",
        "\n",
        "*   **Scalable Data Processing:**  BigFrames allowed us to manipulate BigQuery data using familiar pandas-like syntax, whether processing in memory or through scalable remote functions for terabyte-scale datasets.\n",
        "*   **Simplified Embedding Generation:** BigFrames made it easy to generate embeddings with Vertex AI's embedding models directly within our data pipeline.\n",
        "*   **Efficient Data Management:** We used BigFrames to manage our embeddings in BigQuery, implementing incremental updates and deduplication for optimal performance."
      ],
      "metadata": {
        "id": "I_-3K9iMIjLU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m126",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}