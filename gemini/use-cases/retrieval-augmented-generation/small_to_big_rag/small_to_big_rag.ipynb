{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small-to-big Retrieval-Augmented Generation \n",
    "\n",
    "Keith Ballinger and Megan O'Keefe, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small-to-big RAG is a form of recursive RAG, where small chunks are connected to larger chunks (eg. a passage of a document is connected to the full document). Initially, only small chunks are retrieved during RAG. If needed, the model can request additional context, at which point the larger context can be added.\n",
    "\n",
    "Small-to-big RAG is a way to work around the limits of dense, meaning-rich vector embeddings, where passing in too much context can result in a loss of meeting in the vector representation.  \n",
    "(Text-embedding-gecko has a text input limit of 3,072 tokens)  \n",
    "\n",
    "Small-to-big is also a good option if you are working with long or complex documents that require context exceeding the ideal chunk size. \n",
    "\n",
    "Lastly, Small-to-big can help you optimize inference costs, especially if you're paying per token (eg. Gemini API on Vertex AI) ‚Äî because if a small amount of input context is enough for the model to generate an answer, you don't have to provide the whole large document as context. Said another way - you're only paying for the number of context tokens you actually need to generate an accurate response from Gemini.\n",
    "\n",
    "In this example, we'll walk through a Small-to-big RAG example using a GitHub codebase called [Online Boutique](https://github.com/GoogleCloudPlatform/microservices-demo). This is a microservices, polyglot sample application. We'll help a new Online Boutique user (or contributor) navigate this large codebase by providing a Q&A chatbot functionality, grounded on the codebase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites:\n",
    "- Google Cloud account\n",
    "- Google Cloud project with billing enabled \n",
    "- Enable the Vertex AI API\n",
    "\n",
    "This notebook uses the following products and tools:\n",
    "- Vertex AI - Gemini API \n",
    "- Vertex AI - Text Embeddings API \n",
    "- Chroma (in-memory vector database)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL=\"textembedding-gecko@003\"\n",
    "GENERATIVE_MODEL=\"gemini-1.0-pro\"\n",
    "PROJECT_ID=\"mokeefe-genai-test\"\n",
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"google-cloud-aiplatform>=1.38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.generative_models import GenerativeModel, ChatSession\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(doc) -> list:\n",
    "    model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL)\n",
    "    embeddings = model.get_embeddings([doc])\n",
    "    if len(embeddings) > 1:\n",
    "        raise ValueError(\"More than one embedding returned.\")\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"No embedding returned.\")\n",
    "    return embeddings[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "model = GenerativeModel(GENERATIVE_MODEL)\n",
    "chat = model.start_chat()\n",
    "\n",
    "def gemini_inference(chat: ChatSession, prompt: str) -> str:\n",
    "    text_response = []\n",
    "    responses = chat.send_message(prompt, stream=False)\n",
    "    for chunk in responses:\n",
    "        text_response.append(chunk.text)\n",
    "    return \"\".join(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code sets up a logging mechanism using the Logrus library. It configures the logging level to debug, uses a custom JSONFormatter with specific field names and timestamps, and sets the logging output to the standard output stream.\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "func init() {\n",
    "\tlog = logrus.New()\n",
    "\tlog.Level = logrus.DebugLevel\n",
    "\tlog.Formatter = &logrus.JSONFormatter{\n",
    "\t\tFieldMap: logrus.FieldMap{\n",
    "\t\t\tlogrus.FieldKeyTime:  \"timestamp\",\n",
    "\t\t\tlogrus.FieldKeyLevel: \"severity\",\n",
    "\t\t\tlogrus.FieldKeyMsg:   \"message\",\n",
    "\t\t},\n",
    "\t\tTimestampFormat: time.RFC3339Nano,\n",
    "\t}\n",
    "\tlog.Out = os.Stdout\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "Summarize the following code: \n",
    "{}\n",
    "\"\"\".format(code)\n",
    "result = gemini_inference(chat, prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get summaries of code files \n",
    "\n",
    "First, we'll use Gemini on Vertex AI to get short summaries of each code file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  checkoutservice.go\n",
      "Processing file:  shippingservice.go\n",
      "Processing file:  adservice.java\n",
      "Processing file:  paymentservice.js\n",
      "Processing file:  recommendationservice.py\n",
      "Processing file:  emailservice.py\n",
      "Processing file:  frontend.go\n",
      "Processing file:  productcatalog.go\n",
      "Processing file:  cartservice.cs\n",
      "Processing file:  currencyservice.js\n"
     ]
    }
   ],
   "source": [
    "# for every file in onlineboutique-codefiles/, read it in, and get a summary \n",
    "summaries = {} \n",
    "for file in os.listdir(\"onlineboutique-codefiles/\"):\n",
    "    temp = {}\n",
    "    with open(\"onlineboutique-codefiles/\" + file, \"r\") as f:\n",
    "        print(\"Processing file: \", file)\n",
    "        content = f.read()     \n",
    "        temp[\"content\"] = content \n",
    "        prompt = \"\"\" \n",
    "        You are a helpful code summarizer. Here is a source code file. Please identify the programming language and summarize it in three sentences or less. Give as much detail as possible, including function names and libraries used. Code: \n",
    "        {}\n",
    "        \"\"\".format(content)\n",
    "        try: \n",
    "            summary = gemini_inference(chat, prompt)\n",
    "            temp[\"summary\"] = summary\n",
    "            summaries[file] = temp\n",
    "        except Exception as e:\n",
    "            print(\"Error processing file: \", file)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =  pd.DataFrame.from_dict(summaries, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkoutservice.go</td>\n",
       "      <td>// Copyright 2018 Google LLC\\n//\\n// Licensed ...</td>\n",
       "      <td>This Go program implements an order checkout m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shippingservice.go</td>\n",
       "      <td>// Copyright 2018 Google LLC\\n//\\n// Licensed ...</td>\n",
       "      <td>This Go program implements a shipping service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adservice.java</td>\n",
       "      <td>/*\\n * Copyright 2018, Google LLC.\\n *\\n * Lic...</td>\n",
       "      <td>This Java program implements an ad service usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paymentservice.js</td>\n",
       "      <td>// Copyright 2018 Google LLC\\n//\\n// Licensed ...</td>\n",
       "      <td>This JavaScript code defines a class `HipsterS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recommendationservice.py</td>\n",
       "      <td>#!/usr/bin/python\\n#\\n# Copyright 2018 Google ...</td>\n",
       "      <td>This Python code implements a gRPC server for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  \\\n",
       "0        checkoutservice.go   \n",
       "1        shippingservice.go   \n",
       "2            adservice.java   \n",
       "3         paymentservice.js   \n",
       "4  recommendationservice.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  // Copyright 2018 Google LLC\\n//\\n// Licensed ...   \n",
       "1  // Copyright 2018 Google LLC\\n//\\n// Licensed ...   \n",
       "2  /*\\n * Copyright 2018, Google LLC.\\n *\\n * Lic...   \n",
       "3  // Copyright 2018 Google LLC\\n//\\n// Licensed ...   \n",
       "4  #!/usr/bin/python\\n#\\n# Copyright 2018 Google ...   \n",
       "\n",
       "                                             summary  \n",
       "0  This Go program implements an order checkout m...  \n",
       "1  This Go program implements a shipping service ...  \n",
       "2  This Java program implements an ad service usi...  \n",
       "3  This JavaScript code defines a class `HipsterS...  \n",
       "4  This Python code implements a gRPC server for ...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first column should be named \"filename\"\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns = {'index':'filename'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "df.to_csv(\"code_summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert summaries to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chromadb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"code_summaries2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate over dataframe. convert summary into embeddings. insert summary into collection. \n",
    "for index, row in df.iterrows():\n",
    "    fn = row[\"filename\"]\n",
    "    print(\"Getting embedding for: \", fn)\n",
    "    summ = row[\"summary\"] \n",
    "    print(summ)\n",
    "    e = get_text_embedding(summ) \n",
    "    print(e)\n",
    "    # add vector embedding to in-memory Chroma database. \n",
    "    # the \"small\" summary embedding is linked to the \"big\" raw code file through the metadata key, \"filename.\" \n",
    "    collection.add(\n",
    "        embeddings=[e],\n",
    "        documents=[summ],\n",
    "        metadatas=[{\"filename\": fn}],\n",
    "        ids=[fn])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the Small-to-big RAG workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_to_big(user_prompt):\n",
    "    # SMALL: first, run RAG with the summary embeddings to try to get a response\n",
    "    query_emb = get_text_embedding(user_prompt)\n",
    "    result = collection.query(\n",
    "        query_embeddings=[query_emb], \n",
    "        n_results=1\n",
    "    )\n",
    "        \n",
    "    # process results into something we can pass to Gemini \n",
    "    processed_result = {}\n",
    "    d = result[\"documents\"][0]\n",
    "    for i in range(0, len(d)):\n",
    "        summary = d[i]\n",
    "        filename = result[\"metadatas\"][0][i][\"filename\"]\n",
    "        processed_result[filename] = summary\n",
    "  \n",
    "    prompt_with_small = \"\"\"\n",
    "    You are a codebase helper. You will be given a user's question about the codebase, along with \n",
    "    summaries of relevant code files. Do your best to generate an answer with the short summaries.  \n",
    "    If you feel confident in your answer, respond with \"The answer is:\" followed by your answer.\n",
    "    If you need more information, respond with \"Need full context,\" and you will be prompted again with the \n",
    "    full relevant code file. \n",
    "\n",
    "    The user query is: {} \n",
    "\n",
    "    The summaries are: {}\n",
    "    \"\"\".format(user_prompt, str(processed_result))\n",
    "    print(prompt_with_small)\n",
    "    small_result = gemini_inference(chat, prompt_with_small) \n",
    "    print(\"üêù SMALL RESULT: \" + small_result)\n",
    "    # we're done if Gemini is confident with just the summaries as context... \n",
    "    if \"need full context\" not in small_result.lower():\n",
    "        return \"Gemini responded with just the small summaries as context: \" + small_result + \" üéâ\"\n",
    "        \n",
    "    # otherwise, move on to BIG: \n",
    "    # IF we need the full context, get the filename that most closely matches the user's question\n",
    "    prompt_to_get_filename = \"\"\" \n",
    "    You are a codebase helper. The list of code files that you know about: \n",
    "    - checkoutservice.go\n",
    "    - shippingservice.go\n",
    "    - adservice.java\n",
    "    - paymentservice.js\n",
    "    - recommendationservice.py\n",
    "    - emailservice.py\n",
    "    - frontend.go\n",
    "    - productcatalog.go\n",
    "    - cartservice.cs\n",
    "    - currencyservice.js\n",
    "\n",
    "    The user asks the following question about the codebase: {}\n",
    "\n",
    "    Please respond with the filename that most closely matches the user's question. Respond with ONLY the filename. \n",
    "    \"\"\".format(user_prompt)\n",
    "    filename = gemini_inference(chat, prompt_to_get_filename)\n",
    "    print(\"Prompted for a specific filename, Gemini said: \" + filename)\n",
    "    # is the filename in the dataframe? \n",
    "    if filename not in df[\"filename\"].values:\n",
    "        return \"‚ö†Ô∏è Error: filename {} not found in dataframe\".format(filename)\n",
    "    \n",
    "    # get the full code file \n",
    "    full_code = df[df[\"filename\"] == filename][\"content\"].values[0]\n",
    "    prompt_with_big = \"\"\" \n",
    "    You are a codebase helper. You will be given a user's question about the codebase, along with a complete source code file. Respond to the user's question with as much detail as possible.\n",
    "\n",
    "    The user query is: {}\n",
    "    \n",
    "    The full code file is: {}\n",
    "    \"\"\".format(user_prompt, full_code) \n",
    "\n",
    "    return gemini_inference(chat, prompt_with_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a codebase helper. You will be given a user's question about the codebase, along with \n",
      "    summaries of relevant code files. Do your best to generate an answer with the short summaries.  \n",
      "    If you feel confident in your answer, respond with \"The answer is:\" followed by your answer.\n",
      "    If you need more information, respond with \"Need full context,\" and you will be prompted again with the \n",
      "    full relevant code file. \n",
      "\n",
      "    The user query is: How does the ad service work? \n",
      "\n",
      "    The summaries are: {'adservice.java': 'This JavaScript code sets up a gRPC server for a currency conversion service using the `@grpc/grpc-js` library. It includes functions for:\\n- Initializing OpenTelemetry tracing (if enabled) and Stackdriver Profiler (if enabled).\\n- Loading protocol buffer definitions and adding corresponding service definitions to the gRPC server.\\n- Handling requests for supported currencies and currency conversions.\\n- Handling health checks.\\n- Starting the gRPC server on a specified port.'}\n",
      "    \n",
      "üêù SMALL RESULT: The Ad service retrieves ads for a user, targeted based on the context provided in the request. If no context is provided, it serves random ads. It uses a Guava library for data structures and collections. The main class `AdService` starts and stops the gRPC server and blocks the main thread until server shutdown to prevent daemon threads from terminating the program prematurely. It also includes methods for retrieving ads by category or serving random ads if no category is specified. Additionally, it initializes OpenTelemetry stats and tracing (if enabled). The `AdServiceImpl` class implements the `AdServiceGrpc.AdServiceImplBase` interface, which defines the `getAds` method that processes ad requests and returns an `AdResponse` with a list of ads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gemini responded with just the small summaries as context: The Ad service retrieves ads for a user, targeted based on the context provided in the request. If no context is provided, it serves random ads. It uses a Guava library for data structures and collections. The main class `AdService` starts and stops the gRPC server and blocks the main thread until server shutdown to prevent daemon threads from terminating the program prematurely. It also includes methods for retrieving ads by category or serving random ads if no category is specified. Additionally, it initializes OpenTelemetry stats and tracing (if enabled). The `AdServiceImpl` class implements the `AdServiceGrpc.AdServiceImplBase` interface, which defines the `getAds` method that processes ad requests and returns an `AdResponse` with a list of ads. üéâ'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of a query where only the small (summary) step is needed\n",
    "small_to_big(\"How does the ad service work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a codebase helper. You will be given a user's question about the codebase, along with \n",
      "    summaries of relevant code files. Do your best to generate an answer with the short summaries.  \n",
      "    If you feel confident in your answer, respond with \"The answer is:\" followed by your answer.\n",
      "    If you need more information, respond with \"Need full context,\" and you will be prompted again with the \n",
      "    full relevant code file. \n",
      "\n",
      "    The user query is: exactly how does the SearchProducts() function work in the product catalog service? \n",
      "\n",
      "    The summaries are: {'productcatalog.go': 'This JavaScript code sets up a gRPC server for a currency conversion service using the `@grpc/grpc-js` library. It includes functions for:\\n- Initializing OpenTelemetry tracing (if enabled) and Stackdriver Profiler (if enabled).\\n- Loading protocol buffer definitions and adding corresponding service definitions to the gRPC server.\\n- Handling requests for supported currencies and currency conversions.\\n- Handling health checks.\\n- Starting the gRPC server on a specified port.'}\n",
      "    \n",
      "üêù SMALL RESULT: Need full context.\n",
      "Prompted for a specific filename, Gemini said: productcatalog.go\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The `SearchProducts` function in the product catalog service takes a `SearchProductsRequest` that contains a query string and returns a `SearchProductsResponse` containing a list of products that match the query. The function first sleeps for a specified latency period to simulate network latency. It then iterates through the catalog of products and checks if the product name or description contains the query string in a case-insensitive manner. If a match is found, the product is added to the list of results. Finally, the function returns the list of results.'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of a detailed query that requires the full code file (big)\n",
    "small_to_big(\"exactly how does the SearchProducts() function work in the product catalog service?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the solution snippet of productcatalog.go ^ matches Gemini's response.  \n",
    "\"\"\"\n",
    "func (p *productCatalog) SearchProducts(ctx context.Context, req *pb.SearchProductsRequest) (*pb.SearchProductsResponse, error) {\n",
    "\ttime.Sleep(extraLatency)\n",
    "\n",
    "\tvar ps []*pb.Product\n",
    "\tfor _, product := range p.parseCatalog() {\n",
    "\t\tif strings.Contains(strings.ToLower(product.Name), strings.ToLower(req.Query)) ||\n",
    "\t\t\tstrings.Contains(strings.ToLower(product.Description), strings.ToLower(req.Query)) {\n",
    "\t\t\tps = append(ps, product)\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\treturn &pb.SearchProductsResponse{Results: ps}, nil\n",
    "}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
