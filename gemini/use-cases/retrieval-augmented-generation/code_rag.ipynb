{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Code RAG - Reuse your already created codebase to generate more code\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2Fcode_rag.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Keshav Kumar](https://github.com/solidate)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blqUObshCGtW"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Code RAG (Retrieval-Augmented Generation) is a good technique that can help boost developer productivity. It can help by enhancing the development experience and reduce technical debt from codebases by learning patterns from already written code and suggesting reusable code snippets for the current tasks at hand.\n",
        "\n",
        "Code RAG leverages the already indexed code to either complete or generate code based on user inputs.\n",
        "\n",
        "The notebook uses following Google Cloud ML services and resources:\n",
        "  - **Gemini:** [Gemini](https://ai.google.dev/models/gemini) is a family of generative AI models that lets developers generate content and solve problems. These models are designed and trained to handle both text and images as input.\n",
        "  - **Gemini 1.0 Pro model (gemini-1.0-pro):** Designed to handle natural language tasks, multi-turn text and code chat, and code generation.\n",
        "  - **Code Bison model (code-bison-32k@002):** Designed to handle code related task like code generation, unit test generation, code refactoring etc.\n",
        "\n",
        "- **LangChain:** [LangChain](https://www.langchain.com/) is a framework designed to make integration of Large Language Models (LLM) like Gemini easier for applications.\n",
        "\n",
        "- **Chroma DB:** [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) is the open-source embedding database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs.\n",
        "\n",
        "- **Vertex AI Embeddings for Text:** With [textembedding-gecko](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings) models we can easily create a text embedding with LLM. *textembedding-gecko@003* is the newest stable embedding model.\n",
        "\n",
        "\n",
        "For more information, see the [Generative AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) on Vertex AI documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Objective\n",
        "\n",
        "\n",
        "This notebook utilizes the Google Vertex AI Generative capabilities for performing RAG (Retrieval-Augmented Generation) with code.\n",
        "\n",
        "This notebook uses [LangChain](https://python.langchain.com/docs/get_started/introduction.html) for operationalize workflow and uses [Chroma DB](https://docs.trychroma.com/) for persisting Vertex AI embeddings for the code snippets for similarity search.\n",
        "\n",
        "[Tree Sitter](https://github.com/tree-sitter) is used for code parsing. `Tree Sitter` is a parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited.\n",
        "\n",
        "The code repository used as an example is [Python Fire Repository](https://github.com/google/python-fire) 🔥, which is a tool for automatically generating command line interfaces (CLIs) from absolutely any Python object.\n",
        "This Repository is being used as code knowledge base that is used along with Vertex AI Models like Gemini to provide code completion and code generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A-RplAYaZqM"
      },
      "source": [
        "# Architecture outlining the workflow\n",
        "\n",
        "![code-rag-architecture](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/rag/code_rag/code-rag-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user bigframes==0.25 \\\n",
        "tree-sitter==0.20.1 \\\n",
        "loguru==0.7.2\\\n",
        "chromadb==0.4.22 \\\n",
        "google-cloud-aiplatform==1.45.0 \\\n",
        "langchain_google_vertexai==0.1.2 \\\n",
        "langchain==0.1.12 \\\n",
        "gradio==3.41.2 \\\n",
        "--quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjjMlNjGKbmK"
      },
      "source": [
        "## Clone the tree sitter grammar from github\n",
        "\n",
        "A `Tree Sitter` grammar is composed of a set of rules - objects that describe how syntax nodes can be composed from other syntax nodes. There are several types of rules: symbols, strings, regexes, sequences, choices, repetitions, and a few others.\n",
        "\n",
        "In this notebook, Python files are required to parsed and chunked into unit code snippets like classes and function definitions. The source code for `tree-sitter-python` grammar is downloaded for building the binary that can help in creating the code snippet by extracting entities from Abstract Syntax Tree of the code files. More languages can be supported as well if needed. Follow the [link](https://github.com/tree-sitter/py-tree-sitter) to add more language support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe1nOsmIbd5j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "tree_sitter_parent_dir_name = \"tmp\"  # @param {type:\"string\"}\n",
        "tree_sitter_parent_dir = os.path.join(os.getcwd(), tree_sitter_parent_dir_name)\n",
        "\n",
        "\n",
        "!git clone https://github.com/tree-sitter/tree-sitter-python.git {tree_sitter_parent_dir}/tree-sitter-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT01w4dvKokM"
      },
      "source": [
        "## Clone the Code Repository you want to index and use with RAG\n",
        "\n",
        "In this example, to demonstrate the functionality of RAG with code, a code repository is downloaded locally for indexing. [Python Fire ](https://github.com/google/python-fire) 🔥 is being used for this demo, which is a code repository for automatically generating command line interfaces (CLIs) from absolutely any Python object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFTcc32gHKer"
      },
      "outputs": [],
      "source": [
        "github_repository_clone_url = (\n",
        "    \"https://github.com/google/python-fire.git\"  # @param {type:\"string\"}\n",
        ")\n",
        "\n",
        "!git clone {github_repository_clone_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjP-8prCNpQy"
      },
      "source": [
        "# Build tree_sitter object using language grammar\n",
        "\n",
        "The below code will build the `my-language.so` object compiled using the python grammar that is used to parse the python code files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi84pvUOwW5P"
      },
      "outputs": [],
      "source": [
        "from tree_sitter import Language\n",
        "\n",
        "# Build Tree sitter Parser object\n",
        "Language.build_library(\n",
        "    f\"{tree_sitter_parent_dir}/my-languages.so\",\n",
        "    [\n",
        "        f\"{tree_sitter_parent_dir}/tree-sitter-python\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfRq3sPHNmix"
      },
      "source": [
        "# Helper classes for Code Parsing and Embedding Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjTigItO6Ntt"
      },
      "source": [
        "Creating helper classes that will help in effectively perform code parsing from individual python files of the code repository that we want to index.\n",
        "\n",
        "These classes helps in managing immutable output objects of each task and use batch calling Vertex AI APIs wherever possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r3YTLs3M5294"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import glob\n",
        "import uuid\n",
        "\n",
        "from langchain_google_vertexai.embeddings import VertexAIEmbeddings\n",
        "from loguru import logger\n",
        "import tree_sitter\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Snippet:\n",
        "    \"\"\"Dataclass for storing Embedded Snippets\"\"\"\n",
        "\n",
        "    id: str\n",
        "    embedding: list[float] | None\n",
        "    snippet: str\n",
        "    filename: str\n",
        "    language: str\n",
        "\n",
        "\n",
        "class CodeParser:\n",
        "    \"\"\"Code Parser Class.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str, node_types: list[str], path_to_object_file: str):\n",
        "        self.node_types = node_types\n",
        "        self.language = language\n",
        "        try:\n",
        "            self.parser = tree_sitter.Parser()\n",
        "            self.parser.set_language(\n",
        "                tree_sitter.Language(f\"{path_to_object_file}/my-languages.so\", language)\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.exception(\"failed to build %s parser: \", e)\n",
        "\n",
        "    def parse_file(self, content: str, filename: str):\n",
        "        \"\"\"\n",
        "        Parse code snippets from single code file.\n",
        "\n",
        "        Args:\n",
        "            content: The content of the file.\n",
        "            filename: The name of the code file.\n",
        "\n",
        "        Returns:\n",
        "        List of Parsed Snippets\n",
        "        \"\"\"\n",
        "        try:\n",
        "            tree = self.parser.parse(content)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse snippet: {filename} \\n Error: {e}\")\n",
        "            return\n",
        "\n",
        "        cursor = tree.walk()\n",
        "        parsed_snippets = []\n",
        "\n",
        "        # Walking nodes from abstract syntax tree\n",
        "        while cursor.goto_first_child():\n",
        "            if cursor.node.type in self.node_types:\n",
        "                parsed_snippets.append(\n",
        "                    Snippet(\n",
        "                        id=str(uuid.uuid4()),\n",
        "                        snippet=cursor.node.text,\n",
        "                        filename=filename,\n",
        "                        language=self.language,\n",
        "                        embedding=None,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            while cursor.goto_next_sibling():\n",
        "                if cursor.node.type in self.node_types:\n",
        "                    parsed_snippets.append(\n",
        "                        Snippet(\n",
        "                            id=str(uuid.uuid4()),\n",
        "                            snippet=cursor.node.text,\n",
        "                            filename=filename,\n",
        "                            language=self.language,\n",
        "                            embedding=None,\n",
        "                        )\n",
        "                    )\n",
        "        return parsed_snippets\n",
        "\n",
        "    def parse_directory(self, code_directory_path):\n",
        "        \"\"\"\n",
        "        Parse code snippets from all files in directory.\n",
        "\n",
        "        Args:\n",
        "            code_directory_path: Directory path containing code files.\n",
        "\n",
        "        Returns:\n",
        "        List of Parsed Snippets\n",
        "        \"\"\"\n",
        "        parsed_contents = []\n",
        "        for filename in glob.glob(f\"{code_directory_path}/**/*.py\", recursive=True):\n",
        "            # print(filename)\n",
        "            with open(filename, \"rb\") as codefile:\n",
        "                code_content = codefile.read()\n",
        "\n",
        "            parsed_content = self.parse_file(code_content, filename)\n",
        "            parsed_contents.extend(parsed_content)\n",
        "\n",
        "        return parsed_contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jj0mFVI0UaI"
      },
      "source": [
        "## Helper Function to convert Snippets to DataFrame for easy ChromaDB ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LqFW6Lo3wZOO"
      },
      "outputs": [],
      "source": [
        "def to_dataframe_row(embedded_snippets: list[Snippet]):\n",
        "    \"\"\"\n",
        "    Helper function to convert Embedded Snippet object to a dataframe row\n",
        "    in dictionary format.\n",
        "\n",
        "    Args:\n",
        "        embedded_snippets: List of Snippets to be converted\n",
        "\n",
        "    Returns:\n",
        "        List of Dictionaries\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    for embedded_snippet in embedded_snippets:\n",
        "        output = {\n",
        "            \"ids\": embedded_snippet.id,\n",
        "            \"embeddings\": embedded_snippet.embedding,\n",
        "            \"snippets\": embedded_snippet.snippet,\n",
        "            \"metadatas\": {\n",
        "                \"filenames\": embedded_snippet.filename,\n",
        "                \"languages\": embedded_snippet.language,\n",
        "            },\n",
        "        }\n",
        "        outputs.append(output)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxcdn3zy0kTj"
      },
      "source": [
        "# Parse python code files from repository and create ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXS2PUYmwuiD"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from langchain.vectorstores import Chroma\n",
        "import pandas as pd\n",
        "\n",
        "PERSISTENT_DISK_DIR = \"chromadb\"\n",
        "collection_name = \"coderag\"\n",
        "\n",
        "# Parsing python files from the local code repository\n",
        "parser = CodeParser(\n",
        "    language=\"python\",\n",
        "    node_types=[\"class_definition\", \"function_definition\"],\n",
        "    path_to_object_file=tree_sitter_parent_dir,\n",
        ")\n",
        "logger.info(\"Parsing Code...\")\n",
        "parsed_snippets = parser.parse_directory(\n",
        "    github_repository_clone_url.split(\"/\")[-1].split(\".\")[0]\n",
        ")\n",
        "\n",
        "logger.info(\"Generating Code Embeddings...\")\n",
        "# Generate embedding for the parsed code snippet blocks\n",
        "embedder = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
        "snippet_texts = list(map(lambda x: x.snippet.decode(\"ISO-8859-1\"), parsed_snippets))\n",
        "embedded_texts = embedder.embed_documents(texts=snippet_texts)\n",
        "\n",
        "embedded_snippets = []\n",
        "for code_text, embedding, snippet in zip(\n",
        "    snippet_texts, embedded_texts, parsed_snippets\n",
        "):\n",
        "    snippet.snippet = code_text\n",
        "    snippet.embedding = embedding\n",
        "    embedded_snippets.append(snippet)\n",
        "\n",
        "# Convert Snippets to DataFrame for ChromaDB Ingestion\n",
        "data = pd.DataFrame(to_dataframe_row(embedded_snippets))\n",
        "\n",
        "\n",
        "client = chromadb.PersistentClient(path=PERSISTENT_DISK_DIR)\n",
        "collection = client.get_or_create_collection(\n",
        "    name=collection_name, metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "logger.info(\n",
        "    f\"Adding {data.shape[0]} Code snippets and embedding to \"\n",
        "    \"local chroma db collection...\"\n",
        ")\n",
        "collection.add(\n",
        "    documents=data[\"snippets\"].tolist(),\n",
        "    embeddings=data[\"embeddings\"].tolist(),\n",
        "    metadatas=data[\"metadatas\"].tolist(),\n",
        "    ids=data[\"ids\"].tolist(),\n",
        ")\n",
        "\n",
        "client = None\n",
        "logger.info(\"Code Ingestion Completed.\")\n",
        "\n",
        "# Create langchain Chroma instance for downstream use\n",
        "chroma = Chroma(\n",
        "    persist_directory=PERSISTENT_DISK_DIR,\n",
        "    collection_name=collection_name,\n",
        "    embedding_function=embedder,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfEuN-EtNdWf"
      },
      "source": [
        "# Prompt and Model Config file for different tasks\n",
        "\n",
        "Creating a config file that contains and act as a repository of prompts used to call with Vertex AI generative models for different types of coding tasks like code generation and code completion.\n",
        "\n",
        "This file also contains the configuration of the models that are used to perform individual tasks.\n",
        "\n",
        "The prompts are saved in a key-value pairs, for example: `zero_shot`. More such key-value pairs like `few_shot` can be added, if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cnc-h0YemDP"
      },
      "outputs": [],
      "source": [
        "%%writefile prompts.yaml\n",
        "\n",
        "detect_language:\n",
        "  contextual: >\n",
        "    You are an elite programmer who knows syntax to multiple programming language.\n",
        "    You are provided with a code snippet encapsulated by back-ticks (```).\n",
        "    You are tasked to identify in what programming language out of python, java, golang the code snippet belongs to.\n",
        "    Only give the name of the language and nothing else in the output.\n",
        "    ```\n",
        "    {text}\n",
        "    ```\n",
        "\n",
        "  natural_language: >\n",
        "    Given a piece of natural language text, determine if any of the programming languages \"go\", \"java\", or \"python\" is mentioned.\n",
        "    ### Text:\n",
        "    {text}\n",
        "\n",
        "    If it's not evident from the text, the output should strictly be one of: go, java, python, or None\n",
        "\n",
        "\n",
        "code_completion:\n",
        "  model:\n",
        "    name: \"code-bison-32k@002\"\n",
        "    temperature: 0\n",
        "    max_output_tokens: 1024\n",
        "    prefix: \"\"\n",
        "  prompts:\n",
        "    zero_shot: >\n",
        "      You are an expert programmer.\n",
        "      You will fill in the hole in a snippet of given code.\n",
        "\n",
        "      Considering ### Task, respond with code that works in an IDE only.\n",
        "      Respond only with {language} code.\n",
        "      Include code comments explaining what steps you are taking.\n",
        "\n",
        "      Do not use any textual explanations.\n",
        "      Do not return Markdown.\n",
        "      Do not emit ```{language}\n",
        "      Do not emit ```\n",
        "\n",
        "      ### Task\n",
        "      Complete the snippet below by replacing [HOLE] based on context with only the code necessary to replace [HOLE].\n",
        "      Do not repeat any code that prefixes [HOLE].\n",
        "      The code to complete: \\n\n",
        "\n",
        "      ```\n",
        "\n",
        "      {text}\n",
        "\n",
        "      ```\n",
        "\n",
        "      Reference this code snippet as context when completing the ### Task.\n",
        "\n",
        "      ### Context: \\n\n",
        "\n",
        "      ```\n",
        "\n",
        "      {context}\n",
        "\n",
        "      ```\n",
        "\n",
        "      Note: Please include necessary library imports. Ensure that all used variables and arguments are available.\n",
        "\n",
        "\n",
        "code_generation:\n",
        "  model:\n",
        "    name: \"gemini-1.0-pro-001\"\n",
        "    temperature: 0.1\n",
        "    max_output_tokens: 1024\n",
        "    prefix: \"\"\n",
        "  prompts:\n",
        "    zero_shot: >\n",
        "      Consider yourself a highly skilled programming expert with a deep understanding of various programming languages and paradigms.\n",
        "      Your current task is to provide the most efficient and elegant code solution possible for the given user query.\n",
        "      Leverage any provided context code if relevant, or seamlessly craft new code when necessary.\n",
        "\n",
        "      ###User_Query: \\n\n",
        "\n",
        "      ```\n",
        "\n",
        "      {text}\n",
        "\n",
        "      ```\n",
        "\n",
        "      ###Context_Code: \\n\n",
        "\n",
        "      ```\n",
        "\n",
        "      {context}\n",
        "\n",
        "      ```\n",
        "\n",
        "      Instructions:\n",
        "      1. Carefully understand the programming task described in the ###User_Query.\n",
        "      2. Thoroughly assess the Context Code. Determine if it contains functions, structures, or logic that directly align with the ###User_Query.\n",
        "      3. If the ###Context_Code contains a function or class that already implements the desired functionality, your task is to use that function or class directly in your generated code.\n",
        "         Do not create new implementations unless necessary.\n",
        "      4. If the ###Context_Code has helpful parts that you can modify for the task, adapt and use those parts in your solution.\n",
        "      5. If the ###Context_Code is irrelevant or insufficient, generate a new, complete code solution from scratch that fulfills the User Query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPL_-8uANPSP"
      },
      "source": [
        "# Pair Programming assistant Application to demonstrate Code RAG\n",
        "\n",
        "Class for Pair Programming Assistant and the Gradio UI code that uses the Pair Programming Assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IyucXmaff-VI"
      },
      "outputs": [],
      "source": [
        "from abc import ABC\n",
        "from typing import Any\n",
        "\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from pydantic import BaseModel\n",
        "import yaml\n",
        "\n",
        "\n",
        "class AssistantResponse(ABC, BaseModel):\n",
        "    \"\"\"\n",
        "    Class implementing the return structure for Assistant Response.\n",
        "    \"\"\"\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"\n",
        "        Pydantic Configuration\n",
        "        \"\"\"\n",
        "\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    input_query: str\n",
        "    prompt: str\n",
        "    model_name: str\n",
        "    model_parameters: dict[str, Any]\n",
        "    intent: str\n",
        "    response: str\n",
        "\n",
        "    def to_dict(self) -> dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a dictionary representation of the result\n",
        "        \"\"\"\n",
        "        return self.dict()\n",
        "\n",
        "\n",
        "class PairProgrammingAssistant:\n",
        "    \"\"\"Class for Pair Programming Assistant.\"\"\"\n",
        "\n",
        "    def __init__(self, chroma: Chroma, prompt_tag: str):\n",
        "        self.config = self.read_config(\"/content/prompts.yaml\")\n",
        "        self.chroma = chroma\n",
        "        self.prompt_tag = prompt_tag\n",
        "        self.model_name = None\n",
        "        self.intent = None\n",
        "        self.temperature = 0.2\n",
        "        self.max_output_tokens = 1024\n",
        "\n",
        "    @staticmethod\n",
        "    def read_config(config_file: str):\n",
        "        \"\"\"\n",
        "        Helper function to read config yaml files.\n",
        "\n",
        "        Args:\n",
        "            config_file: File path for the prompt config yaml\n",
        "\n",
        "        Returns:\n",
        "            Loaded Config Dictionary\n",
        "        \"\"\"\n",
        "        with open(config_file, encoding=\"utf-8\") as stream:\n",
        "            config = yaml.safe_load(stream)\n",
        "        return config\n",
        "\n",
        "    def detect_language(self, text: str):\n",
        "        \"\"\"\n",
        "        Helper function to identify the dialect of code input by user.\n",
        "\n",
        "        Args:\n",
        "            text: Raw code input from the user\n",
        "\n",
        "        Returns:\n",
        "            Detected Programming language name\n",
        "        \"\"\"\n",
        "        language_detect_prompt = None\n",
        "        if self.intent == \"code_completion\":\n",
        "            language_detect_prompt = self.config[\"detect_language\"][\"contextual\"]\n",
        "        elif self.intent == \"code_generation\":\n",
        "            language_detect_prompt = self.config[\"detect_language\"][\"natural_language\"]\n",
        "\n",
        "        logger.info(f\"Language detection prompt: {language_detect_prompt}\")\n",
        "        llm = self.get_llm_model()\n",
        "        language = llm.predict(language_detect_prompt.format(text=text))\n",
        "        return language.strip().lower()\n",
        "\n",
        "    def get_model_parameters(self):\n",
        "        \"\"\"\n",
        "        Helper function to fetch the model parameter associated with task.\n",
        "\n",
        "        Returns:\n",
        "            Model Parameters based on current state of intent from config\n",
        "        \"\"\"\n",
        "        self.temperature = self.config[self.intent][\"model\"][\"temperature\"]\n",
        "        self.max_output_tokens = self.config[self.intent][\"model\"][\"max_output_tokens\"]\n",
        "\n",
        "        parameters = dict(\n",
        "            temperature=self.temperature, max_output_tokens=self.max_output_tokens\n",
        "        )\n",
        "        logger.debug(f\"Model parameters: {parameters}\")\n",
        "        return parameters\n",
        "\n",
        "    def get_prompt(self):\n",
        "        \"\"\"\n",
        "        Helper function to fetch the prompt required to perform the task.\n",
        "\n",
        "        Returns:\n",
        "            Prompt to be used based on current state of intent from config\n",
        "        \"\"\"\n",
        "        prompt = self.config[self.intent][\"prompts\"][self.prompt_tag]\n",
        "        logger.debug(f\"Prompt Template: {prompt}\")\n",
        "        return prompt\n",
        "\n",
        "    def get_llm_model(self):\n",
        "        \"\"\"\n",
        "        Helper function to fetch the llm model to be used with the task.\n",
        "\n",
        "        Returns:\n",
        "            LLM to be called based on current state of intent from config\n",
        "        \"\"\"\n",
        "        self.model_name = self.config[self.intent][\"model\"][\"name\"]\n",
        "        llm = VertexAI(model_name=self.model_name)\n",
        "        return llm\n",
        "\n",
        "    def match_chroma(self, snippet: str, language: str):\n",
        "        \"\"\"\n",
        "        Helper function to fetch similar documents from Chroma DB based on\n",
        "        user Inputs.\n",
        "\n",
        "        Args:\n",
        "            snippet: Code snippet to be matched\n",
        "            language: Detected programming language to filter out candidates\n",
        "\n",
        "        Returns:\n",
        "            List of top k matching code snippets\n",
        "        \"\"\"\n",
        "        logger.info(\"Performing similarity search\")\n",
        "        language_filter = (\n",
        "            {\"languages\": language.lower()} if language.lower() != \"none\" else {}\n",
        "        )\n",
        "\n",
        "        code_docs = self.chroma.similarity_search_with_relevance_scores(\n",
        "            query=snippet, k=5, filter=language_filter\n",
        "        )\n",
        "        code_blobs = {}\n",
        "        for code_doc in code_docs:\n",
        "            blob_name = code_doc[0].metadata[\"filenames\"]\n",
        "            if not code_blobs.get(blob_name):\n",
        "                code_blobs[blob_name] = (code_doc[0].page_content, code_doc[1])\n",
        "            break\n",
        "        response = []\n",
        "        for filename, code_doc in code_blobs.items():\n",
        "            response.append(\n",
        "                {\"snippet\": code_doc[0], \"source\": filename, \"score\": code_doc[1]}\n",
        "            )\n",
        "        return response\n",
        "\n",
        "    def perform_coding_task(self, text, **prompt_args):\n",
        "        \"\"\"\n",
        "        Function to perform the intended task with llm and prompt\n",
        "\n",
        "        Args:\n",
        "            text: User Input\n",
        "            prompt_args: Additional prompt arguments (if any)\n",
        "\n",
        "        Returns:\n",
        "            Assistant Response Object\n",
        "        \"\"\"\n",
        "        # Implementation specific to Pair Programming Assistant\n",
        "        llm = self.get_llm_model()\n",
        "        prompt = self.get_prompt()\n",
        "        parameters = self.get_model_parameters()\n",
        "\n",
        "        prefix = prompt.format(text=text, **prompt_args)\n",
        "        logger.debug(f\"Formatted Prompt: {prefix}\")\n",
        "\n",
        "        logger.info(f\"Calling {self.model_name} model\")\n",
        "        result = llm.predict(prefix, **parameters)\n",
        "        response = result\n",
        "\n",
        "        result_object = AssistantResponse(\n",
        "            input_query=text,\n",
        "            intent=self.intent,\n",
        "            prompt=prefix,\n",
        "            model_name=self.model_name,\n",
        "            model_parameters=parameters,\n",
        "            response=response,\n",
        "        )\n",
        "        return result_object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dii3P3zU9ZGI"
      },
      "source": [
        "# Pair Programming Assistant in Action\n",
        "\n",
        "Helper functions that will be used to infer our RAG pipeline along with Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p6NsWDoBJ3X9"
      },
      "outputs": [],
      "source": [
        "# Instantiate Pair Programming assistant with Chroma DB and Prompt Tag\n",
        "pp_assistant = PairProgrammingAssistant(chroma=chroma, prompt_tag=\"zero_shot\")\n",
        "\n",
        "\n",
        "def process(input_: str, intent=None, **prompt_args):\n",
        "    \"\"\"\n",
        "    Helper function to get response for the coding tasks.\n",
        "\n",
        "    Args:\n",
        "        input_: Code/ Prompt input provided by user.\n",
        "        intent: identified intent from user input\n",
        "                i.e. Code Generation, Code Completion etc.\n",
        "\n",
        "    Returns:\n",
        "        A formatted message string to be displayed to user as output.\n",
        "    \"\"\"\n",
        "    global pp_assistant\n",
        "    logger.info(\n",
        "        f\"Inputs from Gradio UI: \\nInput: {input_}, \\n\"\n",
        "        f\"Intent: {intent}, \\nOther args: {prompt_args}\"\n",
        "    )\n",
        "    result_object = pp_assistant.perform_coding_task(text=input_, **prompt_args)\n",
        "\n",
        "    logger.debug(f\"Result from LLM: {result_object.to_dict()}\")\n",
        "\n",
        "    bot_message = str(result_object.response)\n",
        "    bot_message = \"\\n\".join(bot_message.split(\"\\n\")[1:-1])\n",
        "    return bot_message\n",
        "\n",
        "\n",
        "def on_click_completion_generation(code_input, code_task):\n",
        "    \"\"\"\n",
        "    Function to handle the Code Generation and Code Completion tasks with RAG.\n",
        "\n",
        "    Args:\n",
        "        code_input: The code/prompt entered by the user as input\n",
        "        code_task: The type of task to be performed\n",
        "                    i.e. Code Completion, Code Generation etc.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of type:\n",
        "            code_output: The output of Code Completion/Generation task.\n",
        "            file_path: The path of the code file used for context\n",
        "            rag_context: Actual code snippet used as context\n",
        "    \"\"\"\n",
        "    global pp_assistant, pc_handler\n",
        "\n",
        "    logger.debug(\n",
        "        f\"Code completion/generation initiated for: code_input: \"\n",
        "        f\"{code_input}, code_task: {code_task}\"\n",
        "    )\n",
        "\n",
        "    intent_mapping = {\n",
        "        \"Completion\": \"code_completion\",\n",
        "        \"Generation\": \"code_generation\",\n",
        "    }\n",
        "    intent = intent_mapping[code_task]\n",
        "    pp_assistant.intent = intent\n",
        "\n",
        "    language = pp_assistant.detect_language(code_input)\n",
        "    logger.info(f\"Detected language: {language}\")\n",
        "\n",
        "    logger.info(\"Performing Chroma search to fetch indexed code...\")\n",
        "    similar_docs = pp_assistant.match_chroma(snippet=code_input, language=language)\n",
        "    rag_context = \"\"\n",
        "    file_path = \"\"\n",
        "    base_path = \"/content/\"\n",
        "    if similar_docs:\n",
        "        rag_context = similar_docs[0][\"snippet\"]\n",
        "        file_path = base_path + similar_docs[0][\"source\"]\n",
        "\n",
        "    prompt_args = {\"context\": rag_context}\n",
        "    if language.lower() != \"none\":\n",
        "        prompt_args = {\"context\": rag_context, \"language\": language}\n",
        "\n",
        "    return (process(code_input, intent=intent, **prompt_args), file_path, rag_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY9jjbXMOD6t"
      },
      "source": [
        "# Demo\n",
        "\n",
        "Now, Lets write an interface using Gradio that can be used for the three coding tasks we have i.e. Completion and Generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "htsV6_ky9e2o"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "completion_example_input = \"\"\"\n",
        "def strip_lines(lines):\n",
        "  [HOLE]\n",
        "  return lines\n",
        "\"\"\"\n",
        "\n",
        "generation_example_input = \"Encode text string to byte string.\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"\"\"# Code RAG Demo\"\"\")\n",
        "\n",
        "    with gr.Tab(\"Code Interface\"):\n",
        "        with gr.Row():\n",
        "            code_task = gr.Dropdown(\n",
        "                choices=[\"Completion\", \"Generation\"],\n",
        "                label=\"Code task\",\n",
        "                value=\"Completion\",\n",
        "            )\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                code_input = gr.Code(label=\"Task\", lines=20, scale=1, language=\"python\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                code_output = gr.Code(\n",
        "                    label=\"Output\", lines=20, scale=1, language=\"python\"\n",
        "                )\n",
        "\n",
        "        with gr.Row() as metadata_block:\n",
        "            with gr.Column(scale=1):\n",
        "                metadata = gr.Code(label=\"RAG Context\", lines=10, language=\"python\")\n",
        "                ref_file_path = gr.Textbox(\n",
        "                    label=\"Reference File Path\", show_copy_button=True\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                submit = gr.Button(value=\"Run\", variant=\"primary\")\n",
        "            with gr.Column(scale=1):\n",
        "                reset = gr.Button(value=\"Reset\", variant=\"secondary\")\n",
        "\n",
        "        # Sample Examples\n",
        "        with gr.Row():\n",
        "            gr.Examples(\n",
        "                [\n",
        "                    [\"Completion\", f\"\"\"{completion_example_input}\"\"\", \"\", \"\", \"\"],\n",
        "                    [\"Generation\", f\"{generation_example_input}\", \"\", \"\", \"\"],\n",
        "                ],\n",
        "                inputs=[code_task, code_input, code_output, metadata, ref_file_path],\n",
        "            )\n",
        "\n",
        "        submit.click(\n",
        "            fn=on_click_completion_generation,\n",
        "            inputs=[code_input, code_task],\n",
        "            outputs=[code_output, ref_file_path, metadata],\n",
        "        )\n",
        "        reset.click(\n",
        "            fn=lambda x: (\"\", \"\", \"\", \"\"),\n",
        "            inputs=[code_input],\n",
        "            outputs=[code_input, code_output, metadata, ref_file_path],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywwDcY9i_Wpg"
      },
      "source": [
        "## Launch the Demo\n",
        "\n",
        "#### Note: For a better experience, Open the demo application interface in a new tab by clicking on the Localhost url generated after running this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFFWIP4cnUyD"
      },
      "outputs": [],
      "source": [
        "logger.disable(\"__main__\")\n",
        "demo.launch(share=False, debug=True, inbrowser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbj4ooqjNUXv"
      },
      "source": [
        "# Close the demo\n",
        "\n",
        "#### Note: Stop the previous cell to close the Gradio server running locally then run this cell to free up the port utilised for running the server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgqWI-Z1xlVp"
      },
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "code_rag.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
