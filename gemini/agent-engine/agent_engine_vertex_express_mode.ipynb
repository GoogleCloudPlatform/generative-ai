{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Vertex Session and Memory Service for Free with Agent Engine, ADK, and Vertex Express Mode\n",
        "\n",
        "This tutorial extends from the [Quickstart example](https://google.github.io/adk-docs/get-started/quickstart/) for [Agent Development Kit](https://google.github.io/adk-docs/get-started/).\n",
        "\n",
        "We'll embark on building a **Weather Bot agent**, creating single agent that can look up weather, we will connect it with some free Vertex services like the VertexAiSessionService, allowing for users to save their sessions on vertex!\n",
        "\n",
        "**What is ADK Again?**\n",
        "\n",
        "As a reminder, ADK is a Python framework designed to streamline the development of applications powered by Large Language Models (LLMs). It offers robust building blocks for creating agents that can reason, plan, utilize tools, interact dynamically with users, and collaborate effectively within a team.\n",
        "\n",
        "**In this tutorial, you will master:**\n",
        "\n",
        "*   ✅ **Tool Definition & Usage:** Crafting Python functions (`tools`) that grant agents specific abilities (like fetching data) and instructing agents on how to use them effectively.\n",
        "*   ✅ **Agent Engine APIs:** Deploying local agents and using agent engine capabilities like the VertexAiSession service for free\n",
        "\n",
        "**End State Expectation:**\n",
        "\n",
        "By completing this tutorial, you will have built a functional weather agent that can utilize the agent engine building blocks.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "*   ✅ **Solid understanding of Python programming.**\n",
        "*   ✅ **Familiarity with Large Language Models (LLMs), APIs, and the concept of agents.**\n",
        "*   ❗ **Crucially: Completion of the ADK Quickstart tutorial(s) or equivalent foundational knowledge of ADK basics (Agent, Runner, SessionService, basic Tool usage).** This tutorial builds directly upon those concepts.\n",
        "*   ✅ **API Keys** for the LLMs you intend to use (e.g., Google AI Studio for Gemini, OpenAI Platform, Anthropic Console).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to build your agent team? Let's dive in!**"
      ],
      "metadata": {
        "id": "TpDkHI2BNeay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Installation\n",
        "# Install ADK\n",
        "\n",
        "!pip install google-adk -q\n",
        "\n",
        "print(\"Installation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsED19m8K3M6",
        "outputId": "4dcddd78-e43d-4674-b652-a762d1cde115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.1/218.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m878.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstallation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbwxKypOSBkN"
      },
      "outputs": [],
      "source": [
        "# @title Import necessary libraries\n",
        "import os\n",
        "import asyncio\n",
        "from google import adk\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import VertexAiSessionService\n",
        "from google.adk.sessions import VertexAiMemoryService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "from google import vertexai\n",
        "\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure API Keys (Replace with your actual keys!)\n",
        "We can use Vertex Services like VertexAiSessionService and access models for free through Vertex Express Mode! Sign up with your google account here: https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview to unlock free access to certain services and models for free without the need of adding your credit card.\n",
        "\n",
        "Vertex Express mode combined with ADK allow for the creation of advanced agents for free! You will have access to certain Gemini models and Agent Engine services like Session and Memory, all for free without a billing account!"
      ],
      "metadata": {
        "id": "NqEJDL_6k58J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mNsVI5eSDOi",
        "outputId": "bd9de692-d339-40aa-ddeb-a46299b97e56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys Set:\n",
            "Google API Key set: Yes\n"
          ]
        }
      ],
      "source": [
        "# Gemini API Key (Get from Vertex Express Mode)\n",
        "easygcp_api_key = \"INSERT API KEY HERE\" #@param {type:\"string\"}\n",
        "os.environ[\"GOOGLE_API_KEY\"] = easygcp_api_key\n",
        "# Set vertex to true\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
        "\n",
        "# --- Verify Keys (Optional Check) ---\n",
        "print(\"API Keys Set:\")\n",
        "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When creating an agent, we need to choose the model we want. The Vertex Express mode API key allows for the use of several gemini models for free, and you can use any of the models listed here: https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview#models"
      ],
      "metadata": {
        "id": "6-xwIAk-lf1T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI_qvZJrSJuR",
        "outputId": "b8dffc4a-84f6-4df9-a0b5-9ae89d99037f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Environment configured.\n"
          ]
        }
      ],
      "source": [
        "# --- Define Model Constants for easier use ---\n",
        "\n",
        "# Use an allowlisted model for EasyGCP, we will use gemini 2.0\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "print(\"\\nEnvironment configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7LZM3ysSOMu"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Your First Agent \\- Basic Weather Lookup\n",
        "\n",
        "Let's begin by building the fundamental component of our Weather Bot: a single agent capable of performing a specific task – looking up weather information. This involves creating two core pieces:\n",
        "\n",
        "1. **A Tool:** A Python function that equips the agent with the *ability* to fetch weather data.  \n",
        "2. **An Agent:** The AI \"brain\" that understands the user's request, knows it has a weather tool, and decides when and how to use it.\n",
        "\n",
        "---\n",
        "\n",
        "**1\\. Define the Tool (`get_weather`)**\n",
        "\n",
        "In ADK, **Tools** are the building blocks that give agents concrete capabilities beyond just text generation. They are typically regular Python functions that perform specific actions, like calling an API, querying a database, or performing calculations.\n",
        "\n",
        "Our first tool will provide a *mock* weather report. This allows us to focus on the agent structure without needing external API keys yet. Later, you could easily swap this mock function with one that calls a real weather service.\n",
        "\n",
        "**Key Concept: Docstrings are Crucial\\!** The agent's LLM relies heavily on the function's **docstring** to understand:\n",
        "\n",
        "* *What* the tool does.  \n",
        "* *When* to use it.  \n",
        "* *What arguments* it requires (`city: str`).  \n",
        "* *What information* it returns.\n",
        "\n",
        "**Best Practice:** Write clear, descriptive, and accurate docstrings for your tools. This is essential for the LLM to use the tool correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILy7YTCbSRAT",
        "outputId": "2aca279e-ca1a-4bdd-f3fe-0b75eccea0bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
            "--- Tool: get_weather called for city: Paris ---\n",
            "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
          ]
        }
      ],
      "source": [
        "# @title Define the get_weather Tool\n",
        "def get_weather(city: str) -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes a 'report' key with weather details.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
        "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
        "\n",
        "    # Mock weather data\n",
        "    mock_weather_db = {\n",
        "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
        "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
        "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        return mock_weather_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
        "\n",
        "# Example tool usage (optional test)\n",
        "print(get_weather(\"New York\"))\n",
        "print(get_weather(\"Paris\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAM0BqGWSTo5"
      },
      "source": [
        "---\n",
        "\n",
        "**2\\. Define the Agent (`weather_agent`)**\n",
        "\n",
        "Now, let's create the **Agent** itself. An `Agent` in ADK orchestrates the interaction between the user, the LLM, and the available tools.\n",
        "\n",
        "We configure it with several key parameters:\n",
        "\n",
        "* `name`: A unique identifier for this agent (e.g., \"weather\\_agent\\_v1\").  \n",
        "* `model`: Specifies which LLM to use (e.g., `MODEL_GEMINI_2_0_FLASH`). We'll start with a specific Gemini model.  \n",
        "* `description`: A concise summary of the agent's overall purpose. This becomes crucial later when other agents need to decide whether to delegate tasks to *this* agent.  \n",
        "* `instruction`: Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically *how and when* to utilize its assigned `tools`.  \n",
        "* `tools`: A list containing the actual Python tool functions the agent is allowed to use (e.g., `[get_weather]`).\n",
        "\n",
        "**Best Practice:** Provide clear and specific `instruction` prompts. The more detailed the instructions, the better the LLM can understand its role and how to use its tools effectively. Be explicit about error handling if needed.\n",
        "\n",
        "**Best Practice:** Choose descriptive `name` and `description` values. These are used internally by ADK and are vital for features like automatic delegation (covered later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ho1COmKSUeV",
        "outputId": "dd45fbde-839c-4e58-fb79-b0ff0431ba0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash-001'.\n"
          ]
        }
      ],
      "source": [
        "# @title Define the Weather Agent\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"weather_agent_v1\",\n",
        "    model=MODEL_GEMINI_2_0_FLASH,\n",
        "    description=\"Provides weather information for specific cities.\",\n",
        "    instruction=\"You are a helpful weather assistant. \"\n",
        "                \"When the user asks for the weather in a specific city, \"\n",
        "                \"use the 'get_weather' tool to find the information. \"\n",
        "                \"If the tool returns an error, inform the user politely. \"\n",
        "                \"If the tool is successful, present the weather report clearly.\",\n",
        "    tools=[get_weather, adk.tools.preload_memory_tool.PreloadMemoryTool()], # Pass the function directly\n",
        ")\n",
        "\n",
        "print(f\"Agent '{weather_agent.name}' created using model '{MODEL_GEMINI_2_0_FLASH}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**3\\. Define the Agent Engine**\n",
        "\n",
        "An Agent Engine is a set of services that enables developers to deploy, manage, and scale AI agents in production. Agent Engine handles the infrastructure to scale agents in production so you can focus on creating applications. In Vertex Express mode, we have access to certain Agent Engine services for free, mainly the Session and Memory services, which allow for context management. Each session and memory is associated with an Agent Engine.\n",
        "\n",
        "We configure our Agent Engine with several key parameters:\n",
        "\n",
        "* `displayName`: A unique identifier for this agent engine (e.g., \"weather\\_agent\\_v1\").  \n",
        "* `description`: A concise summary of the agent engine's overall purpose. This can help you remember what is does."
      ],
      "metadata": {
        "id": "J3-rNFdr1urU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the Agent Engine\n",
        "from google import genai\n",
        "\n",
        "# Create Agent Engine with GenAI SDK\n",
        "client = genai.Client(vertexai = True)._api_client\n",
        "response = client.request(\n",
        "        http_method='POST',\n",
        "        path=f'reasoningEngines',\n",
        "        request_dict={\"displayName\": \"Express-Mode-Agent-Engine\", \"description\": \"Test Agent Engine demo\"},\n",
        "    )\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goqjFK2SMNa4",
        "outputId": "a89e361e-c3a5-4a16-a7f7-fc961d66a7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'projects/174749575465/locations/us-central1/reasoningEngines/105157292080496640/operations/8824284088027316224',\n",
              " 'metadata': {'@type': 'type.googleapis.com/google.cloud.aiplatform.v1beta1.CreateReasoningEngineOperationMetadata',\n",
              "  'genericMetadata': {'createTime': '2025-06-17T20:20:24.693909Z',\n",
              "   'updateTime': '2025-06-17T20:20:24.693909Z'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "APP_NAME=\"/\".join(response['name'].split(\"/\")[:6])\n",
        "APP_NAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ik0W51d_MW5Q",
        "outputId": "d1b0edc7-d213-4756-d07f-890d319e9c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'projects/174749575465/locations/us-central1/reasoningEngines/105157292080496640'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "APP_ID=APP_NAME.split('/')[-1]\n",
        "APP_ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_DSD83dghQpJ",
        "outputId": "940d19c3-3160-42dd-ced3-3bf7fd918ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'105157292080496640'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvz7LDhbSZxL"
      },
      "source": [
        "---\n",
        "\n",
        "**4\\. Setup Runner and Session Service**\n",
        "\n",
        "To manage conversations and execute the agent, we need two more components:\n",
        "\n",
        "* `SessionService`: Responsible for managing conversation history and state for different users and sessions. The `VertexAiSessionService` is an implementation that stores everything in vertex, allowing for persistent session storage. It keeps track of the messages exchanged. We'll explore state persistence more in Step 4\\.  \n",
        "* `Runner`: The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the `SessionService`, and yields events representing the progress of the interaction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Our Initial Session\n",
        "\n",
        "# Create Vertex AI Session through ADK\n",
        "session_service = VertexAiSessionService()\n",
        "memory_service = VertexAiMemoryService(APP_NAME)\n",
        "\n",
        "USER_ID = \"INSERT USER ID HERE\" #@param {type:\"string\"}\n",
        "session = await session_service.create_session(app_name=APP_ID, user_id=USER_ID, state={\"name\": \"test\"})\n",
        "SESSION_ID = session.id\n",
        "session"
      ],
      "metadata": {
        "id": "Nykf_acxMk5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bd523a-994b-4474-b457-f80d489fbd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Session(id='2820498563652386816', app_name='105157292080496640', user_id='INSERT USER ID HERE', state={'name': 'test'}, events=[], last_update_time=1750194050.962662)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create an Agent Runner\n",
        "\n",
        "# Connect with ADK. ADK will also use the easygcp key to generate content\n",
        "print(f\"Session created: App='{APP_ID}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "# --- Runner ---\n",
        "# Key Concept: Runner orchestrates the agent execution loop.\n",
        "runner = Runner(\n",
        "    agent=weather_agent, # The agent we want to run\n",
        "    app_name=APP_ID,   # Associates runs with our app\n",
        "    session_service=session_service, # Uses vertex session service\n",
        "    memory_service=memory_service # Uses vertex memory service\n",
        ")\n",
        "print(f\"Runner created for agent '{runner.agent.name}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_80NpAeF6-i6",
        "outputId": "acabc301-1686-4aa6-99eb-553e103dadd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session created: App='105157292080496640', User='INSERT USER ID HERE', Session='2820498563652386816'\n",
            "Runner created for agent 'weather_agent_v1'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zKGVwRkSduA"
      },
      "source": [
        "---\n",
        "\n",
        "**5\\. Interact with the Agent**\n",
        "\n",
        "We need a way to send messages to our agent and receive its responses. Since LLM calls and tool executions can take time, ADK's `Runner` operates asynchronously.\n",
        "\n",
        "We'll define an `async` helper function (`call_agent_async`) that:\n",
        "\n",
        "1. Takes a user query string.  \n",
        "2. Packages it into the ADK `Content` format.  \n",
        "3. Calls `runner.run_async`, providing the user/session context and the new message.  \n",
        "4. Iterates through the **Events** yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).  \n",
        "5. Identifies and prints the **final response** event using `event.is_final_response()`.\n",
        "\n",
        "**Why `async`?** Interactions with LLMs and potentially tools (like external APIs) are I/O-bound operations. Using `asyncio` allows the program to handle these operations efficiently without blocking execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZJr8lbkSebH"
      },
      "outputs": [],
      "source": [
        "# @title Define Agent Interaction Function\n",
        "\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "async def call_agent_async(query: str, runner, user_id, session_id):\n",
        "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
        "  print(f\"\\n>>> User Query: {query}\")\n",
        "\n",
        "  # Prepare the user's message in ADK format\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
        "\n",
        "  # Key Concept: run_async executes the agent logic and yields Events.\n",
        "  # We iterate through events to find the final answer.\n",
        "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "      # You can uncomment the line below to see *all* events during execution\n",
        "      print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
        "\n",
        "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "             # Assuming text response in the first part\n",
        "             final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
        "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
        "          # Add more checks here if needed (e.g., specific error codes)\n",
        "          break # Stop processing events once the final response is found\n",
        "\n",
        "  print(f\"<<< Agent Response: {final_response_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6DQSqrqk5ic"
      },
      "source": [
        "---\n",
        "\n",
        "**6\\. Run the Conversation**\n",
        "\n",
        "Finally, let's test our setup by sending a few queries to the agent. We wrap our `async` calls in a main `async` function and run it using `await`.\n",
        "\n",
        "Watch the output:\n",
        "\n",
        "* See the user queries.  \n",
        "* Notice the `--- Tool: get_weather called... ---` logs when the agent uses the tool.  \n",
        "* Observe the agent's final responses, including how it handles the case where weather data isn't available (for Paris)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEd2QhHyUKY8",
        "outputId": "3a1fe78f-04d6-4a7e-99eb-c96933d1275f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> User Query: What is the weather like in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Event] Author: weather_agent_v1, Type: Event, Final: False, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=FunctionCall(id='adk-e57dbbf0-4f2d-4b84-9fd8-cc66e0609ed7', args={'city': 'London'}, name='get_weather'), function_response=None, text=None)] role='model'\n",
            "--- Tool: get_weather called for city: London ---\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: False, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=FunctionResponse(will_continue=None, scheduling=None, id='adk-e57dbbf0-4f2d-4b84-9fd8-cc66e0609ed7', name='get_weather', response={'status': 'success', 'report': \"It's cloudy in London with a temperature of 15°C.\"}), text=None)] role='user'\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: True, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text=\"It's cloudy in London with a temperature of 15°C.\\n\")] role='model'\n",
            "<<< Agent Response: It's cloudy in London with a temperature of 15°C.\n",
            "\n",
            "\n",
            ">>> User Query: How about Paris?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Event] Author: weather_agent_v1, Type: Event, Final: False, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=FunctionCall(id='adk-c6a63abe-edcf-4cce-a376-749bf2c8199b', args={'city': 'Paris'}, name='get_weather'), function_response=None, text=None)] role='model'\n",
            "--- Tool: get_weather called for city: Paris ---\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: False, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=FunctionResponse(will_continue=None, scheduling=None, id='adk-c6a63abe-edcf-4cce-a376-749bf2c8199b', name='get_weather', response={'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}), text=None)] role='user'\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: True, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text=\"Sorry, I don't have weather information for Paris.\\n\")] role='model'\n",
            "<<< Agent Response: Sorry, I don't have weather information for Paris.\n",
            "\n",
            "\n",
            ">>> User Query: Tell me the weather in New York\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Event] Author: weather_agent_v1, Type: Event, Final: False, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=FunctionCall(id='adk-91342f9f-b324-4395-8fa0-7b8778a41e5b', args={'city': 'New York'}, name='get_weather'), function_response=None, text=None)] role='model'\n",
            "--- Tool: get_weather called for city: New York ---\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: False, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=FunctionResponse(will_continue=None, scheduling=None, id='adk-91342f9f-b324-4395-8fa0-7b8778a41e5b', name='get_weather', response={'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}), text=None)] role='user'\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: True, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='The weather in New York is sunny with a temperature of 25°C.\\n')] role='model'\n",
            "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
            "\n",
            "\n",
            ">>> User Query: I prefer the weather in New York, that sounds nicer than the weather in London\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: True, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='I understand. You prefer sunny weather with a temperature of 25°C like in New York, rather than cloudy weather with a temperature of 15°C like in London.\\n')] role='model'\n",
            "<<< Agent Response: I understand. You prefer sunny weather with a temperature of 25°C like in New York, rather than cloudy weather with a temperature of 15°C like in London.\n",
            "\n",
            "\n",
            ">>> User Query: What cities did I ask you about previously?\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: True, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='You previously asked me about the weather in London, Paris, and New York.\\n')] role='model'\n",
            "<<< Agent Response: You previously asked me about the weather in London, Paris, and New York.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Run the Initial Conversation\n",
        "\n",
        "# We need an async function to await our interaction helper\n",
        "async def run_conversation():\n",
        "    await call_agent_async(\"What is the weather like in London?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "    await call_agent_async(\"How about Paris?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
        "\n",
        "    await call_agent_async(\"Tell me the weather in New York\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "    await call_agent_async(\"I prefer the weather in New York, that sounds nicer than the weather in London\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "    await call_agent_async(\"What cities did I ask you about previously?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
        "await run_conversation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbUzAGvsmB2a"
      },
      "source": [
        "---\n",
        "\n",
        "Congratulations\\! You've successfully built and interacted with your first ADK agent, and used the Vertex Session Service for free!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**8\\. Test out Agent Memory**\n",
        "\n",
        "Lets see if the agent will remember our preferences from our previous session"
      ],
      "metadata": {
        "id": "mdCneYx5tbcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create a Memory\n",
        "\n",
        "# Create a memory based on the previous conversation\n",
        "print(f\"{APP_NAME}/sessions/{session.id}\")\n",
        "memory_service.add_session_to_memory(session)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ZZua59gP4Q",
        "outputId": "92db6d7b-352f-4ffd-c7f1-275cd150b016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projects/174749575465/locations/us-central1/reasoningEngines/105157292080496640/sessions/7432184582079774720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'projects/174749575465/locations/us-central1/reasoningEngines/105157292080496640/operations/8824284088027316224',\n",
              " 'metadata': {'@type': 'type.googleapis.com/google.cloud.aiplatform.v1beta1.CreateReasoningEngineOperationMetadata',\n",
              "  'genericMetadata': {'createTime': '2025-06-17T20:20:24.693909Z',\n",
              "   'updateTime': '2025-06-17T20:20:24.693909Z'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test the Agent Memory\n",
        "\n",
        "# Create a new session, and lets see if it will remember our preferences based on our user id\n",
        "session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, state={\"name\": \"test\"})\n",
        "SESSION_ID = session.id\n",
        "\n",
        "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "await call_agent_async(\"What weather do I prefer?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)"
      ],
      "metadata": {
        "id": "l6nw_N41eztw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f902c4-489c-4541-f86a-db75fa187e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session created: App='projects/174749575465/locations/us-central1/reasoningEngines/105157292080496640', User='INSERT USER ID HERE', Session='1667577059045539840'\n",
            "\n",
            ">>> User Query: What weather do I prefer?\n",
            "  [Event] Author: weather_agent_v1, Type: Event, Final: True, Content: parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='You prefer sunny weather with a temperature of 25°C.\\n')] role='model'\n",
            "<<< Agent Response: You prefer sunny weather with a temperature of 25°C.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}