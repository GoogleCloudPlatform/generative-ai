{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Get started with Vertex AI Memory Bank - LangGraph\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/tree/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fagent-engine%2Fmemory%2Fget_started_with_memory_bank_langgraph.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/memory/get_started_with_memory_bank_langgraph.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Ivan Nardini](https://github.com/inardini) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to build a conversational agent using **LangGraph** with the **Vertex AI Memory Bank API** to provide the agent with long-term memory. This enables the agent to personalize responses based on information learned from prior conversations.\n",
    "\n",
    "### What you'll learn\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "  * Provision a Vertex AI Agent Engine to use Memory Bank.\n",
    "  * Define a LangGraph agent state and graph structure.\n",
    "  * Create a graph node that retrieves memories, generates personalized responses, and stores new information.\n",
    "  * Build and run a conversational agent that maintains context across sessions.\n",
    "\n",
    "### Why this is important\n",
    "\n",
    "While standard chatbot frameworks can manage short-term conversation history, they often fail to retain information across different sessions. LangGraph allows you to build robust, stateful applications by defining agent workflows as graphs. Integrating it with a dedicated long-term memory service like Vertex AI Memory Bank unlocks the ability to create truly personalized and context-aware agents that build a continuous understanding of their users over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform>=1.100.0 langgraph langchain-core langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and enviroment\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "import uuid\n",
    "import vertexai\n",
    "\n",
    "# Project configuration\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "    if not PROJECT_ID:\n",
    "        raise ValueError(\"Project ID not found. Please set it in the form above or as the GOOGLE_CLOUD_PROJECT environment variable.\")\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "# Agent configuration\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "USER_ID = f\"user_{uuid.uuid4()}\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "print(f\"Session User ID: {USER_ID}\")\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "client = vertexai.Client(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the necessary libraries for building our conversational agent with LangGraph and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from functools import partial\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langgraph.graph import StateGraph, START, END, add_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZChEL1y9BIU7"
   },
   "source": [
    "### Helper functions\n",
    "\n",
    "These helper functions will manage the user-facing chat loop and process a single turn of the conversation with our LangGraph agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7mGHiul1NQe"
   },
   "outputs": [],
   "source": [
    "def run_single_turn(user_input: str, user_id: str) -> str:\n",
    "    \"\"\"Run a single conversation turn.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": user_id}}\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=user_input)],\n",
    "        \"user_id\": user_id\n",
    "    }\n",
    "\n",
    "    response_content = None\n",
    "    for event in compiled_graph.stream(state, config):\n",
    "        for value in event.values():\n",
    "            if value.get(\"messages\"):\n",
    "                response_content = value[\"messages\"][-1].content\n",
    "\n",
    "    return response_content\n",
    "\n",
    "\n",
    "def chat_loop(user_id: str) -> None:\n",
    "    \"\"\"Main chat interface loop.\"\"\"\n",
    "    print(\"\\nStarting chat. Type 'exit' or 'quit' to end.\")\n",
    "    print(\"Every message will be automatically stored in memory.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nAssistant: Thank you for chatting. Have a great day!\")\n",
    "            break\n",
    "\n",
    "        response = run_single_turn(user_input, user_id)\n",
    "        if response:\n",
    "            print(f\"\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Creating the Agent Engine with Memory Bank\n",
    "\n",
    "Vertex AI Memory Bank is part of the Vertex AI Agent Engine, a managed service that enables developers to deploy, manage, and scale AI agents.\n",
    "\n",
    "To use Memory Bank, you first create a new Agent Engine instance, which provides the APIs to store and retrieve user-specific memories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5tUDbo8YVhe"
   },
   "outputs": [],
   "source": [
    "agent_engine = client.agent_engines.create()\n",
    "print(f\"Created Agent Engine: {agent_engine.api_resource.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8oPi1rEO55v"
   },
   "source": [
    "## (Optional) Initialize user memory\n",
    "\n",
    "You can optionally pre-seed the Memory Bank with initial facts about a user. This demonstrates how to store information that the agent can reference from the very first interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6xRZ6-sg_7L"
   },
   "outputs": [],
   "source": [
    "import_initial_memory_op = client.agent_engines.create_memory(\n",
    "    name=agent_engine.api_resource.name,\n",
    "    fact=f\"The user's name is {USER_ID} and they have a passion for learning new things.\",\n",
    "    scope={\"user_id\": USER_ID},\n",
    ")\n",
    "memory = import_initial_memory_op.response\n",
    "print(\"Created initial memory:\")\n",
    "print(f\"  Fact: {memory.fact}\")\n",
    "print(f\"  User ID: {memory.scope.get('user_id')}\")\n",
    "print(f\"  Created at: {memory.create_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZwY1np4TyjF"
   },
   "outputs": [],
   "source": [
    "memories = client.agent_engines.retrieve_memories(\n",
    "    name=agent_engine.api_resource.name,\n",
    "    scope={\"user_id\": USER_ID},\n",
    ")\n",
    "\n",
    "for i, memory in enumerate(memories, 1):\n",
    "    if hasattr(memory, 'memory') and hasattr(memory.memory, 'fact'):\n",
    "        print(f\"{i}. {memory.memory.fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntRLeQdtCoj1"
   },
   "source": [
    "## Building the LangGraph Agent\n",
    "\n",
    "Now, we will construct our conversational agent. The agent will be designed to:\n",
    "\n",
    "1.  Retrieve relevant memories before generating a response.\n",
    "2.  Use these memories to personalize its conversation.\n",
    "3.  Automatically store each conversation turn to build its long-term memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opi14Ez06rNU"
   },
   "source": [
    "### Define the Agent State\n",
    "\n",
    "The `State` object is a central concept in LangGraph. It holds the data that persists and is passed between the nodes in our graph. For this agent, the state will track the conversation `messages` and the `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z12_8xnH01WJ"
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"Agent state definition.\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_id: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvZhpAbr6sWn"
   },
   "source": [
    "### Create the Core Logic: The Chatbot Node\n",
    "\n",
    "This node contains the main logic of our agent. For each turn, it performs a sequence of actions:\n",
    "\n",
    "1.  **Retrieve Memories**: It queries the Memory Bank API to find memories relevant to the user's latest message. It can use either semantic search for relevance or chronological retrieval for a complete history.\n",
    "2.  **Construct Prompt**: It injects the retrieved memories into the system prompt, providing the LLM with personalized context.\n",
    "3.  **Generate Response**: It calls the Gemini model with the context-enhanced prompt to generate a response.\n",
    "4.  **Store New Memories**: It sends the latest user message and agent response back to Memory Bank to be processed and stored for future conversations.\n",
    "\n",
    "This \"always store\" approach ensures the agent continuously learns from every interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c3RfqnE0zte"
   },
   "outputs": [],
   "source": [
    "def chatbot_node(state: State, agent_engine_name: str, use_semantic_search: bool = True) -> dict:\n",
    "    \"\"\"Main agent logic node - stores every interaction.\"\"\"\n",
    "\n",
    "    user_id = state[\"user_id\"]\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "\n",
    "    # Check user_message is string\n",
    "    if not isinstance(user_message, str):\n",
    "        raise ValueError(\"User message must be a string\")\n",
    "\n",
    "    # Retrieve memories based on the selected method\n",
    "    if use_semantic_search:\n",
    "        # Use semantic search to find relevant memories based on the user's current message\n",
    "        memories = client.agent_engines.retrieve_memories(\n",
    "            name=agent_engine_name,\n",
    "            scope={\"user_id\": user_id},\n",
    "            similarity_search_params={\n",
    "                \"search_query\": user_message,\n",
    "                \"top_k\": 10  # Retrieve top 10 most relevant memories\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # Retrieve all memories\n",
    "        memories = client.agent_engines.retrieve_memories(\n",
    "            name=agent_engine_name,\n",
    "            scope={\"user_id\": user_id},\n",
    "            simple_retrieval_params={}\n",
    "        )\n",
    "\n",
    "    # Extract facts from memory objects\n",
    "    memory_facts = [\n",
    "        memory.memory.fact\n",
    "        for memory in memories\n",
    "        if hasattr(memory, 'memory') and hasattr(memory.memory, 'fact')\n",
    "    ]\n",
    "\n",
    "    # Format memory context based on retrieval method\n",
    "    if memory_facts:\n",
    "        if use_semantic_search:\n",
    "            memory_context = \"Relevant memories from previous conversations:\\n\" + \"\\n\".join(\n",
    "                f\"- {fact}\" for fact in memory_facts\n",
    "            )\n",
    "        else:\n",
    "            memory_context = \"Previous conversation history:\\n\" + \"\\n\".join(\n",
    "                f\"- {fact}\" for fact in memory_facts\n",
    "            )\n",
    "    else:\n",
    "        memory_context = \"No previous conversations found.\"\n",
    "\n",
    "    # Create system prompt with memory context\n",
    "    system_message = SystemMessage(\n",
    "        content=f\"\"\"You are a helpful assistant with perfect memory.\n",
    "\n",
    "        {memory_context}\n",
    "\n",
    "        Instructions:\n",
    "        - Use the context to personalize responses\n",
    "        - Naturally reference past conversations when relevant\n",
    "        - Build upon previous knowledge about the user\n",
    "        - If using semantic search, the memories shown are the most relevant to the current query\"\"\"\n",
    "    )\n",
    "\n",
    "    # Generate response using LLM\n",
    "    messages = [system_message] + state[\"messages\"]\n",
    "    llm = ChatVertexAI(model=MODEL_NAME, project=PROJECT_ID, location=LOCATION)\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Store the conversation turn in memory\n",
    "    operation = client.agent_engines.generate_memories(\n",
    "        name=agent_engine_name,\n",
    "        direct_contents_source={\n",
    "            \"events\": [{\n",
    "                \"content\": {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [{\"text\": user_message}],\n",
    "                }\n",
    "            }, {\n",
    "                \"content\": {\n",
    "                    \"role\": \"model\",\n",
    "                    \"parts\": [{\"text\": response.content}],\n",
    "                }\n",
    "            }]\n",
    "        },\n",
    "        scope={\"user_id\": user_id},\n",
    "        config={\"wait_for_completion\": True},\n",
    "    )\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apId5Vvi6_QA"
   },
   "source": [
    "### Build and Compile the Conversation Graph\n",
    "\n",
    "LangGraph uses a state graph to define the conversational flow. Our graph is simple: it has a single `chatbot` node that processes each turn. We connect the `START` of the graph to our node and the node's output to the `END`.\n",
    "\n",
    "We use `functools.partial` to bind the `agent_engine_name` and `use_semantic_search` arguments to our node function ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3G6Pax34R8tB"
   },
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# Create chatbot node with agent engine name and search preference bound\n",
    "chatbot_with_memory = partial(\n",
    "    chatbot_node,\n",
    "    agent_engine_name=agent_engine.api_resource.name,\n",
    "    use_semantic_search=True\n",
    ")\n",
    "\n",
    "# Assemble and compile the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_memory)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "compiled_graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4WVP3nuaIV-"
   },
   "source": [
    "## Interacting with the Agent\n",
    "\n",
    "Now, let's interact with our memory-enabled agent. Try the following phrases to see how the agent builds and uses its memory.\n",
    "\n",
    "```\n",
    "You: Hi, I work as an agent engineer\n",
    "You: I love hiking and have a dog named Max\n",
    "You: I'm working on a recommendation system project\n",
    "You: What do you remember about me?\n",
    "You: Bye\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tqSiFUzaKBC"
   },
   "outputs": [],
   "source": [
    "chat_loop(USER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLG-474VXfzy"
   },
   "source": [
    "## Check for Stored Memories\n",
    "\n",
    "After the conversation, we can retrieve all stored memories to see what the agent has learned about the user. This demonstrates that the information has been successfully processed and persisted by Vertex AI Memory Bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hpsMHAIOrxN"
   },
   "outputs": [],
   "source": [
    "print(\"Stored Memories\")\n",
    "memories = client.agent_engines.retrieve_memories(\n",
    "    name=agent_engine.api_resource.name,\n",
    "    scope={\"user_id\": USER_ID},\n",
    ")\n",
    "\n",
    "for memory in memories:\n",
    "    if hasattr(memory, 'memory') and hasattr(memory.memory, 'fact'):\n",
    "        print(f\"{memory.memory.fact}\")\n",
    "\n",
    "# Show total memory count\n",
    "print(f\"\\nTotal memories stored: {len(list(memories))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To avoid charges, delete the agent engine when you're done experimenting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAtT65gEcAFB"
   },
   "outputs": [],
   "source": [
    "delete_engine = True\n",
    "\n",
    "if delete_engine:\n",
    "    client.agent_engines.delete(name=agent_engine.api_resource.name, force=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_memory_bank_langgraph.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
