{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex AI Supervised tuning token count and cost estimation.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fvertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Lehui Liu](https://github.com/liulehui), [Erwin Huizenga](https://github.com/Huize501) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook serves as a tool to preprocess and estimate token counts for tuning costs for tuning [`gemini-1.5-pro-002`](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning).\n",
        "\n",
        "At the end you will also find the code to preprocess and estimate token counts for tuning costs for tuning `gemini-1.0-pro-002`. If you get started please start with `gemini-1.5-pro-002`.\n",
        "\n",
        "For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform[tokenization] numpy==1.26.4 tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Tuning token count and cost estimation: `Gemini 1.5 Pro` and `Gemini 1.5 Flash`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhY560YQijW"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "4498u5KpQijW"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import dataclasses\n",
        "import json\n",
        "\n",
        "from google.cloud import storage\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from vertexai.generative_models import Content, Part\n",
        "from vertexai.preview.tokenization import get_tokenizer_for_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvqIIG1M0YCy"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "This example is for text only. Define the Google Cloud Storage URIs pointing to your training and validation datasets or continue using the URIs provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "oue9Q0GG0Rvk"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"gemini-1.5-pro-002\"  # @param ['gemini-1.5-pro-002']{type:\"string\"}\n",
        "training_dataset_uri = \"gs://github-repo/generative-ai/gemini/tuning/train_sft_train_samples.jsonl\"  # @param {type:\"string\"}\n",
        "validation_dataset_uri = \"gs://github-repo/generative-ai/gemini/tuning/val_sft_val_samples.jsonl\"  # @param {type:\"string\"}\n",
        "\n",
        "tokenizer = get_tokenizer_for_model(\"gemini-1.5-pro-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbl6UD5P3LIH"
      },
      "source": [
        "We'll now load the dataset and conduct some basic statistical analysis to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "M9TrzApr1tYQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Num training examples: 500\n",
            "First example:\n",
            "{'role': 'user', 'parts': [{'text': 'Honesty is usually the best policy. It is disrespectful to lie to someone. If you don\\'t want to date someone, you should say so.  Sometimes it is easy to be honest. For example, you might be able to truthfully say, \"No, thank you, I already have a date for that party.\" Other times, you might need to find a kinder way to be nice. Maybe you are not attracted to the person. Instead of bluntly saying that, try saying, \"No, thank you, I just don\\'t think we would be a good fit.\" Avoid making up a phony excuse. For instance, don\\'t tell someone you will be out of town this weekend if you won\\'t be. There\\'s a chance that you might then run into them at the movies, which would definitely cause hurt feelings. A compliment sandwich is a really effective way to provide feedback. Essentially, you \"sandwich\" your negative comment between two positive things. Try using this method when you need to reject someone.  An example of a compliment sandwich is to say something such as, \"You\\'re an awesome person. Unfortunately, I\\'m not interested in dating you. Someone else is going to be really lucky to date someone with such a great personality!\" You could also try, \"You are a really nice person. I\\'m only interested you as a friend. I like when we hang out in big groups together!\" Be sincere. If you offer false compliments, the other person will likely be able to tell and feel hurt. If you do not want to date someone, it is best to be upfront about your feelings. Do not beat around the bush. If your mind is made up, it is best to clearly state your response.  If someone asks you to date them and you don\\'t want to, you can be direct and kind at the same time. State your answer clearly. You can make your feelings clear without purposefully hurting someone else\\'s feelings. Try smiling and saying, \"That sounds fun, but no thank you. I\\'m not interested in dating you.\" Don\\'t beat around the bush. If you do not want to accept the date, there is no need to say, \"Let me think about it.\" It is best to get the rejection over with. You don\\'t want to give someone false hope. Avoid saying something like, \"Let me check my schedule and get back to you.\" Try to treat the person the way you would want to be treated. This means that you should choose your words carefully. Be thoughtful in your response.  It\\'s okay to pause before responding. You might be taken by surprise and need a moment to collect your thoughts. Say thank you. It is a compliment to be asked out. You can say, \"I\\'m flattered. Unfortunately, I can\\'t accept.\" Don\\'t laugh. Many people laugh nervously in awkward situations. Try to avoid giggling, as that is likely to result in hurt feelings. Sometimes it is not what you say, but how you say it. If you need to reject someone, think about factors other than your words. Non-verbal communication matters, too.  Use the right tone of voice. Try to sound gentle but firm. Make eye contact. This helps convey that you are being serious, and also shows respect for the other person. If you are in public, try not to speak too loudly. It is not necessary for everyone around you to know that you are turning down a date.\\n\\nProvide a summary of the article in two or three sentences:\\n\\n'}]}\n",
            "CountTokensResult(total_tokens=730)\n",
            "{'role': 'model', 'parts': [{'text': 'Tell the truth. Use a \"compliment sandwich\". Be direct. Treat the person with respect. Communicate effectively.'}]}\n",
            "CountTokensResult(total_tokens=23)\n",
            "Num validation examples: 100\n"
          ]
        }
      ],
      "source": [
        "example_training_dataset = []\n",
        "example_validation_dataset = []\n",
        "\n",
        "try:\n",
        "    with tf.io.gfile.GFile(training_dataset_uri) as dataset_jsonl_file:\n",
        "        example_training_dataset = [\n",
        "            json.loads(dataset_line) for dataset_line in dataset_jsonl_file\n",
        "        ]\n",
        "except KeyError as e:\n",
        "    print(\n",
        "        f\"KeyError: Please check if your file '{training_dataset_uri}' is a JSONL file with correct JSON format. Error: {e}\"\n",
        "    )\n",
        "    # Exit the script if there's an error in the training data\n",
        "    import sys\n",
        "\n",
        "    sys.exit(1)\n",
        "\n",
        "print()\n",
        "\n",
        "if validation_dataset_uri:\n",
        "    try:\n",
        "        with tf.io.gfile.GFile(validation_dataset_uri) as dataset_jsonl_file:\n",
        "            example_validation_dataset = [\n",
        "                json.loads(dataset_line) for dataset_line in dataset_jsonl_file\n",
        "            ]\n",
        "    except KeyError as e:\n",
        "        print(\n",
        "            f\"KeyError: Please check if your file '{validation_dataset_uri}' is a JSONL file with correct JSON format. Error: {e}\"\n",
        "        )\n",
        "        # Exit the script if there's an error in the validation data\n",
        "        import sys\n",
        "\n",
        "        sys.exit(1)\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num training examples:\", len(example_training_dataset))\n",
        "if example_training_dataset:  # Check if the list is not empty\n",
        "    print(\"First example:\")\n",
        "    for item in example_training_dataset[0][\"contents\"]:\n",
        "        print(item)\n",
        "        text_content = item.get(\"parts\", [{}])[0].get(\"text\", \"\")\n",
        "        print(tokenizer.count_tokens(text_content))  # Make sure 'tokenizer' is defined\n",
        "\n",
        "if example_validation_dataset:\n",
        "    print(\"Num validation examples:\", len(example_validation_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5RhrH6r4NrC"
      },
      "source": [
        "You can perform various error checks to validate that each tuning example in the dataset adheres to the format expected by the tuning API. Errors are categorized based on their nature for easier debugging.  \n",
        "  \n",
        "For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about).\n",
        "\n",
        "1. **Presence of System Instruction**: Checks if there is a system instruction and if its there for all rows. System instruction is optional. Warning type: `systemInstruction is missing in some rows`.\n",
        "2. **Presence of Contents List:** Checks if a `contents` list is present in each entry. Error type: `missing_contents_list`.\n",
        "3. **Content Item Format:** Validates that each item in the `contents` list is a dictionary. Error type: `invalid_content_item`.\n",
        "4. **Content Item Format:** Validates that each item in the `contents` list is a dictionary. Error type: `invalid_content_item`.\n",
        "5. **Role Validation:** Checks if the role is one of `user`, or `model` for `contents` list and system for `systemInstruction` list. Error type: `unrecognized_role`.\n",
        "6. **Parts List Validation:** Verifies that the `parts` key contains a list. Error type: `missing_or_invalid_parts`.\n",
        "7. **Part Format:** Checks if each part in the `parts` list is a dictionary and contains the key `text`. Error type: `invalid_part`.\n",
        "8. **Text Validation:** Ensures that the `text` key has textual data and is a string. Error type: `missing_text`.\n",
        "9. **Consecutive Turns:** For the chat history, it is enforced that the message roles alternate (user, then model, then user, etc.). Error type: `consecutive_turns`. This check is not applicable for systemInstruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "S2FVNbIX0R0n"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def validate_dataset_format(dataset):\n",
        "    \"\"\"Validates the dataset.\n",
        "\n",
        "    Args:\n",
        "      dataset_uri: The dataset uri to be validated.\n",
        "    \"\"\"\n",
        "    format_errors = defaultdict(list)\n",
        "    system_instruction_missing = False  # Flag to track missing systemInstruction\n",
        "\n",
        "    if not dataset or len(dataset) == 0:\n",
        "        print(\"Input dataset file is empty or inaccessible.\")\n",
        "        return\n",
        "\n",
        "    for row_idx, example in enumerate(dataset):\n",
        "        # Verify presence of contents list\n",
        "        if not isinstance(example, dict):\n",
        "            format_errors[\"invalid_input\"].append(row_idx)\n",
        "            continue\n",
        "\n",
        "        # Check for systemInstruction and validate if present\n",
        "        system_instruction = example.get(\"systemInstruction\", None)\n",
        "        if system_instruction:\n",
        "            try:\n",
        "                # Validate the list within \"parts\"\n",
        "                validate_contents(\n",
        "                    system_instruction.get(\"parts\", []),\n",
        "                    format_errors,\n",
        "                    row_idx,\n",
        "                    is_system_instruction=True,\n",
        "                )\n",
        "            except (TypeError, AttributeError, KeyError) as e:\n",
        "                print(\"Invalid input during system instruction validation: %s\", e)\n",
        "                format_errors[\"invalid_system_instruction\"].append(row_idx)\n",
        "        else:\n",
        "            system_instruction_missing = True  # Set the flag if missing\n",
        "\n",
        "        contents = example.get(\"contents\", None)\n",
        "        if not contents:\n",
        "            format_errors[\"missing_contents_list\"].append(row_idx)\n",
        "            continue\n",
        "        try:\n",
        "            validate_contents(contents, format_errors, row_idx)\n",
        "        except (TypeError, AttributeError, KeyError) as e:\n",
        "            print(\"Invalid input during contents validation: %s\", e)\n",
        "            format_errors[\"invalid_input\"].append(row_idx)\n",
        "\n",
        "    if format_errors:\n",
        "        print(\"Found errors for this dataset:\")\n",
        "        for k, v in format_errors.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "    else:\n",
        "        print(\"No errors found for this dataset.\")\n",
        "\n",
        "    # Print warning only once after processing all rows\n",
        "    if system_instruction_missing:\n",
        "        print(\"Warning: systemInstruction is missing in some rows.\")\n",
        "\n",
        "\n",
        "def validate_contents(contents, format_errors, row_index, is_system_instruction=False):\n",
        "    \"\"\"Validates contents list format.\"\"\"\n",
        "\n",
        "    if not isinstance(contents, list):\n",
        "        format_errors[\"invalid_contents_list\"].append(row_index)\n",
        "        return\n",
        "\n",
        "    prev_role = None\n",
        "    for content_item in contents:  # Iterate over content items in the \"contents\" list\n",
        "        if not isinstance(content_item, dict):\n",
        "            format_errors[\"invalid_content_item\"].append(row_index)\n",
        "            return\n",
        "\n",
        "        # Skip key checks for system instructions\n",
        "        if not is_system_instruction and (\n",
        "            \"role\" not in content_item or \"parts\" not in content_item\n",
        "        ):\n",
        "            format_errors[\"content_item_missing_key\"].append(row_index)\n",
        "            return\n",
        "\n",
        "        # ... (rest of the validation logic remains the same)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "nSLnbeJ00R2v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found for this dataset.\n",
            "Warning: systemInstruction is missing in some rows.\n",
            "No errors found for this dataset.\n",
            "Warning: systemInstruction is missing in some rows.\n"
          ]
        }
      ],
      "source": [
        "validate_dataset_format(example_training_dataset)\n",
        "if example_validation_dataset:\n",
        "    validate_dataset_format(example_validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJFwhPid_7oG"
      },
      "source": [
        "### Utils for dataset analysis and token counting\n",
        "\n",
        "This section focuses on analyzing the structure and token counts of your datasets. You will also define some utility functions to streamline subsequent steps in the notebook.\n",
        "\n",
        "* Load and inspect sample data from the training and validation datasets.\n",
        "* Calculate token counts for messages to understand the dataset's characteristics.\n",
        "* Define utility functions for calculating token distributions and dataset statistics. These will help assess the suitability of your data for supervised tuning and estimate potential costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "al_uUWOP4Ss2"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class DatasetDistribution:\n",
        "    \"\"\"Dataset disbribution for given a population of values.\n",
        "\n",
        "    It optionally contains a histogram consists of bucketized data representing\n",
        "    the distribution of those values. The summary statistics are the sum, min,\n",
        "    max, mean, median, p5, p95.\n",
        "\n",
        "    Attributes:\n",
        "      sum: Sum of the values in the population.\n",
        "      max: Max of the values in the population.\n",
        "      min: Min of the values in the population.\n",
        "      mean: The arithmetic mean of the values in the population.\n",
        "      median: The median of the values in the population.\n",
        "      p5: P5 quantile of the values in the population.\n",
        "      p95: P95 quantile of the values in the population.\n",
        "    \"\"\"\n",
        "\n",
        "    sum: int | None = None\n",
        "    max: float | None = None\n",
        "    min: float | None = None\n",
        "    mean: float | None = None\n",
        "    median: float | None = None\n",
        "    p5: float | None = None\n",
        "    p95: float | None = None\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class DatasetStatistics:\n",
        "    \"\"\"Dataset statistics used for dataset profiling.\n",
        "\n",
        "    Attributes:\n",
        "      total_number_of_dataset_examples: Number of tuning examples in the dataset.\n",
        "      total_number_of_records_for_training: Number of tuning records after\n",
        "        formatting. Each model turn in the chat message will be considered as a record for tuning.\n",
        "      total_number_of_billable_tokens: Number of total billable tokens in the\n",
        "        dataset.\n",
        "      user_input_token_length_stats: Stats for input token length.\n",
        "      user_output_token_length_stats: Stats for output token length.\n",
        "    \"\"\"\n",
        "\n",
        "    total_number_of_dataset_examples: int | None = None\n",
        "    total_number_of_records_for_training: int | None = None\n",
        "    total_number_of_billable_tokens: int | None = None\n",
        "    user_input_token_length_stats: DatasetDistribution | None = None\n",
        "    user_output_token_length_stats: DatasetDistribution | None = None\n",
        "\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 32 * 1024\n",
        "ESTIMATE_PADDING_TOKEN_PER_EXAMPLE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "21xgvR3g0R5P"
      },
      "outputs": [],
      "source": [
        "def calculate_distribution_for_population(population) -> DatasetDistribution:\n",
        "    \"\"\"Calculates the distribution from the population of values.\n",
        "\n",
        "    Args:\n",
        "      population: The population of values to calculate distribution for.\n",
        "\n",
        "    Returns:\n",
        "      DatasetDistribution of the given population of values.\n",
        "    \"\"\"\n",
        "    if not population:\n",
        "        raise ValueError(\"population is empty\")\n",
        "\n",
        "    return DatasetDistribution(\n",
        "        sum=np.sum(population),\n",
        "        max=np.max(population),\n",
        "        min=np.min(population),\n",
        "        mean=np.mean(population),\n",
        "        median=np.median(population),\n",
        "        p5=np.percentile(population, 5, method=\"nearest\"),\n",
        "        p95=np.percentile(population, 95, method=\"nearest\"),\n",
        "    )\n",
        "\n",
        "\n",
        "def get_token_distribution_for_one_tuning_dataset_example(example):\n",
        "    model_turn_token_list = []\n",
        "    input_token_list = []\n",
        "    input = []\n",
        "    n_too_long = 0\n",
        "    number_of_records_for_training = 0  # each model turn in the chat message will be considered as a record for tuning\n",
        "\n",
        "    # Handle optional systemInstruction\n",
        "    system_instruction = example.get(\"systemInstruction\")\n",
        "    if system_instruction:\n",
        "        text = system_instruction.get(\"parts\")[0].get(\n",
        "            \"text\"\n",
        "        )  # Assuming single part in system instruction\n",
        "        input.append(Content(role=\"system\", parts=[Part.from_text(text)]))\n",
        "\n",
        "    for content_item in example[\"contents\"]:\n",
        "        role = content_item.get(\"role\").lower()\n",
        "        text = content_item.get(\"parts\")[0].get(\n",
        "            \"text\"\n",
        "        )  # Assuming single part in content item\n",
        "\n",
        "        if role.lower() == \"model\":\n",
        "            result = tokenizer.count_tokens(input)\n",
        "            input_token_list.append(result.total_tokens)\n",
        "            model_turn_token_list.append(tokenizer.count_tokens(text).total_tokens)\n",
        "            number_of_records_for_training += 1\n",
        "            if (\n",
        "                result.total_tokens + tokenizer.count_tokens(text).total_tokens\n",
        "                > MAX_TOKENS_PER_EXAMPLE\n",
        "            ):\n",
        "                n_too_long += 1\n",
        "                break\n",
        "\n",
        "        input.append(Content(role=role, parts=[Part.from_text(text)]))\n",
        "\n",
        "    return (\n",
        "        input_token_list,\n",
        "        model_turn_token_list,\n",
        "        number_of_records_for_training,\n",
        "        np.sum(model_turn_token_list) + np.sum(input_token_list),\n",
        "        n_too_long,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_dataset_stats_for_dataset(dataset):\n",
        "    results = map(get_token_distribution_for_one_tuning_dataset_example, dataset)\n",
        "    user_input_token_list = []\n",
        "    model_turn_token_list = []\n",
        "    number_of_records_for_training = 0\n",
        "    total_number_of_billable_tokens = 0\n",
        "    n_too_long_for_dataset = 0\n",
        "    for (\n",
        "        input_token_list_per_example,\n",
        "        model_turn_token_list_per_example,\n",
        "        number_of_records_for_training_per_example,\n",
        "        number_of_billable_token_per_example,\n",
        "        n_too_long,\n",
        "    ) in results:\n",
        "        user_input_token_list.extend(input_token_list_per_example)\n",
        "        model_turn_token_list.extend(model_turn_token_list_per_example)\n",
        "        number_of_records_for_training += number_of_records_for_training_per_example\n",
        "        total_number_of_billable_tokens += number_of_billable_token_per_example\n",
        "        n_too_long_for_dataset += n_too_long\n",
        "\n",
        "    print(\n",
        "        f\"\\n{n_too_long_for_dataset} examples may be over the {MAX_TOKENS_PER_EXAMPLE} token limit, they will be truncated during tuning.\"\n",
        "    )\n",
        "\n",
        "    return DatasetStatistics(\n",
        "        total_number_of_dataset_examples=len(dataset),\n",
        "        total_number_of_records_for_training=number_of_records_for_training,\n",
        "        total_number_of_billable_tokens=total_number_of_billable_tokens\n",
        "        + number_of_records_for_training * ESTIMATE_PADDING_TOKEN_PER_EXAMPLE,\n",
        "        user_input_token_length_stats=calculate_distribution_for_population(\n",
        "            user_input_token_list\n",
        "        ),\n",
        "        user_output_token_length_stats=calculate_distribution_for_population(\n",
        "            model_turn_token_list\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def print_dataset_stats(dataset):\n",
        "    dataset_stats = get_dataset_stats_for_dataset(dataset)\n",
        "    print(\"Below you can find the dataset statistics:\")\n",
        "    print(\n",
        "        f\"Total number of examples in the dataset: {dataset_stats.total_number_of_dataset_examples}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Total number of records for training: {dataset_stats.total_number_of_records_for_training}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Total number of billable tokens in the dataset: {dataset_stats.total_number_of_billable_tokens}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"User input token length distribution: {dataset_stats.user_input_token_length_stats}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"User output token length distribution: {dataset_stats.user_output_token_length_stats}\"\n",
        "    )\n",
        "    return dataset_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FF4ReY6Atw3"
      },
      "source": [
        "Next you can analyze the structure and token counts of your datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "sZqsWno60R7O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0 examples may be over the 32768 token limit, they will be truncated during tuning.\n",
            "Below you can find the dataset statistics:\n",
            "Total number of examples in the dataset: 500\n",
            "Total number of records for training: 500\n",
            "Total number of billable tokens in the dataset: 259243\n",
            "User input token length distribution: DatasetDistribution(sum=233592, max=2932, min=25, mean=467.184, median=414.5, p5=101, p95=1002)\n",
            "User output token length distribution: DatasetDistribution(sum=21651, max=237, min=3, mean=43.302, median=37.0, p5=15, p95=89)\n",
            "\n",
            "0 examples may be over the 32768 token limit, they will be truncated during tuning.\n",
            "Below you can find the dataset statistics:\n",
            "Total number of examples in the dataset: 100\n",
            "Total number of records for training: 100\n",
            "Total number of billable tokens in the dataset: 50154\n",
            "User input token length distribution: DatasetDistribution(sum=45535, max=1418, min=29, mean=455.35, median=413.5, p5=145, p95=846)\n",
            "User output token length distribution: DatasetDistribution(sum=3819, max=165, min=8, mean=38.19, median=32.0, p5=17, p95=76)\n"
          ]
        }
      ],
      "source": [
        "training_dataset_stats = print_dataset_stats(example_training_dataset)\n",
        "\n",
        "if example_validation_dataset:\n",
        "    validation_dataset_stats = print_dataset_stats(example_validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFWbXu17DfiS"
      },
      "source": [
        "### Cost Estimation for Supervised Fine-tuning\n",
        "In this final section, you will estimate the total cost for supervised fine-tuning based on the number of tokens processed. The number of tokens used will be charged to you. Please refer to the [pricing page for the rate](https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models).\n",
        "\n",
        "**Important Note:** The final cost may vary slightly from this estimate due to dataset formatting and truncation logic during training.\n",
        "\n",
        "The code calculates the total number of billable tokens by summing up the tokens from the training dataset and (if provided) the validation dataset. Then, it estimates the total cost by multiplying the total billable tokens with the number of training epochs (default is 4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "k3ZJ_8fQ0R9x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~309397 tokens that will be charged\n",
            "By default, you'll train for 4 epochs on this dataset.\n",
            "By default, you'll be charged for ~1237588 tokens.\n"
          ]
        }
      ],
      "source": [
        "epoch_count = 4  # @param {type:\"integer\"}\n",
        "if epoch_count is None:\n",
        "    epoch_count = 4\n",
        "\n",
        "\n",
        "total_number_of_billable_tokens = training_dataset_stats.total_number_of_billable_tokens\n",
        "\n",
        "\n",
        "if validation_dataset_stats:\n",
        "    total_number_of_billable_tokens += (\n",
        "        validation_dataset_stats.total_number_of_billable_tokens\n",
        "    )\n",
        "\n",
        "print(f\"Dataset has ~{total_number_of_billable_tokens} tokens that will be charged\")\n",
        "print(f\"By default, you'll train for {epoch_count} epochs on this dataset.\")\n",
        "print(\n",
        "    f\"By default, you'll be charged for ~{epoch_count * total_number_of_billable_tokens} tokens.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1EMMeRfH14a"
      },
      "source": [
        "## Convert `Gemini 1.0 Pro` fine-tuning dataset to `Gemini 1.5 Pro` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "oOKZgdSLJUFx"
      },
      "outputs": [],
      "source": [
        "source_uri = (\n",
        "    \"gs://next-23-tuning-demo/example-fine-tuning.json\"  # @param {type:\"string\"}\n",
        ")\n",
        "destination_uri = (\n",
        "    \"gs://next-23-tuning-demo/new-data-format.jsonl\"  # @param {type:\"string\"}\n",
        ")\n",
        "system_instruction = \"You are a helpful and friendly AI assistant\"  # Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "fgNjg3Y4CSq8"
      },
      "outputs": [],
      "source": [
        "def convert_jsonl_format(\n",
        "    source_uri: str,\n",
        "    destination_uri: str,\n",
        "    system_instruction: str = None,\n",
        "):\n",
        "    \"\"\"Converts a JSONL file from the old format to the new format.\n",
        "\n",
        "    Args:\n",
        "        source_uri: Google Cloud Storage URI of the source JSONL file.\n",
        "        destination_uri: Google Cloud Storage URI for the new JSONL file.\n",
        "        system_instruction: Optional system instruction text.\n",
        "                            If provided, it will be added as \"systemInstruction\" in the new format.\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Extract bucket and file name from source URI\n",
        "    source_bucket_name, source_blob_name = extract_bucket_and_blob_name(source_uri)\n",
        "    source_bucket = storage_client.bucket(source_bucket_name)\n",
        "    source_blob = source_bucket.blob(source_blob_name)\n",
        "\n",
        "    # Extract bucket and file name from destination URI\n",
        "    dest_bucket_name, dest_blob_name = extract_bucket_and_blob_name(destination_uri)\n",
        "    dest_bucket = storage_client.bucket(dest_bucket_name)\n",
        "    dest_blob = dest_bucket.blob(dest_blob_name)\n",
        "\n",
        "    # Download the source JSONL file\n",
        "    source_data = source_blob.download_as_string().decode(\"utf-8\")\n",
        "\n",
        "    new_data = []\n",
        "    for line in source_data.splitlines():\n",
        "        try:\n",
        "            json_data = json.loads(line)\n",
        "            new_json_data = convert_json_object(json_data, system_instruction)\n",
        "            new_data.append(new_json_data)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Skipping invalid JSON line: {line} - Error: {e}\")\n",
        "\n",
        "    # Upload the new JSONL file\n",
        "    new_data_str = \"\\n\".join([json.dumps(data) for data in new_data])\n",
        "    dest_blob.upload_from_string(new_data_str)\n",
        "\n",
        "    print(f\"Successfully converted and uploaded to {destination_uri}\")\n",
        "\n",
        "\n",
        "def convert_json_object(json_data: dict, system_instruction: str = None) -> dict:\n",
        "    \"\"\"Converts a single JSON object from the old format to the new format.\n",
        "\n",
        "    Args:\n",
        "        json_data: The JSON object to convert.\n",
        "        system_instruction: Optional system instruction text.\n",
        "\n",
        "    Returns:\n",
        "        The converted JSON object.\n",
        "    \"\"\"\n",
        "    new_json_data = {}  # Create an empty dict instead of initializing with \"contents\"\n",
        "\n",
        "    if system_instruction:\n",
        "        new_json_data[\"systemInstruction\"] = {\n",
        "            \"role\": \"system\",\n",
        "            \"parts\": [{\"text\": system_instruction}],\n",
        "        }\n",
        "\n",
        "    new_json_data[\"contents\"] = []  # Initialize \"contents\" after \"systemInstruction\"\n",
        "\n",
        "    for message in json_data.get(\"messages\", []):\n",
        "        new_message = {\"role\": message[\"role\"], \"parts\": [{\"text\": message[\"content\"]}]}\n",
        "        new_json_data[\"contents\"].append(new_message)\n",
        "\n",
        "    return new_json_data\n",
        "\n",
        "\n",
        "def extract_bucket_and_blob_name(gcs_uri: str) -> tuple:\n",
        "    \"\"\"Extracts the bucket name and blob name from a Google Cloud Storage URI.\n",
        "\n",
        "    Args:\n",
        "        gcs_uri: The Google Cloud Storage URI (e.g., \"gs://my-bucket/my-file.jsonl\")\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the bucket name and blob name.\n",
        "    \"\"\"\n",
        "    if not gcs_uri.startswith(\"gs://\"):\n",
        "        raise ValueError(\"Invalid Google Cloud Storage URI\")\n",
        "    parts = gcs_uri[5:].split(\"/\", 1)\n",
        "    return parts[0], parts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "WAqrR4yDH1LT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully converted and uploaded to gs://next-23-tuning-demo/new-data-format.jsonl\n"
          ]
        }
      ],
      "source": [
        "convert_jsonl_format(source_uri, destination_uri, system_instruction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k1GJaFIEvd-"
      },
      "source": [
        "## Tuning token count and cost estimation for `Gemini 1.0 Pro` legacy users.\n",
        "\n",
        "Only use this part if you still use `Gemini 1.0 Pro`. Its best to upgrade to using [`gemini-1.5-pro-002`](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sgetU8OTJ7X"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Define the Google Cloud Storage URIs pointing to your training and validation datasets or continue using the URIs provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0JwfuPSSofK"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"gemini-1.0-pro-002\"  # @param ['gemini-1.0-pro-002']{type:\"string\"}\n",
        "training_dataset_uri = \"gs://cloud-samples-data/ai-platform/generative_ai/sft_train_data.jsonl\"  # @param {type:\"string\"}\n",
        "validation_dataset_uri = \"gs://cloud-samples-data/ai-platform/generative_ai/sft_validation_data.jsonl\"  # @param {type:\"string\"}\n",
        "\n",
        "tokenizer = get_tokenizer_for_model(BASE_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pHktd7HVao7"
      },
      "source": [
        "We'll now load the dataset and conduct some basic statistical analysis to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTvunHqRTHqe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num training examples: 500\n",
            "First example:\n",
            "{'role': 'user', 'content': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\\n\\nProvide a summary of the article in two or three sentences:\\n\\n\"}\n",
            "CountTokensResult(total_tokens=277)\n",
            "{'role': 'model', 'content': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises them to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\"}\n",
            "CountTokensResult(total_tokens=41)\n",
            "Num validation examples: 100\n"
          ]
        }
      ],
      "source": [
        "with tf.io.gfile.GFile(training_dataset_uri) as dataset_jsonl_file:\n",
        "    example_training_dataset = [\n",
        "        json.loads(dataset_line) for dataset_line in dataset_jsonl_file\n",
        "    ]\n",
        "\n",
        "if validation_dataset_uri:\n",
        "    with tf.io.gfile.GFile(validation_dataset_uri) as dataset_jsonl_file:\n",
        "        example_validation_dataset = [\n",
        "            json.loads(dataset_line) for dataset_line in dataset_jsonl_file\n",
        "        ]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num training examples:\", len(example_training_dataset))\n",
        "print(\"First example:\")\n",
        "for message in example_training_dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "    print(tokenizer.count_tokens(message.get(\"content\")))\n",
        "\n",
        "if example_validation_dataset:\n",
        "    print(\"Num validation examples:\", len(example_validation_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ytlPXmKV3K2"
      },
      "source": [
        "### Validate the format of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cOkGZxNV4Ed"
      },
      "source": [
        "You can perform various error checks to validate that each tuning example in the dataset adheres to the format expected by the tuning API. Errors are categorized based on their nature for easier debugging.  \n",
        "  \n",
        "For how to prepare dataset for tuning gemini, please refer to this [tutorial](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about).\n",
        "\n",
        "1. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`:\n",
        "2. **Message Keys Check**: Validates that each message in the messages list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
        "3. **Role Validation**: Ensures the role is one of `system`, `user`, or `model`. Error type: `unrecognized_role`. Note: only the first message can have `system` as role.\n",
        "5. **Content Validation**: Verifies that content has textual data and is a string. Error type: `missing_content`.\n",
        "6. **Consecutive Turns**. For the chat history, it is enforced that the message must can repeat in an alternating manner. Error type: `consecutive_turns`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mzpB9PUVp5z"
      },
      "outputs": [],
      "source": [
        "def validate_dataset_format(dataset):\n",
        "    \"\"\"Validates the dataset.\n",
        "\n",
        "    Args:\n",
        "      dataset_uri: The dataset uri to be validated.\n",
        "    \"\"\"\n",
        "    format_errors = defaultdict(list)\n",
        "    if not dataset or len(dataset) == 0:\n",
        "        print(\"Input dataset file is empty or inaccessible.\")\n",
        "        return\n",
        "\n",
        "    for row_idx, example in enumerate(dataset):\n",
        "        # Verify presence of messages list\n",
        "        if not isinstance(example, dict):\n",
        "            format_errors[\"missing_messages_list\"].append(row_idx)\n",
        "            continue\n",
        "        messages = example.get(\"messages\", None)\n",
        "        try:\n",
        "            validate_messages(messages, format_errors, row_idx)\n",
        "        except (TypeError, AttributeError, KeyError) as e:\n",
        "            print(\"Invalid input during validation: %s\", e)\n",
        "            format_errors[\"invalid_input\"].append(row_idx)\n",
        "\n",
        "    if format_errors:\n",
        "        print(\"Found errors for this dataset:\")\n",
        "        for k, v in format_errors.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "    else:\n",
        "        print(\"No errors found for this dataset.\")\n",
        "\n",
        "\n",
        "def validate_messages(messages, format_errors, row_index):\n",
        "    \"\"\"Validates messages list format.\"\"\"\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"].append(row_index)\n",
        "        return\n",
        "\n",
        "    # Check if the first role is for system instruction\n",
        "    if messages[0].get(\"role\", \"\").lower() == \"system\":\n",
        "        messages = messages[1:]\n",
        "    else:\n",
        "        messages = messages[:]\n",
        "\n",
        "    prev_role = None\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"].append(row_index)\n",
        "            return\n",
        "\n",
        "        if message.get(\"role\", \"\").lower() not in (\"user\", \"model\"):\n",
        "            format_errors[\"unrecognized_role\"].append(row_index)\n",
        "            return\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content:\n",
        "            format_errors[\"missing_content\"].append(row_index)\n",
        "            return\n",
        "\n",
        "            role = message.get(\"role\", \"\").lower()\n",
        "            # messages to have alternate turns.\n",
        "            if role == prev_role:\n",
        "                format_errors[\"consecutive_turns\"].append(row_index)\n",
        "                return\n",
        "\n",
        "            prev_role = role"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpcJAtzhOKbv"
      },
      "source": [
        "Now you can check the data for any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUCpEmEFM0eX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found for this dataset.\n",
            "No errors found for this dataset.\n"
          ]
        }
      ],
      "source": [
        "validate_dataset_format(example_training_dataset)\n",
        "if example_validation_dataset:\n",
        "    validate_dataset_format(example_validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDlwyek4OUE2"
      },
      "source": [
        "### Utils for dataset analysis and token counting\n",
        "\n",
        "This section focuses on analyzing the structure and token counts of your datasets. You will also define some utility functions to streamline subsequent steps in the notebook.\n",
        "\n",
        "* Load and inspect sample data from the training and validation datasets.\n",
        "* Calculate token counts for messages to understand the dataset's characteristics.\n",
        "* Define utility functions for calculating token distributions and dataset statistics. These will help assess the suitability of your data for supervised tuning and estimate potential costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgFmhH2XOdzu"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class DatasetDistribution:\n",
        "    \"\"\"Dataset disbribution for given a population of values.\n",
        "\n",
        "    It optionally contains a histogram consists of bucketized data representing\n",
        "    the distribution of those values. The summary statistics are the sum, min,\n",
        "    max, mean, median, p5, p95.\n",
        "\n",
        "    Attributes:\n",
        "      sum: Sum of the values in the population.\n",
        "      max: Max of the values in the population.\n",
        "      min: Min of the values in the population.\n",
        "      mean: The arithmetic mean of the values in the population.\n",
        "      median: The median of the values in the population.\n",
        "      p5: P5 quantile of the values in the population.\n",
        "      p95: P95 quantile of the values in the population.\n",
        "    \"\"\"\n",
        "\n",
        "    sum: int | None = None\n",
        "    max: float | None = None\n",
        "    min: float | None = None\n",
        "    mean: float | None = None\n",
        "    median: float | None = None\n",
        "    p5: float | None = None\n",
        "    p95: float | None = None\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class DatasetStatistics:\n",
        "    \"\"\"Dataset statistics used for dataset profiling.\n",
        "\n",
        "    Attributes:\n",
        "      total_number_of_dataset_examples: Number of tuning examples in the dataset.\n",
        "      total_number_of_records_for_training: Number of tuning records after\n",
        "        formatting. Each model turn in the chat message will be considered as a record for tuning.\n",
        "      total_number_of_billable_tokens: Number of total billable tokens in the\n",
        "        dataset.\n",
        "      user_input_token_length_stats: Stats for input token length.\n",
        "      user_output_token_length_stats: Stats for output token length.\n",
        "    \"\"\"\n",
        "\n",
        "    total_number_of_dataset_examples: int | None = None\n",
        "    total_number_of_records_for_training: int | None = None\n",
        "    total_number_of_billable_tokens: int | None = None\n",
        "    user_input_token_length_stats: DatasetDistribution | None = None\n",
        "    user_output_token_length_stats: DatasetDistribution | None = None\n",
        "\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 32 * 1024\n",
        "ESTIMATE_PADDING_TOKEN_PER_EXAMPLE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm0Jwzt0RDGd"
      },
      "outputs": [],
      "source": [
        "def calculate_distribution_for_population(population) -> DatasetDistribution:\n",
        "    \"\"\"Calculates the distribution from the population of values.\n",
        "\n",
        "    Args:\n",
        "      population: The population of values to calculate distribution for.\n",
        "\n",
        "    Returns:\n",
        "      DatasetDistribution of the given population of values.\n",
        "    \"\"\"\n",
        "    if not population:\n",
        "        raise ValueError(\"population is empty\")\n",
        "\n",
        "    return DatasetDistribution(\n",
        "        sum=np.sum(population),\n",
        "        max=np.max(population),\n",
        "        min=np.min(population),\n",
        "        mean=np.mean(population),\n",
        "        median=np.median(population),\n",
        "        p5=np.percentile(population, 5, method=\"nearest\"),\n",
        "        p95=np.percentile(population, 95, method=\"nearest\"),\n",
        "    )\n",
        "\n",
        "\n",
        "def get_token_distribution_for_one_tuning_dataset_example(example):\n",
        "    model_turn_token_list = []\n",
        "    input_token_list = []\n",
        "    input = []\n",
        "    n_too_long = 0\n",
        "    number_of_records_for_training = 0  # each model turn in the chat message will be considered as a record for tuning\n",
        "    for message in example[\"messages\"]:\n",
        "        role = message.get(\"role\").lower()\n",
        "        text = message.get(\"content\")\n",
        "\n",
        "        if role.lower() == \"model\":\n",
        "            result = tokenizer.count_tokens(input)\n",
        "            input_token_list.append(result.total_tokens)\n",
        "            model_turn_token_list.append(tokenizer.count_tokens(text).total_tokens)\n",
        "            number_of_records_for_training += 1\n",
        "            if (\n",
        "                result.total_tokens + tokenizer.count_tokens(text).total_tokens\n",
        "                > MAX_TOKENS_PER_EXAMPLE\n",
        "            ):\n",
        "                n_too_long += 1\n",
        "                break\n",
        "\n",
        "        input.append(Content(role=role, parts=[Part.from_text(text)]))\n",
        "\n",
        "    return (\n",
        "        input_token_list,\n",
        "        model_turn_token_list,\n",
        "        number_of_records_for_training,\n",
        "        np.sum(model_turn_token_list) + np.sum(input_token_list),\n",
        "        n_too_long,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_dataset_stats_for_dataset(dataset):\n",
        "    results = map(get_token_distribution_for_one_tuning_dataset_example, dataset)\n",
        "    user_input_token_list = []\n",
        "    model_turn_token_list = []\n",
        "    number_of_records_for_training = 0\n",
        "    total_number_of_billable_tokens = 0\n",
        "    n_too_long_for_dataset = 0\n",
        "    for (\n",
        "        input_token_list_per_example,\n",
        "        model_turn_token_list_per_example,\n",
        "        number_of_records_for_training_per_example,\n",
        "        number_of_billable_token_per_example,\n",
        "        n_too_long,\n",
        "    ) in results:\n",
        "        user_input_token_list.extend(input_token_list_per_example)\n",
        "        model_turn_token_list.extend(model_turn_token_list_per_example)\n",
        "        number_of_records_for_training += number_of_records_for_training_per_example\n",
        "        total_number_of_billable_tokens += number_of_billable_token_per_example\n",
        "        n_too_long_for_dataset += n_too_long\n",
        "\n",
        "    print(\n",
        "        f\"\\n{n_too_long_for_dataset} examples may be over the {MAX_TOKENS_PER_EXAMPLE} token limit, they will be truncated during tuning.\"\n",
        "    )\n",
        "\n",
        "    return DatasetStatistics(\n",
        "        total_number_of_dataset_examples=len(dataset),\n",
        "        total_number_of_records_for_training=number_of_records_for_training,\n",
        "        total_number_of_billable_tokens=total_number_of_billable_tokens\n",
        "        + number_of_records_for_training * ESTIMATE_PADDING_TOKEN_PER_EXAMPLE,\n",
        "        user_input_token_length_stats=calculate_distribution_for_population(\n",
        "            user_input_token_list\n",
        "        ),\n",
        "        user_output_token_length_stats=calculate_distribution_for_population(\n",
        "            model_turn_token_list\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def print_dataset_stats(dataset):\n",
        "    dataset_stats = get_dataset_stats_for_dataset(dataset)\n",
        "    print(\"Below you can find the dataset statistics:\")\n",
        "    print(\n",
        "        f\"Total number of examples in the dataset: {dataset_stats.total_number_of_dataset_examples}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Total number of records for training: {dataset_stats.total_number_of_records_for_training}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Total number of billable tokens in the dataset: {dataset_stats.total_number_of_billable_tokens}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"User input token length distribution: {dataset_stats.user_input_token_length_stats}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"User output token length distribution: {dataset_stats.user_output_token_length_stats}\"\n",
        "    )\n",
        "    return dataset_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSn_0r1BXo0i"
      },
      "source": [
        "Next you can analyze the structure and token counts of your datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOWsUbwVXoTU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0 examples may be over the 32768 token limit, they will be truncated during tuning.\n",
            "Below you can find the dataset statistics:\n",
            "Total number of examples in the dataset: 500\n",
            "Total number of records for training: 500\n",
            "Total number of billable tokens in the dataset: 130300\n",
            "User input token length distribution: DatasetDistribution(sum=109172, max=712, min=70, mean=218.344, median=198.5, p5=89, p95=403)\n",
            "User output token length distribution: DatasetDistribution(sum=17128, max=124, min=12, mean=34.256, median=31.0, p5=17, p95=63)\n",
            "\n",
            "0 examples may be over the 32768 token limit, they will be truncated during tuning.\n",
            "Below you can find the dataset statistics:\n",
            "Total number of examples in the dataset: 100\n",
            "Total number of records for training: 100\n",
            "Total number of billable tokens in the dataset: 28414\n",
            "User input token length distribution: DatasetDistribution(sum=23922, max=829, min=70, mean=239.22, median=225.5, p5=92, p95=430)\n",
            "User output token length distribution: DatasetDistribution(sum=3692, max=93, min=12, mean=36.92, median=36.0, p5=17, p95=63)\n"
          ]
        }
      ],
      "source": [
        "training_dataset_stats = print_dataset_stats(example_training_dataset)\n",
        "\n",
        "if example_validation_dataset:\n",
        "    validation_dataset_stats = print_dataset_stats(example_validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA6Eb-svYYN8"
      },
      "source": [
        "### Cost Estimation for Supervised Fine-tuning\n",
        "In this final section, you will estimate the total cost for supervised fine-tuning based on the number of tokens processed. The number of tokens used will be charged to you. Please refer to the [pricing page for the rate](https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models).\n",
        "\n",
        "**Important Note:** The final cost may vary slightly from this estimate due to dataset formatting and truncation logic during training.\n",
        "\n",
        "The code calculates the total number of billable tokens by summing up the tokens from the training dataset and (if provided) the validation dataset. Then, it estimates the total cost by multiplying the total billable tokens with the number of training epochs (default is 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YwYMYE-Wrr_"
      },
      "source": [
        "### Cost estimation\n",
        "\n",
        "In this final section, you will estimate the total number of tokens used for supervised tuning. The number of tokens will be charged to you.\n",
        "\n",
        "There might be a slight difference between the estimation and actual cost due to dataset formatting and truncation logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVIpbaGYRJQc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~158714 tokens that will be charged\n",
            "By default, you'll train for 4 epochs on this dataset.\n",
            "By default, you'll be charged for ~634856 tokens.\n"
          ]
        }
      ],
      "source": [
        "epoch_count = 4  # @param {type:\"integer\"}\n",
        "if epoch_count is None:\n",
        "    epoch_count = 4\n",
        "\n",
        "\n",
        "total_number_of_billable_tokens = training_dataset_stats.total_number_of_billable_tokens\n",
        "\n",
        "\n",
        "if validation_dataset_stats:\n",
        "    total_number_of_billable_tokens += (\n",
        "        validation_dataset_stats.total_number_of_billable_tokens\n",
        "    )\n",
        "\n",
        "print(f\"Dataset has ~{total_number_of_billable_tokens} tokens that will be charged\")\n",
        "print(f\"By default, you'll train for {epoch_count} epochs on this dataset.\")\n",
        "print(\n",
        "    f\"By default, you'll be charged for ~{epoch_count * total_number_of_billable_tokens} tokens.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "R5Xep4W9lq-Z",
        "dmWOrTJ3gx13",
        "DF4l8DTdWgPY"
      ],
      "name": "vertexai_supervised_tuning_token_count_and_cost_estimation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
