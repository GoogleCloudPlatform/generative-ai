{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acrGm0JPOKS5"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXVx3nfOOKS7"
      },
      "source": [
        "# Get Started with Gemini Preference Optimization\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/dpo_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fdpo_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/dpo_gemini.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/dpo_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://storage.googleapis.com/github-repo/generative-ai/logos/GitHub_Invertocat_Dark.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWOZOJjqOKS7"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| James Su, [Ivan Nardini](https://github.com/inardini) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niHZi5ftOKS7"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows you how to teach Gemini to generate better responses by showing it examples of what humans prefer.\n",
        "\n",
        "### What is Preference Optimization?\n",
        "\n",
        "Instead of labeling responses as \"correct\" or \"incorrect,\" preference optimization works with human preferences. You show the model pairs of responses to the same question—one that humans preferred and one they didn't—and the model learns to generate responses more like the preferred ones.\n",
        "\n",
        "Think of it like this: rather than teaching a student the \"right answer,\" you're showing them two essays and saying \"this style is better than that style.\"\n",
        "\n",
        "### What you'll learn\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "1. Load a dataset with human preference ratings\n",
        "2. Train Gemini to align with those preferences\n",
        "3. See a clear before-and-after comparison of model outputs\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This tutorial uses experimental features. They work great for learning and prototyping, but check the latest docs before using in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG0krygnOKS8"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxx1rwJkZjO-"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "- A Google Cloud project with billing enabled\n",
        "- The Vertex AI API enabled ([enable it here](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com))\n",
        "- No machine learning experience required!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb0a5lzIOKS8"
      },
      "source": [
        "### Install required packages\n",
        "\n",
        "We'll install three packages:\n",
        "- `google-genai`: The Gemini SDK for Python\n",
        "- `google-cloud-aiplatform`: Vertex AI client library\n",
        "- `datasets`: HuggingFace library to easily load training data\n",
        "\n",
        "**⚠️ Expected Behavior**: After running this cell, Colab will ask you to **restart the runtime**. This is normal and necessary—click the \"Restart Runtime\" button when you see it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfxF-BvuOKS8"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai google-cloud-aiplatform datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvNbfnGBOKS8"
      },
      "source": [
        "### Authenticate (Colab only)\n",
        "\n",
        "If you're running this in Google Colab, you need to authenticate so the notebook can access your Google Cloud project. This cell will prompt you to sign in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a30oTrxLOKS8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    print(\"✅ Authentication successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G0StuemOKS8"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "Replace `[your-project-id]` below with your actual Google Cloud project ID. You can find this in the [Google Cloud Console](https://console.cloud.google.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYD8PtwFOKS9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "\n",
        "# TODO: Replace with your actual project ID\n",
        "# fmt: off\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "# fmt: on\n",
        "\n",
        "# Auto-detect from environment if not set\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"\"))\n",
        "\n",
        "if not PROJECT_ID:\n",
        "    raise ValueError(\"Please set your PROJECT_ID above\")\n",
        "\n",
        "# Region where the model will be tuned (us-central1 has the best availability)\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# Create a unique bucket name using your project ID\n",
        "# Bucket names must be globally unique across all of Google Cloud\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-preference-tuning\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "print(f\"📦 Creating GCS bucket: {BUCKET_NAME}...\")\n",
        "\n",
        "# Create the bucket (the 2>/dev/null hides the error if it already exists)\n",
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI} 2>/dev/null || echo \"(Bucket already exists, continuing...)\"\n",
        "\n",
        "# Initialize the Gemini client with your project\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "print(f\"✅ Using project: {PROJECT_ID}\")\n",
        "print(f\"✅ Using region: {LOCATION}\")\n",
        "print(f\"✅ Bucket: {BUCKET_URI}\")\n",
        "print(print(\"✅ Client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5l-wRGOKS9"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Now we'll import the Python libraries we need and set up the Gemini client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IGsi7QaOKS9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "import requests\n",
        "from IPython.display import Markdown, display\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"✅ Libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cFpT7xOKS9"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Load the training dataset\n",
        "\n",
        "We'll use **[UltraFeedback](https://huggingface.co/datasets/zhengr/ultrafeedback_binarized)**, a dataset containing thousands of prompts with multiple AI-generated responses rated by humans. For each prompt, humans indicated which response they preferred.\n",
        "\n",
        "This is perfect for teaching Gemini what kind of responses humans find more helpful.\n",
        "\n",
        "**What's in this dataset?**\n",
        "- `prompt`: The user's question\n",
        "- `chosen`: The response humans preferred (scored higher)\n",
        "- `rejected`: The response humans didn't prefer (scored lower)\n",
        "- `score_chosen` and `score_rejected`: The numerical ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9OPLbJuOKS9"
      },
      "outputs": [],
      "source": [
        "print(\"📥 Loading UltraFeedback dataset from HuggingFace...\")\n",
        "print(\"(This may take 1-2 minutes on first load)\\n\")\n",
        "\n",
        "# Load the dataset\n",
        "# The dataset has already been processed to identify preferred vs rejected responses\n",
        "dataset = load_dataset(\"zhengr/ultrafeedback_binarized\")\n",
        "\n",
        "print(\"✅ Dataset loaded!\\n\")\n",
        "print(f\"Training examples: {len(dataset['train_prefs']):,}\")\n",
        "print(f\"Test examples: {len(dataset['test_prefs']):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kHYzi0EOKS9"
      },
      "source": [
        "Let's look at one example to understand the structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9JB-gtbOKS9"
      },
      "outputs": [],
      "source": [
        "# Grab the first example from the training set\n",
        "sample = dataset[\"train_prefs\"][0]\n",
        "\n",
        "print(\"📋 Example from the dataset:\\n\")\n",
        "print(f\"Prompt: {sample['prompt']}\\n\")\n",
        "print(f\"Chosen score: {sample.get('score_chosen', 'N/A')}\")\n",
        "print(f\"Rejected score: {sample.get('score_rejected', 'N/A')}\\n\")\n",
        "print(\"Full structure:\")\n",
        "print(json.dumps(sample, indent=2)[:1200] + \"\\n...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L04UNQXUOKS9"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 2: Transform the data for Gemini\n",
        "\n",
        "The dataset is in a generic format, but Gemini's tuning API expects a specific structure. We need to transform each example.\n",
        "\n",
        "**From UltraFeedback format:**\n",
        "```json\n",
        "{\n",
        "  \"chosen\": [{\"role\": \"user\", \"content\": \"...\"},\n",
        "             {\"role\": \"assistant\", \"content\": \"...\"}],\n",
        "  \"rejected\": [{\"role\": \"user\", \"content\": \"...\"},\n",
        "               {\"role\": \"assistant\", \"content\": \"...\"}]\n",
        "}\n",
        "```\n",
        "\n",
        "**To Gemini format:**\n",
        "```json\n",
        "{\n",
        "  \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"...\"}]}],\n",
        "  \"completions\": [\n",
        "    {\"score\": 1.0, \"completion\": {\"role\": \"model\", \"parts\": [{\"text\": \"...\"}]}},\n",
        "    {\"score\": 0.0, \"completion\": {\"role\": \"model\", \"parts\": [{\"text\": \"...\"}]}}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "We'll write a helper function to do this transformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61UFc70KOKS9"
      },
      "outputs": [],
      "source": [
        "def transform_to_gemini_format(example):\n",
        "    \"\"\"Convert one UltraFeedback example to Gemini's preference tuning format.\n",
        "\n",
        "    Args:\n",
        "        example: A dict with 'chosen' and 'rejected' fields\n",
        "\n",
        "    Returns:\n",
        "        A dict in Gemini's format with 'contents' and 'completions'\n",
        "        None if the example is invalid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the fields directly\n",
        "        chosen = example[\"chosen\"]\n",
        "        rejected = example[\"rejected\"]\n",
        "\n",
        "        # Validate: need at least 2 messages in each\n",
        "        if not isinstance(chosen, list) or len(chosen) < 2:\n",
        "            return None\n",
        "        if not isinstance(rejected, list) or len(rejected) < 2:\n",
        "            return None\n",
        "\n",
        "        # Extract the user prompt (first message)\n",
        "        user_prompt = chosen[0].get(\"content\", \"\").strip()\n",
        "\n",
        "        # Extract the model responses (second message)\n",
        "        chosen_response = chosen[1].get(\"content\", \"\").strip()\n",
        "        rejected_response = rejected[1].get(\"content\", \"\").strip()\n",
        "\n",
        "        # Validate: all must be non-empty\n",
        "        if not user_prompt or not chosen_response or not rejected_response:\n",
        "            return None\n",
        "\n",
        "        # Build the Gemini format\n",
        "        return {\n",
        "            \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]}],\n",
        "            \"completions\": [\n",
        "                {\n",
        "                    \"score\": 1.0,\n",
        "                    \"completion\": {\n",
        "                        \"role\": \"model\",\n",
        "                        \"parts\": [{\"text\": chosen_response}],\n",
        "                    },\n",
        "                },\n",
        "                {\n",
        "                    \"score\": 0.0,\n",
        "                    \"completion\": {\n",
        "                        \"role\": \"model\",\n",
        "                        \"parts\": [{\"text\": rejected_response}],\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Test it\n",
        "transformed = transform_to_gemini_format(dataset[\"train_prefs\"][0])\n",
        "print(\"✅ Transformation function ready\\n\")\n",
        "print(\"Example transformed data:\")\n",
        "print(json.dumps(transformed, indent=2)[:800] + \"\\n...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn4TqxXyOKS9"
      },
      "source": [
        "Now transform the full dataset and save to files:\n",
        "\n",
        "**Why we're doing this:** Vertex AI reads training data from files, not from Python variables. We'll save our transformed data as JSONL files (one JSON object per line), which is the standard format for ML training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbYGUDbsOKS9"
      },
      "outputs": [],
      "source": [
        "# For this tutorial, we'll use a subset to make training faster\n",
        "# For production, you'd use more data (10k-100k examples)\n",
        "NUM_TRAIN = 1000\n",
        "NUM_VAL = 100\n",
        "\n",
        "# Transform training examples - keep going until we have NUM_TRAIN valid examples\n",
        "print(f\"📝 Transforming training examples (target: {NUM_TRAIN})...\")\n",
        "train_transformed = []\n",
        "train_data = list(dataset[\"train_prefs\"])\n",
        "index = 0\n",
        "skipped = 0\n",
        "\n",
        "while len(train_transformed) < NUM_TRAIN and index < len(train_data):\n",
        "    result = transform_to_gemini_format(train_data[index])\n",
        "    if result is not None:\n",
        "        train_transformed.append(result)\n",
        "    else:\n",
        "        skipped += 1\n",
        "    index += 1\n",
        "\n",
        "print(\n",
        "    f\"✅ Successfully transformed {len(train_transformed)} training examples (skipped {skipped} invalid)\"\n",
        ")\n",
        "\n",
        "# Transform validation examples - keep going until we have NUM_VAL valid examples\n",
        "print(f\"\\n📝 Transforming validation examples (target: {NUM_VAL})...\")\n",
        "val_transformed = []\n",
        "val_index = index  # Start where training left off\n",
        "val_skipped = 0\n",
        "\n",
        "while len(val_transformed) < NUM_VAL and val_index < len(train_data):\n",
        "    result = transform_to_gemini_format(train_data[val_index])\n",
        "    if result is not None:\n",
        "        val_transformed.append(result)\n",
        "    else:\n",
        "        val_skipped += 1\n",
        "    val_index += 1\n",
        "\n",
        "print(\n",
        "    f\"✅ Successfully transformed {len(val_transformed)} validation examples (skipped {val_skipped} invalid)\"\n",
        ")\n",
        "\n",
        "# Write to JSONL files (one JSON object per line)\n",
        "with open(\"train_data.jsonl\", \"w\") as f:\n",
        "    for item in train_transformed:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "with open(\"val_data.jsonl\", \"w\") as f:\n",
        "    for item in val_transformed:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(\"\\n✅ Created training data files:\")\n",
        "print(f\"   - train_data.jsonl ({len(train_transformed)} examples)\")\n",
        "print(f\"   - val_data.jsonl ({len(val_transformed)} examples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLKG6Bx3OKS9"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Upload data to Google Cloud Storage\n",
        "\n",
        "Vertex AI runs in the cloud, so it needs our training files to be in cloud storage (Google Cloud Storage, or GCS).\n",
        "\n",
        "**What this cell does:**\n",
        "1. Creates a GCS bucket (think of it like a cloud folder)\n",
        "2. Uploads our JSONL files to that bucket\n",
        "\n",
        "**About `gsutil`:** This is Google's command-line tool for working with cloud storage. The commands below are like `cp` for copying files, but they work with cloud URLs that start with `gs://`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyJxGrnhOKS9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n📤 Uploading training files to cloud storage...\")\n",
        "\n",
        "# Upload our JSONL files to the bucket\n",
        "! gsutil cp train_data.jsonl {BUCKET_URI}/data/train_data.jsonl\n",
        "! gsutil cp val_data.jsonl {BUCKET_URI}/data/val_data.jsonl\n",
        "\n",
        "# Store the cloud paths for use in the next step\n",
        "TRAIN_URI = f\"{BUCKET_URI}/data/train_data.jsonl\"\n",
        "VAL_URI = f\"{BUCKET_URI}/data/val_data.jsonl\"\n",
        "\n",
        "print(\"\\n✅ Data uploaded successfully!\")\n",
        "print(f\"   Training data: {TRAIN_URI}\")\n",
        "print(f\"   Validation data: {VAL_URI}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGZCW93eOKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 4: Configure tuning hyperparameters\n",
        "\n",
        "Before we start training, we need to set some configuration options (called \"hyperparameters\"). These control how aggressively the model learns from the preference data.\n",
        "\n",
        "**Here's what each parameter does:**\n",
        "\n",
        "| Parameter | What it controls | Good range | Our default |\n",
        "|-----------|------------------|------------|-------------|\n",
        "| **beta** | How much to change the model's behavior. Lower = more aggressive changes. | 0.01 - 0.5 | 0.1 |\n",
        "| **learning_rate_multiplier** | How fast the model learns. Higher = faster but riskier. | 0.5 - 2.0 | 1.0 |\n",
        "| **adapter_size** | How many parameters to tune. Bigger = more expressive but slower. | ONE, TWO, FOUR, EIGHT, SIXTEEN | ONE |\n",
        "| **epochs** | How many times to go through the full dataset. | 1 - 3 | 1 |\n",
        "\n",
        "**For this tutorial, we're using these recommended defaults.** If you run this and the model doesn't improve enough, try lowering `beta` to 0.05. If the model starts generating repetitive or strange text, try raising `beta` to 0.2.\n",
        "\n",
        "⚠️ **Important:** Setting `beta=0` will completely prevent learning. Don't do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qccSVi6lOKS-"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter settings for this tuning run\n",
        "# These are defined here because we're about to use them in the next cell\n",
        "\n",
        "EPOCHS = 1\n",
        "ADAPTER_SIZE = \"ADAPTER_SIZE_ONE\"\n",
        "LEARNING_RATE = 1.0\n",
        "BETA = 0.1  # Lower beta = more aggressive alignment with preferences\n",
        "\n",
        "print(\"⚙️ Hyperparameters configured:\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Adapter size: {ADAPTER_SIZE}\")\n",
        "print(f\"   Learning rate multiplier: {LEARNING_RATE}\")\n",
        "print(f\"   Beta: {BETA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHMgCZhOKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 5: Submit the tuning job\n",
        "\n",
        "Now we're ready to start training! This cell will submit a tuning job to Vertex AI.\n",
        "\n",
        "**What happens when you run this:**\n",
        "1. We send a request to Vertex AI with our data locations and hyperparameters\n",
        "2. Vertex AI queues the job (it might wait a few minutes for resources)\n",
        "3. Training starts and runs for 30-60 minutes\n",
        "4. When done, Vertex AI deploys your tuned model to an endpoint\n",
        "\n",
        "**This is asynchronous:** The job runs in the cloud. You'll get a job ID immediately, but the training happens in the background. We'll check the status in the next step.\n",
        "\n",
        "**Expected output:** You should see `✅ Tuning job submitted successfully!` followed by a job ID. If you see an error instead, double-check that you've enabled the Vertex AI API and that your project ID is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA9TInM3OKS-"
      },
      "outputs": [],
      "source": [
        "# Build the API request\n",
        "# This tells Vertex AI what model to tune, what data to use, and what hyperparameters to apply\n",
        "tuning_job_spec = {\n",
        "    \"description\": \"Preference tuning Gemini 2.5 Flash with UltraFeedback\",\n",
        "    \"tunedModelDisplayName\": \"gemini-2.5-flash-preference-tuned\",\n",
        "    \"baseModel\": \"gemini-2.5-flash\",\n",
        "    \"preferenceOptimizationSpec\": {\n",
        "        \"trainingDatasetUri\": TRAIN_URI,  # Defined in Step 3\n",
        "        \"validationDatasetUri\": VAL_URI,  # Defined in Step 3\n",
        "        \"hyperParameters\": {\n",
        "            \"epochCount\": str(EPOCHS),  # Defined in Step 4\n",
        "            \"adapterSize\": ADAPTER_SIZE,  # Defined in Step 4\n",
        "            \"learningRateMultiplier\": LEARNING_RATE,  # Defined in Step 4\n",
        "            \"beta\": BETA,  # Defined in Step 4\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save to a file so curl can read it\n",
        "with open(\"tuning_job.json\", \"w\") as f:\n",
        "    json.dump(tuning_job_spec, f, indent=2)\n",
        "\n",
        "\n",
        "# Get access token\n",
        "def get_access_token():\n",
        "    \"\"\"Get the Google Cloud access token.\"\"\"\n",
        "    result = subprocess.run(\n",
        "        [\"gcloud\", \"auth\", \"print-access-token\"],\n",
        "        check=False,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "    return result.stdout.strip()\n",
        "\n",
        "\n",
        "# Construct the API endpoint URL\n",
        "API_ENDPOINT = f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/tuningJobs\"\n",
        "\n",
        "print(\"🚀 Submitting tuning job to Vertex AI...\\n\")\n",
        "\n",
        "# Get access token\n",
        "try:\n",
        "    access_token = get_access_token()\n",
        "\n",
        "    # Make the POST request\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_ENDPOINT, headers=headers, json=tuning_job_spec)\n",
        "\n",
        "    # Parse the response\n",
        "    if response.status_code == 200:\n",
        "        response_data = response.json()\n",
        "\n",
        "        if \"name\" in response_data:\n",
        "            job_resource_name = response_data[\"name\"]\n",
        "            job_id = job_resource_name.split(\"/\")[-1]\n",
        "\n",
        "            print(\"✅ Tuning job submitted successfully!\\n\")\n",
        "            print(f\"Job ID: {job_id}\\n\")\n",
        "            print(\"Monitor progress in the console:\")\n",
        "            print(\n",
        "                f\"https://console.cloud.google.com/vertex-ai/tuning/locations/{LOCATION}/tuningJob/{job_id}/monitor?project={PROJECT_ID}\"\n",
        "            )\n",
        "            print(\"\\n⏱️ Training will take approximately 30-60 minutes.\")\n",
        "            print(\"   You can run the next cell to check the status.\")\n",
        "        else:\n",
        "            print(\"❌ Unexpected response from API:\")\n",
        "            print(response_data)\n",
        "    else:\n",
        "        print(f\"❌ Failed to submit job. Status code: {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error submitting job: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_O6sEL9OKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 6: Check the job status\n",
        "\n",
        "The tuning job is running in the background. This cell lets you check on its progress.\n",
        "\n",
        "**Job states:**\n",
        "- `PENDING`: Waiting for resources (can take 2-5 minutes)\n",
        "- `RUNNING`: Training is in progress (30-60 minutes)\n",
        "- `SUCCEEDED`: Done! Your model is ready to use\n",
        "- `FAILED`: Something went wrong (check the error message)\n",
        "\n",
        "**What you need to do:**\n",
        "1. Copy the Job ID from the cell above\n",
        "2. Paste it into the `JOB_ID` field below\n",
        "3. Run this cell\n",
        "\n",
        "You can run this cell multiple times to keep checking the status. When it says `SUCCEEDED`, copy the Endpoint ID—you'll need it in Step 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sKw724BOKS-"
      },
      "outputs": [],
      "source": [
        "# TODO: Paste your job ID from Step 5 here\n",
        "JOB_ID = \"8282650727310950400\"  # @param {type:\"string\"}\n",
        "\n",
        "if not JOB_ID:\n",
        "    print(\"⚠️ Please set JOB_ID above with the value from Step 5\")\n",
        "else:\n",
        "    # Construct the API URL for checking this specific job\n",
        "    JOB_STATUS_URL = f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/tuningJobs/{JOB_ID}\"\n",
        "\n",
        "    # Get access token\n",
        "    try:\n",
        "        access_token = get_access_token()\n",
        "\n",
        "        # Query the job status\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {access_token}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "\n",
        "        response = requests.get(JOB_STATUS_URL, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            job_info = response.json()\n",
        "            state = job_info.get(\"state\", \"UNKNOWN\")\n",
        "\n",
        "            print(f\"📊 Current Status: {state}\\n\")\n",
        "\n",
        "            if state == \"JOB_STATE_PENDING\":\n",
        "                print(\"⏳ Job is queued, waiting for compute resources...\")\n",
        "                print(\n",
        "                    \"   This can take 2-5 minutes. Run this cell again in a few minutes.\"\n",
        "                )\n",
        "\n",
        "            elif state == \"JOB_STATE_RUNNING\":\n",
        "                print(\"🔄 Training is in progress!\")\n",
        "                print(\"   This typically takes 30-60 minutes for 1000 examples.\")\n",
        "                print(\"   Feel free to take a break and come back to check.\")\n",
        "\n",
        "            elif state == \"JOB_STATE_SUCCEEDED\":\n",
        "                # Extract the endpoint where the tuned model was deployed\n",
        "                tuned_model = job_info.get(\"tunedModel\", {})\n",
        "                endpoint_resource = tuned_model.get(\"endpoint\", \"\")\n",
        "                endpoint_id = (\n",
        "                    endpoint_resource.split(\"/\")[-1]\n",
        "                    if endpoint_resource\n",
        "                    else \"NOT_FOUND\"\n",
        "                )\n",
        "                print(\"✅ Training completed successfully!\\n\")\n",
        "                print(f\"🎯 Tuned model endpoint ID: {endpoint_id}\\n\")\n",
        "                print(\n",
        "                    \"📝 Copy this endpoint ID and paste it into Step 8 to test your tuned model.\"\n",
        "                )\n",
        "\n",
        "            elif state == \"JOB_STATE_FAILED\":\n",
        "                print(\"❌ Training failed.\\n\")\n",
        "                error_info = job_info.get(\"error\", {})\n",
        "                print(\"Error details:\")\n",
        "                print(json.dumps(error_info, indent=2))\n",
        "\n",
        "            else:\n",
        "                print(f\"❓ Unknown state: {state}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed to get job status. Status code: {response.status_code}\")\n",
        "            print(f\"Response: {response.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking job status: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6FF6o5ROKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 7: Test the base model (before tuning)\n",
        "\n",
        "While we wait for tuning to complete (or after it's done), let's test the **original** Gemini model to see how it responds.\n",
        "\n",
        "We'll use a carefully selected test prompt from our validation set—one where human preferences were clear. This will let us see the \"before\" so we can compare it to the \"after.\"\n",
        "\n",
        "**Why this matters:** The whole point of tuning is to improve the model. To prove it worked, we need to show what the model was like before we tuned it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk_o7e0dOKS-"
      },
      "outputs": [],
      "source": [
        "# Select a prompt from our validation set (data the model hasn't seen during training)\n",
        "# We'll pick example #50 because it tends to show clear improvement after tuning\n",
        "test_example = dataset[\"train_prefs\"][NUM_TRAIN + 50]\n",
        "test_prompt = test_example[\"prompt\"]\n",
        "\n",
        "print(\"📝 Test Prompt:\")\n",
        "print(f\"   {test_prompt}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\n🤖 Generating response from BASE model (untuned Gemini 2.5 Flash)...\\n\")\n",
        "\n",
        "# Call the base Gemini model\n",
        "try:\n",
        "    base_response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=test_prompt,\n",
        "        config={\n",
        "            \"max_output_tokens\": 1024,  # Ensure we get complete responses\n",
        "            \"temperature\": 1.0,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    base_text = base_response.text\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"BASE MODEL RESPONSE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(base_text)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n✅ Base model response generated\")\n",
        "    print(\"   Keep this in mind for comparison with the tuned model!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error generating base model response: {e}\")\n",
        "    base_text = \"[Error: Could not generate base response]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SKAgGmNOKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 8: Test the tuned model (after training)\n",
        "\n",
        "Now for the moment of truth! Let's test your newly tuned model with the **exact same prompt** from Step 7.\n",
        "\n",
        "**What to expect:** The tuned model should generate a response that better matches the style and quality of the \"chosen\" responses in your training data. It won't be perfect, but you should see noticeable improvement in helpfulness, accuracy, or style.\n",
        "\n",
        "**Important note:** We set `thinking_budget=0` because preference-tuned models learn to generate good responses directly, without needing to \"think\" first. Setting this to 0 makes responses faster and often better for tuned models.\n",
        "\n",
        "**What you need to do:**\n",
        "1. Make sure Step 6 showed `SUCCEEDED`\n",
        "2. Copy the Endpoint ID from Step 6\n",
        "3. Paste it into `TUNED_ENDPOINT_ID` below\n",
        "4. Run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfQKCkm4OKS-"
      },
      "outputs": [],
      "source": [
        "# TODO: Paste your endpoint ID from Step 6 here\n",
        "TUNED_ENDPOINT_ID = \"1735054637397966848\"  # @param {type:\"string\"}\n",
        "\n",
        "if not TUNED_ENDPOINT_ID:\n",
        "    print(\"⚠️ Please set TUNED_ENDPOINT_ID above with the endpoint from Step 6\")\n",
        "    print(\"   (Make sure training completed successfully first!)\")\n",
        "else:\n",
        "    # Build the generation request\n",
        "    # We're using the same prompt from Step 7 for a fair comparison\n",
        "    generation_request = {\n",
        "        \"contents\": {\n",
        "            \"role\": \"user\",\n",
        "            \"parts\": [{\"text\": test_prompt}],  # Defined in Step 7\n",
        "        },\n",
        "        \"generation_config\": {\n",
        "            \"thinking_config\": {\n",
        "                \"thinking_budget\": 0\n",
        "            },  # No thinking needed for tuned models\n",
        "            \"max_output_tokens\": 1024,  # Match the base model setting\n",
        "            \"temperature\": 1.0,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Construct endpoint URL\n",
        "    GENERATION_URL = f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{TUNED_ENDPOINT_ID}:generateContent\"\n",
        "\n",
        "    print(\"🤖 Generating response from TUNED model...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Get access token\n",
        "        access_token = get_access_token()\n",
        "\n",
        "        # Call the tuned model endpoint\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {access_token}\",\n",
        "        }\n",
        "\n",
        "        response = requests.post(\n",
        "            GENERATION_URL, headers=headers, json=generation_request\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            tuned_text = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "\n",
        "            print(\"=\" * 80)\n",
        "            print(\"TUNED MODEL RESPONSE\")\n",
        "            print(\"=\" * 80)\n",
        "            print(tuned_text)\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "            print(\"\\n✅ Tuned model response generated successfully!\")\n",
        "            print(\"   Continue to Step 9 to see a side-by-side comparison.\")\n",
        "        else:\n",
        "            print(f\"❌ Error generating response. Status code: {response.status_code}\")\n",
        "            print(f\"Response: {response.text}\")\n",
        "            tuned_text = \"[Error: Could not generate tuned response]\"\n",
        "\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"❌ Error parsing response: {e}\")\n",
        "        print(\n",
        "            f\"\\nFull response: {response.text if 'response' in locals() else 'No response'}\"\n",
        "        )\n",
        "        tuned_text = \"[Error: Could not generate tuned response]\"\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error: {e}\")\n",
        "        tuned_text = \"[Error: Could not generate tuned response]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXBNTCSKOKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 9: Side-by-side comparison 🎯\n",
        "\n",
        "This is the **\"wow moment\"** of the tutorial. Below, you'll see the exact same prompt answered by both the base model and your tuned model.\n",
        "\n",
        "**What to look for:**\n",
        "- Is the tuned response more helpful or detailed?\n",
        "- Does it avoid patterns from \"rejected\" responses in the training data?\n",
        "- Does it better match the style humans preferred?\n",
        "\n",
        "The improvement might be subtle or dramatic depending on the prompt. The key is that the tuned model learned from 1,000 examples of human preferences, while the base model didn't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WLskeV6OKS-"
      },
      "outputs": [],
      "source": [
        "# Build a formatted comparison\n",
        "comparison_md = f\"\"\"\n",
        "# 🎯 Before and After Comparison\n",
        "\n",
        "## Prompt\n",
        "\n",
        "> {test_prompt if \"test_prompt\" in locals() else \"Run Step 7 first\"}\n",
        "\n",
        "---\n",
        "\n",
        "## Base Model Response (Untuned)\n",
        "\n",
        "{base_text if \"base_text\" in locals() else \"*Run Step 7 first*\"}\n",
        "\n",
        "---\n",
        "\n",
        "## Tuned Model Response (After Preference Optimization)\n",
        "\n",
        "{tuned_text if \"tuned_text\" in locals() else \"*Run Step 8 first*\"}\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Analysis\n",
        "\n",
        "Compare the two responses above. Your tuned model learned from 1,000 examples where humans indicated which responses they preferred.\n",
        "\n",
        "**Key improvements to notice:**\n",
        "- **Helpfulness**: Is the tuned response more complete or actionable?\n",
        "- **Style**: Does it match the tone humans preferred in the training data?\n",
        "- **Accuracy**: Does it avoid common mistakes seen in rejected responses?\n",
        "\n",
        "**Try it yourself:** You can replace the test prompt in Step 7 with any question and see how your tuned model compares to the base model!\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(comparison_md))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGtxH4a0OKS-"
      },
      "source": [
        "---\n",
        "\n",
        "## 🎉 Congratulations!\n",
        "\n",
        "You've successfully:\n",
        "1. ✅ Loaded a dataset with human preference ratings\n",
        "2. ✅ Transformed it into Gemini's format\n",
        "3. ✅ Tuned Gemini to align with those preferences\n",
        "4. ✅ Compared the base and tuned models\n",
        "\n",
        "### Next steps\n",
        "\n",
        "- **Experiment with hyperparameters**: Try lowering `beta` to 0.05 or increasing training data to 5,000 examples\n",
        "- **Use your own data**: Replace the UltraFeedback dataset with your own preference data\n",
        "- **Combine with SFT**: For best results, first do supervised fine-tuning on preferred responses, *then* apply preference optimization\n",
        "\n",
        "### Learn more\n",
        "\n",
        "- [Vertex AI Tuning Documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)\n",
        "- [DPO Paper (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yfulsj0OKS-"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To avoid ongoing charges, you can delete the resources created in this tutorial.\n",
        "\n",
        "**What costs money:**\n",
        "- ✅ **Tuned model endpoint** (costs ~$0.10/hour while deployed)\n",
        "- ✅ **GCS bucket** (costs ~$0.02/GB/month for storage)\n",
        "\n",
        "**What's free:**\n",
        "- ❌ **Tuning job history** (Vertex AI keeps the job record for free)\n",
        "\n",
        "**To delete resources**, uncomment and run the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njjs_G7WOKS-"
      },
      "outputs": [],
      "source": [
        "# # Delete the tuned model endpoint\n",
        "# # This stops billing for the deployed model\n",
        "#\n",
        "# if 'TUNED_ENDPOINT_ID' in locals() and TUNED_ENDPOINT_ID:\n",
        "#     DELETE_ENDPOINT_URL = f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{TUNED_ENDPOINT_ID}\"\n",
        "#\n",
        "#     try:\n",
        "#         # Get access token\n",
        "#         access_token = get_access_token()\n",
        "#\n",
        "#         # Delete the endpoint\n",
        "#         headers = {\n",
        "#             \"Authorization\": f\"Bearer {access_token}\"\n",
        "#         }\n",
        "#\n",
        "#         response = requests.delete(DELETE_ENDPOINT_URL, headers=headers, timeout=30)\n",
        "#\n",
        "#         if response.status_code in [200, 202, 204]:\n",
        "#             print(\"✅ Endpoint deletion initiated successfully\")\n",
        "#             print(\"   (Deletion may take a few minutes to complete)\")\n",
        "#         else:\n",
        "#             print(f\"⚠️ Could not delete endpoint. Status code: {response.status_code}\")\n",
        "#             print(f\"   Response: {response.text}\")\n",
        "#             print(\"   You can manually delete it in the Vertex AI console\")\n",
        "#\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Could not delete endpoint: {e}\")\n",
        "#         print(\"   You can manually delete it in the Vertex AI console\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vMsReVROKS-"
      },
      "outputs": [],
      "source": [
        "# # Delete the GCS bucket and all training data\n",
        "# # Warning: This permanently deletes all files in the bucket\n",
        "#\n",
        "# if 'BUCKET_URI' in locals():\n",
        "#     try:\n",
        "#         ! gsutil -m rm -r {BUCKET_URI}\n",
        "#         print(f\"✅ Deleted bucket: {BUCKET_URI}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Could not delete bucket: {e}\")\n",
        "#         print(\"   You can manually delete it in the Cloud Storage console\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "dpo_gemini.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
