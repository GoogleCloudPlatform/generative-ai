{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tXEYL-zSWru"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDcI0SYySZiu"
      },
      "source": [
        "# Supervised Fine Tuning with Gemini for Question & Answering\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_tuning_qa.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fgemini_supervised_tuning_qa.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/gemini_supervised_tuning_qa.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_tuning_qa.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97b1d7579d2b"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Erwin Huizenga](https://github.com/erwinh85) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5LXfKSgShfJ"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates fine-tuning the Gemini generative model using the Vertex AI Supervised Tuning feature. Supervised Tuning allows you to use your training data to refine the base model's capabilities toward specific tasks.\n",
        "\n",
        "Supervised Tuning uses labeled examples to tune a model. Each example demonstrates the output you want from your text model during inference.\n",
        "\n",
        "- Data Preparation: Your role is crucial in ensuring your training data is high-quality, well-labeled, and directly relevant to the target task. The quality of the data can significantly impact the model's performance and the presence of bias in the fine-tuned model, underscoring the importance of your contribution.\n",
        "- Training: This phase presents an exciting opportunity to experiment with different configurations, allowing you to optimize the model's performance on the target task. Your creativity and innovation can lead to significant improvements in the model's capabilities.\n",
        "- Evaluation:\n",
        "  - Metric: Choose appropriate evaluation metrics that accurately reflect the success of the fine-tuned model for your specific task\n",
        "  - Evaluation Set: Use a separate set of data to evaluate the model's performance\n",
        "\n",
        "### Recommended configurations\n",
        "The following table shows the recommended configurations for tuning a foundation model by task:\n",
        "\n",
        "| Task           | No. of examples in dataset | Number of epochs |\n",
        "| -------------- | -------------------------- | ----------- |\n",
        "| Classification | 500+                       | 2-4         |\n",
        "| Summarization  | 1000+                      | 2-4         |\n",
        "| Extractive QA  | 500+                       | 2-4         |\n",
        "| Chat           | 1000+                      | 2-4         |\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "\n",
        "- A Google Cloud project: Provide your project ID in the `PROJECT_ID` variable.\n",
        "\n",
        "- Authenticated your Colab environment: Run the authentication code block at the beginning.\n",
        "\n",
        "- Prepared training data: Data should be formatted in JSON Lines with prompts and corresponding completions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D59iF36T62k"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fEBa5FbT-dc"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0M04I5j3_KY5"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLXs74x8UD6A"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8nnS20eqUJSK"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoh03Du8UNAn"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è The kernel is going to restart. Please wait until it is finished before continuing to the next step. ‚ö†Ô∏è</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID0JD1lUUPmx"
      },
      "source": [
        "## Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t49529b_UZJ0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKcjFI9iUa2T"
      },
      "source": [
        "- If you are running this notebook in a local development environment:\n",
        "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
        "\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKgURhIUnAM"
      },
      "source": [
        "## Set Project and Location\n",
        "\n",
        "First, you will have to set your project_id, location, and bucket_name. You can also use an existing bucket within the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4acO9tVcU1Ey"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "BUCKET_NAME = \"[your-bucket]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "udWd9pp7YTSb"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"<your-bucket-name>\":\n",
        "    BUCKET_NAME = \"vertex-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVe_7ETAasWS"
      },
      "source": [
        "**warning**: Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6L_c9lwatsw"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJllsmMiavrH"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Mrn9G6ayl3"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkBZ-e85UeiI"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jsnIinC4UfZq"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.preview.tuning import sft\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "import datetime\n",
        "from typing import Union\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbaPSIO4_iur"
      },
      "source": [
        "## Supervised fine tuning with Gemini on a question and answer dataset\n",
        "\n",
        "Now it's time for you to create a tuning job. You will be using a Q&A with a context dataset in JSON format.\n",
        "\n",
        "Supervised fine-tuning offers a solution, allowing focused adaptation of foundation models to new tasks. You can create a supervised text model tuning job using the Google Cloud console, API, or the Vertex AI SDK for Python.¬†You can read more on our [documentation page](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning),\n",
        "\n",
        "But how do you ensure your data is primed for success with supervised fine-tuning? Here's a breakdown of critical areas to focus on:\n",
        "\n",
        "- **Domain Alignment:** Supervised fine-tuning thrives on smaller datasets, but they must be highly relevant to your downstream task. Seek out data that closely mirrors the domain you will encounter in real-world use cases.\n",
        "- **Labeling Accuracy:** Noisy labels will sabotage even the best technique. Prioritize accuracy in your annotations and labeling.\n",
        "- **Noise Reduction:** Outliers, inconsistencies, or irrelevant examples hurt model adaptation. Implement preprocessing, such as removing duplicates, fixing typos, and verifying that data conforms to your task's expectations.\n",
        "- **Distribution:** A diverse range of examples will help your model generalize better within the confines of your target task. Refrain from overloading the process with excessive variance that strays from your core domain.\n",
        "- **Balanced Classes:** For classification tasks, try to keep a reasonable balance between different classes to avoid the model learning biases towards a specific class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivFjWO5M-Z8H"
      },
      "source": [
        "### Fetching data from BigQuery\n",
        "üíæ Your model tuning dataset must be in a JSONL format where each line contains a single training example. You must make sure that you include instructions.\n",
        "\n",
        "You will use the [StackOverflow dataset](https://cloud.google.com/blog/topics/public-datasets/google-bigquery-public-datasets-now-include-stack-overflow-q-a) on BigQuery Public Datasets, limiting to questions with the `python` tag, and accepted answers for answers since 2020-01-01.\n",
        "\n",
        "You will use a helper function to read the data from BigQuery and create a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2JIlL-aVbNPg"
      },
      "outputs": [],
      "source": [
        "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
        "    Args:\n",
        "        sql: SQL query, as a string, to execute in BigQuery\n",
        "    Returns:\n",
        "        df: DataFrame of results from query,  or error, if any\n",
        "    \"\"\"\n",
        "\n",
        "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return DataFrame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11WLzqp-b59c"
      },
      "source": [
        "Next you will write the query. For now you will limit our example to 550."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLC_elwzb3ZF"
      },
      "outputs": [],
      "source": [
        "stack_overflow_df = run_bq_query(\n",
        "    \"\"\"SELECT\n",
        "           CONCAT(q.title, q.body) AS input_text,\n",
        "           a.body AS output_text\n",
        "       FROM `bigquery-public-data.stackoverflow.posts_questions` q\n",
        "       JOIN `bigquery-public-data.stackoverflow.posts_answers` a\n",
        "         ON q.accepted_answer_id = a.id\n",
        "       WHERE q.accepted_answer_id IS NOT NULL\n",
        "         AND REGEXP_CONTAINS(q.tags, \"python\")\n",
        "         AND a.creation_date >= \"2020-01-01\"\n",
        "       LIMIT 550\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "stack_overflow_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b404hW8jcRDQ"
      },
      "source": [
        "There should be 550 questions and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUg-lF61cUVI"
      },
      "outputs": [],
      "source": [
        "print(len(stack_overflow_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHda8BzbmRMC"
      },
      "source": [
        "#### Adding instructions\n",
        "Finetuning language models on a collection of datasets phrased as instructions have been shown to improve model performance and generalization to unseen tasks [(Google, 2022)](https://arxiv.org/pdf/2210.11416.pdf).\n",
        "\n",
        "An instruction refers to a specific directive or guideline that conveys a task or action to be executed. These instructions can be expressed in various forms, such as step-by-step procedures, commands, or rules. When we don't use the instructions, it's only a question and answer. The instruction tells the large language model what to do. We want them to answer the question. We have to give a hint about the task we want to perform. Let's extend the dataset with an instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XIy7BjKWmu5j"
      },
      "outputs": [],
      "source": [
        "INSTRUCTION_TEMPLATE = \"\"\"\\\n",
        "You are a helpful Python developer \\\n",
        "You are good at answering StackOverflow questions \\\n",
        "Your mission is to provide developers with helpful answers that work\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tM_f1b3n4TK"
      },
      "source": [
        "You will create a new column for the `INSTRUCTION_TEMPLATE`. Use a new column and do not overwrite the existing one, which you might want to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJpAJG8uoE7F"
      },
      "outputs": [],
      "source": [
        "stack_overflow_df[\"input_text_instruct\"] = INSTRUCTION_TEMPLATE\n",
        "\n",
        "stack_overflow_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNMMxaB2cZvY"
      },
      "source": [
        "Next, you will randomly split the data into training and evaluation. For Extractive Q&A tasks, we advise 500+ training examples. In this case, you will use 440 to generate a tuning job that runs faster. \n",
        "\n",
        "20% of your dataset will be used for test. The `random_state` controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. Feel free to adjust this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdrweRsscfgU"
      },
      "outputs": [],
      "source": [
        "# split is set to 80/20\n",
        "train, evaluation = train_test_split(stack_overflow_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(train))\n",
        "print(len(evaluation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_MuRRGmfLni"
      },
      "source": [
        "#### Generating the JSONL files\n",
        "\n",
        "Prepare your training data in a JSONL (JSON Lines) file and store it in a Google Cloud Storage (GCS) bucket. This format ensures efficient processing. Each line of the JSONL file must represent a single data instance and follow a well-defined schema:\n",
        "\n",
        "`{\"messages\": [{\"role\": \"system\", \"content\": \"instructions\"}, {\"role\": \"user\", \"content\": \"question\"}, {\"role\": \"model\", \"content\": \"answering\"}]}`\n",
        "\n",
        "This is how it maps to the Pandas df columns:\n",
        "\n",
        "*   `instructions -> input_text_instruct`\n",
        "*   `question -> input_text`\n",
        "*   `answer -> output_text`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fgPXoXOlc0vI"
      },
      "outputs": [],
      "source": [
        "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
        "\n",
        "tuning_data_filename = f\"tune_data_stack_overflow_qa-{date}.jsonl\"\n",
        "validation_data_filename = f\"validation_data_stack_overflow_qa-{date}.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9-oHmx0wfElN"
      },
      "outputs": [],
      "source": [
        "def format_messages(row):\n",
        "    \"\"\"Formats a single row into the desired JSONL structure\"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": row[\"input_text_instruct\"]},\n",
        "            {\"role\": \"user\", \"content\": row[\"input_text\"]},\n",
        "            {\"role\": \"model\", \"content\": row[\"output_text\"]},\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8mBwn2jJEkYl"
      },
      "outputs": [],
      "source": [
        "# Apply formatting function to each row, then convert to JSON Lines format\n",
        "tuning_data = train.apply(format_messages, axis=1).to_json(orient=\"records\", lines=True)\n",
        "\n",
        "# Save the result to a JSONL file\n",
        "with open(tuning_data_filename, \"w\") as f:\n",
        "    f.write(tuning_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz9IbouGftaZ"
      },
      "source": [
        "Next you can check if the number of rows match with your Pandas df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4JfgAijikHp"
      },
      "outputs": [],
      "source": [
        "with open(tuning_data_filename, \"r\") as f:\n",
        "    num_rows = sum(1 for _ in f)\n",
        "\n",
        "print(\"Number of rows in the JSONL file:\", num_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42u53mHQVZk3"
      },
      "source": [
        "You will do the same for the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nBc6ufE0h2zL"
      },
      "outputs": [],
      "source": [
        "# Apply formatting function to each row, then convert to JSON Lines format\n",
        "validation_data = evaluation.apply(format_messages, axis=1).to_json(\n",
        "    orient=\"records\", lines=True\n",
        ")\n",
        "\n",
        "# Save the result to a JSONL file\n",
        "with open(validation_data_filename, \"w\") as f:\n",
        "    f.write(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYGr7h_2ahqb"
      },
      "source": [
        "Next, you will copy the JSONL files into the Google Cloud Storage bucket you specified or created at the beginning of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq0MYC6nxhKy"
      },
      "outputs": [],
      "source": [
        "!gsutil cp $tuning_data_filename $validation_data_filename $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBq0NMIxa2iD"
      },
      "source": [
        "Next you can check if the files are in the bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVel0g6pkOiA"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsc0xhNGa7ZQ"
      },
      "source": [
        "Now, you will create two variables for the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tXzEZFjtkTWJ"
      },
      "outputs": [],
      "source": [
        "TUNING_DATA_URI = f\"{BUCKET_URI}/{tuning_data_filename}\"\n",
        "VALIDATION_DATA_URI = f\"{BUCKET_URI}/{validation_data_filename}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAOu-xJnA54y"
      },
      "source": [
        "### Create a supervised tuning job using Gemini\n",
        "Now it's time for you to start your tuning job. You will use the `gemini-1.0-pro-002` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SodJv2vWicfu"
      },
      "outputs": [],
      "source": [
        "foundation_model = GenerativeModel(\"gemini-1.0-pro-002\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e7zBH5foZbC"
      },
      "outputs": [],
      "source": [
        "# Tune a model using `train` method.\n",
        "sft_tuning_job = sft.train(\n",
        "    source_model=foundation_model,\n",
        "    train_dataset=TUNING_DATA_URI,\n",
        "    # Optional:\n",
        "    validation_dataset=VALIDATION_DATA_URI,\n",
        "    epochs=3,\n",
        "    learning_rate_multiplier=1.0,\n",
        ")\n",
        "\n",
        "# Get the tuning job info.\n",
        "sft_tuning_job.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LSm5Ns5gjx-"
      },
      "source": [
        "Lets monitor the state. Wait for the next step to complete. Tuning a model will take some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgukIEFPlVdD"
      },
      "source": [
        "Next you can retrieve the model resource name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3yiKi-KofGK"
      },
      "outputs": [],
      "source": [
        "# Get the resource name of the tuning job\n",
        "sft_tuning_job_name = sft_tuning_job.resource_name\n",
        "sft_tuning_job_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1RZVqWKRdg"
      },
      "source": [
        "Tuning takes time. Please wait until the job is finished before you continue after the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyug1dw4FAgn"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Wait for job completion\n",
        "while not sft_tuning_job.refresh().has_ended:\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyDS9G2TTX9p"
      },
      "outputs": [],
      "source": [
        "# tuned model name\n",
        "tuned_model_name = sft_tuning_job.tuned_model_name\n",
        "tuned_model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU2AVJdLliUh"
      },
      "source": [
        "And the model endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s57xpI5o9m0"
      },
      "source": [
        "You can use `tuning.TuningJob.list()` to retrieve your tuning jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QtT3uJ3Jw0N"
      },
      "outputs": [],
      "source": [
        "sft_tuning_job.list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KQmyjjcJ9uz"
      },
      "source": [
        "You model is automatically deployed as a Vertex AI Endpoint and ready for usage!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9uQD-Ee_h6h"
      },
      "outputs": [],
      "source": [
        "# tuned model endpoint name\n",
        "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
        "tuned_model_endpoint_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRPlb4ZO8ulD"
      },
      "source": [
        "# Load tuned Generative Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhfU4wTOtH1y"
      },
      "outputs": [],
      "source": [
        "tuned_model = GenerativeModel(tuned_model_endpoint_name)\n",
        "print(tuned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b1b39693d75"
      },
      "source": [
        "Call the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1q1PT2zJRO9"
      },
      "outputs": [],
      "source": [
        "tuned_model.generate_content(\n",
        "    \"How do I store a TensorFlow checkpoint on Google Cloud Storage while training?\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gemini_supervised_tuning_qa.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
