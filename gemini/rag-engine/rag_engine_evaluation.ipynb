{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Advanced RAG Techniques - Vertex RAG Engine Retrieval Quality Evaluation and Hyperparameters Tuning\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/rag_engine_evaluation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Frag-engine%2Frag_engine_evaluation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/rag-engine/rag_engine_evaluation.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/rag_engine_evaluation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "|           |                                         |\n",
        "|-----------|---------------------------------------- |\n",
        "| Author(s) | [Ed Tsoi](https://github.com/edtsoi430) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Retrieval Quality is arguably the most important component of a Retrieval Augmented Generation (RAG) application. Not only does it directly impact the quality of the generated response, in some cases poor retrieval could also lead to irrelevant, incomplete or hallucinated output.\n",
        "\n",
        "This notebook aims to provide guidelines on:\n",
        "> **You'll learn how to:**\n",
        "> * Evaluate retrieval quality using the [BEIR-fiqa 2018 dataset](https://arxiv.org/abs/2104.08663) (or your own!).\n",
        "> * Understand the impact of key parameters on retrieval performance. (e.g. embedding model, chunk size)\n",
        "> * Tune hyperparameters to improve accuracy of the RAG system.\n",
        "\n",
        "**Note:** This notebook assumes that you already have an understanding on how to implement a RAG system with Vertex AI RAG Engine. For more general instructions on how to use Vertex AI RAG Engine, please refer to the [RAG Engine API Documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/rag-api).\n",
        "\n",
        "We'll explore how these hyperparameters influence retrieval:\n",
        "\n",
        "| Parameter                 | Description                                                                         |\n",
        "|---------------------------|-------------------------------------------------------------------------------------|\n",
        "| Chunk Size                | Size of each chunk (in tokens). Affects granularity of retrieval.                   |\n",
        "| Chunk Overlap             | Overlap between chunks. Helps capture relevant information across chunk boundaries. |\n",
        "| Top K                     | Maximum number of retrieved contexts.  Balance recall and precision.                |\n",
        "| Vector Distance threshold | Filters contexts based on similarity.  A stricter threshold prioritizes precision.  |\n",
        "| Embedding model           | Model used to convert text to embeddings. Significantly impacts retrieval accuracy. |\n",
        "\n",
        "### How exactly could we use this notebook to improve the RAG system?\n",
        "\n",
        "* **Hyperparameters Tuning:** There are a couple of hyperparameters that could impact retrieval quality:\n",
        "\n",
        "| Parameter | Description |\n",
        "|------------|----------------------|\n",
        "| Chunk Size | When documents are ingested into an index, they are split into chunks. The `chunk_size` parameter (in tokens) specifies the size of each chunk. |\n",
        "| Chunk Overlap |  By default, documents are split into chunks with a certain amount of overlap to improve relevance and retrieval quality. |\n",
        "| Top K | Controls the maximum number of contexts that are retrieved. |\n",
        "| Vector Distance threshold | Only contexts with a distance smaller than the threshold are considered. |\n",
        "| Embedding model | The embedding model used to convert input text into embeddings for retrieval.|\n",
        "\n",
        "You may use this notebook to evaluate your retrieval quality, and see how changing certain parameters (top k, chunk size) impact or improve your retrieval quality (`recall@k`, `precision@k`, `ndcg@k`).\n",
        "\n",
        "* **Response Quality Evaluation:** Once you have optimized the retrieval metrics, you can understand how it impacts response quality using the [Evaluation Service API Notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_rag_gen_ai_evaluation_service_sdk.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform beir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize the Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9caed7b620d"
      },
      "outputs": [],
      "source": [
        "!gcloud auth application-default login\n",
        "!gcloud auth application-default set-quota-project {PROJECT_ID}\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09720c707f1c"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e614bd6fe56b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "from beir import util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from vertexai.preview import rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "# 1. Option A. Create a RAG Corpus (or use an existing RAG Corpus that you would like to evaluate on).\n",
        "\n",
        "* If you would like to bring your own existing `RagCorpus` (with imported files), skip to Option B below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Create a `RagCorpus` with the specified configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "# See the list of current supported embedding models here: https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview#supported-embedding-models\n",
        "# You may adjust the embedding model here if you would like.\n",
        "embedding_model_config = rag.EmbeddingModelConfig(\n",
        "    publisher_model=\"publishers/google/models/text-embedding-004\"  # @param {type:\"string\", isTemplate: true},\n",
        ")\n",
        "\n",
        "rag_corpus = rag.create_corpus(\n",
        "    display_name=\"test-corpus\",\n",
        "    description=\"A test corpus where we import the BEIR-FiQA-2018 dataset\",\n",
        "    embedding_model_config=embedding_model_config,\n",
        ")\n",
        "\n",
        "print(rag_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Load BEIR Fiqa dataset (test split)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "# Download and load a BEIR dataset\n",
        "dataset = \"fiqa\"  # @param [\"arguana\", \"climate-fever\", \"cqadupstack\", \"dbpedia-entity\", \"fever\", \"fiqa\", \"germanquad\", \"hotpotqa\", \"mmarco\", \"mrtydi\", \"msmarco-v2\", \"msmarco\", \"nfcorpus\", \"nq-train\", \"nq\", \"quora\", \"scidocs\", \"scifact\", \"trec-covid-beir\", \"trec-covid-v2\", \"trec-covid\", \"vihealthqa\", \"webis-touche2020\"]\n",
        "url = (\n",
        "    f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        ")\n",
        "out_dir = \"datasets\"\n",
        "data_path = util.download_and_unzip(url, out_dir)\n",
        "\n",
        "# Load the dataset\n",
        "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
        "print(\n",
        "    f\"Successfully loaded the {dataset} dataset with {len(corpus)} files and {len(queries)} queries!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sHOGwdTDXZ"
      },
      "source": [
        "### Define helper function for processing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_k4EWn3TTWVh"
      },
      "outputs": [],
      "source": [
        "def convert_beir_to_rag_corpus(corpus, output_dir):\n",
        "    \"\"\"\n",
        "    Convert a BEIR corpus to Vertex RAG corpus format with a maximum of 10,000\n",
        "    files per subdirectory.\n",
        "\n",
        "    For each document in the BEIR corpus, we will create a new txt where:\n",
        "      * doc_id will be the file name\n",
        "      * doc_content will be the document text prepended by title (if any).\n",
        "\n",
        "    Args:\n",
        "      corpus: BEIR corpus\n",
        "      output_dir (str): Directory where the converted corpus will be saved\n",
        "\n",
        "    Returns:\n",
        "      None (will write output to disk)\n",
        "    \"\"\"\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    file_count, subdir_count = 0, 0\n",
        "    current_subdir = os.path.join(output_dir, f\"{subdir_count}\")\n",
        "    os.makedirs(current_subdir, exist_ok=True)\n",
        "\n",
        "    # Convert each file in the corpus\n",
        "    for doc_id, doc_content in corpus.items():\n",
        "        # Combine title and text (if title exists)\n",
        "        full_text = doc_content.get(\"title\", \"\")\n",
        "        if full_text:\n",
        "            full_text += \"\\n\\n\"\n",
        "        full_text += doc_content[\"text\"]\n",
        "\n",
        "        # Create a new file for each file.\n",
        "        file_path = os.path.join(current_subdir, f\"{doc_id}.txt\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(full_text)\n",
        "\n",
        "        file_count += 1\n",
        "\n",
        "        # Create a new subdirectory if the current one has reached the limit\n",
        "        if file_count >= 10000:\n",
        "            subdir_count += 1\n",
        "            current_subdir = os.path.join(output_dir, f\"{subdir_count}\")\n",
        "            os.makedirs(current_subdir, exist_ok=True)\n",
        "            file_count = 0\n",
        "\n",
        "    print(f\"Conversion complete. {len(corpus)} files saved in {output_dir}\")\n",
        "\n",
        "\n",
        "def count_files_in_gcs_bucket(bucket_path):\n",
        "    \"\"\"\n",
        "    Counts the number of files in a Google Cloud Storage bucket path,\n",
        "    excluding directories.\n",
        "\n",
        "    Args:\n",
        "      bucket_path: The path to the directory in the GCS bucket.\n",
        "\n",
        "    Returns:\n",
        "      The number of files in the bucket path.\n",
        "    \"\"\"\n",
        "    # Add a trailing slash if it's not present\n",
        "    if not bucket_path.endswith(\"/\"):\n",
        "        bucket_path += \"/\"\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        f\"gsutil ls {bucket_path}** | grep -v '/$' | wc -l\",\n",
        "        shell=True,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "    )\n",
        "    stdout, stderr = process.communicate()\n",
        "    if stderr:\n",
        "        raise RuntimeError(f\"Error counting files in GCS: {stderr.decode()}\")\n",
        "    return int(stdout.decode().strip())\n",
        "\n",
        "\n",
        "def count_directories_after_split(gcs_bucket_path):\n",
        "    \"\"\"\n",
        "    Counts the number of directories in a Google Cloud Storage bucket path.\n",
        "\n",
        "    Args:\n",
        "      gcs_bucket_path: The path to the directory in the GCS bucket.\n",
        "\n",
        "    Returns:\n",
        "      The number of directories in the bucket path.\n",
        "    \"\"\"\n",
        "    num_files_in_bucket = count_files_in_gcs_bucket(gcs_bucket_path)\n",
        "    num_directories = math.ceil(num_files_in_bucket / 10000)\n",
        "    return num_directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me5Ee8yCTDpo"
      },
      "source": [
        "### Convert BEIR corpus to `RagCorpus` format and upload to GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0j_ceMjXddK"
      },
      "outputs": [],
      "source": [
        "CONVERTED_DATASET_PATH = f\"/converted_dataset_{dataset}\"\n",
        "# Convert BEIR corpus to RAG format.\n",
        "convert_beir_to_rag_corpus(corpus, CONVERTED_DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQd2c173rPeP"
      },
      "source": [
        "#### Create a test bucket for uploading BEIR evaluation dataset to (or use an existing bucket of your choice)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mG26u-QrOO2"
      },
      "outputs": [],
      "source": [
        "!gsutil mb gs://beir-test-bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIKdALWnp-s5"
      },
      "source": [
        "#### Upload to specified GCS bucket (Modify the GCS bucket path to desired location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk3AGhX1bJas"
      },
      "outputs": [],
      "source": [
        "GCS_BUCKET_PATH = \"gs://beir-test-bucket/beir-fiqa\"  # @param {type: \"string\"}\n",
        "\n",
        "!echo \"Uploading files from ${CONVERTED_DATASET_PATH} to ${GCS_BUCKET_PATH}\"\n",
        "# Upload RAG format dataset to GCS bucket.\n",
        "!gsutil -m rsync -r -d $CONVERTED_DATASET_PATH $GCS_BUCKET_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTCo-81oTDzx"
      },
      "source": [
        "### Import evaluation dataset files into `RagCorpus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v0f2i3PBwQcP"
      },
      "outputs": [],
      "source": [
        "num_subdirectories = count_directories_after_split(GCS_BUCKET_PATH)\n",
        "paths = [GCS_BUCKET_PATH + f\"/{i}/\" for i in range(num_subdirectories)]\n",
        "\n",
        "chunk_size = 512  # @param {type:\"integer\"}\n",
        "chunk_overlap = 102  # @param {type:\"integer\"}\n",
        "total_imported, total_num_of_files = 0, 0\n",
        "\n",
        "for path in paths:\n",
        "    num_files_to_be_imported = count_files_in_gcs_bucket(path)\n",
        "    total_num_of_files += num_files_to_be_imported\n",
        "    max_retries, attempt, imported = 10, 0, 0\n",
        "    while attempt < max_retries and imported < num_files_to_be_imported:\n",
        "        response = rag.import_files(\n",
        "            rag_corpus.name,\n",
        "            [path],\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            timeout=20000,\n",
        "            max_embedding_requests_per_min=1400,\n",
        "        )\n",
        "        imported += (\n",
        "            response.imported_rag_files_count\n",
        "            if response.imported_rag_files_count\n",
        "            else 0\n",
        "        )\n",
        "        attempt += 1\n",
        "    total_imported += imported\n",
        "\n",
        "print(f\"{total_imported} files out of {total_num_of_files} imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EioRjxUN2aw"
      },
      "source": [
        "# 1. Option B. Bring your own existing `RagCorpus` (insert `RAG_CORPUS_ID` here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDCv3clrN616"
      },
      "outputs": [],
      "source": [
        "# Specify your rag corpus ID here that you want to use.\n",
        "RAG_CORPUS_ID = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "rag_corpus = rag.get_corpus(\n",
        "    name=f\"projects/{PROJECT_ID}/locations/{LOCATION}/ragCorpora/{RAG_CORPUS_ID}\"\n",
        ")\n",
        "\n",
        "print(rag_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXsmEoZNOPLo"
      },
      "source": [
        "# 2. Run Retrieval Quality Evaluation\n",
        "\n",
        "For Retrieval Quality Evaluation, we focus on the following metrics:\n",
        "\n",
        "- **Recall@k:**\n",
        "  - Measures how many of the relevant documents/chunks are successfully retrieved within the top k results\n",
        "  - Helps evaluate the retrieval component's ability to find ALL relevant information\n",
        "- **Precision@k:**\n",
        "  - Measures the proportion of retrieved documents that are actually relevant within the top k results\n",
        "  - Helps evaluate how \"focused\" your retrieval is\n",
        "- **nDCG@K:**\n",
        "  - Measures both relevance AND ranking quality\n",
        "  - Takes into account the position of relevant documents\n",
        "\n",
        "Follow the Notebook to get these metrics numbers for your configurations, and to optimize your settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5sz02OdOPYk"
      },
      "source": [
        "### Define evaluation helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "VUHxSZ7hwZ6k"
      },
      "outputs": [],
      "source": [
        "def extract_rag_id(rag_resource_name):\n",
        "    \"\"\"Extracts the RAG ID from a RAG resource name.\n",
        "\n",
        "    Args:\n",
        "      rag_resource_name: The full name of the RAG resource.\n",
        "\n",
        "    Returns:\n",
        "      The RAG ID extracted from the resource name.\n",
        "    \"\"\"\n",
        "    return rag_resource_name.split(\"/\")[-1]\n",
        "\n",
        "\n",
        "def extract_doc_id(file_path):\n",
        "    \"\"\"Extracts the document ID from a file path.\n",
        "\n",
        "    The function assumes the document ID is the number before the '.txt' extension in the file path.\n",
        "\n",
        "    Args:\n",
        "      file_path: The path to the file.\n",
        "\n",
        "    Returns:\n",
        "      The document ID extracted from the file path, or None if no ID is found.\n",
        "    \"\"\"\n",
        "    # Use regular expression to find the number before .txt, with optional /\n",
        "    match = re.search(r\"/?(\\d+)\\.txt$\", file_path)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# RAG Engine helper function to extract doc_id, snippet, and score.\n",
        "\n",
        "\n",
        "def extract_retrieval_details(response):\n",
        "    doc_id = extract_doc_id(response.source_uri)\n",
        "    retrieved_snippet = response.text\n",
        "    distance = response.distance\n",
        "    return (doc_id, retrieved_snippet, distance)\n",
        "\n",
        "\n",
        "# RAG Engine helper function for retrieval.\n",
        "\n",
        "\n",
        "def rag_api_retrieve(query, corpus_id, top_k):\n",
        "    return rag.retrieval_query(\n",
        "        rag_resources=[\n",
        "            rag.RagResource(\n",
        "                rag_corpus=f\"projects/{PROJECT_ID}/locations/{LOCATION}/ragCorpora/{corpus_id}\"\n",
        "            )\n",
        "        ],\n",
        "        text=query,\n",
        "        similarity_top_k=top_k,\n",
        "        vector_distance_threshold=0.5,\n",
        "    ).contexts.contexts\n",
        "\n",
        "\n",
        "def calculate_document_level_recall_precision(retrieved_response, cur_qrel):\n",
        "    \"\"\"Calculates the recall and precision for a list of sorted retrieved contexts at a given top_k value.\n",
        "\n",
        "    Args:\n",
        "      sorted_retrieved_contexts: A list of retrieved contexts sorted by vector distance in ascending order (smallest distance first).\n",
        "      cur_qrel: qrel[query_id] dictionary.\n",
        "\n",
        "    Returns:\n",
        "      A tuple containing the recall, precision and ndcg scores.\n",
        "    \"\"\"\n",
        "    if not retrieved_response:\n",
        "        return (0, 0)\n",
        "\n",
        "    relevant_retrieved_unique = set()\n",
        "    num_relevant_retrieved_snippet = 0\n",
        "    for res in retrieved_response:\n",
        "        doc_id, text, score = extract_retrieval_details(res)\n",
        "        if doc_id in cur_qrel:\n",
        "            relevant_retrieved_unique.add(doc_id)\n",
        "            num_relevant_retrieved_snippet += 1\n",
        "    recall = len(relevant_retrieved_unique) / len(cur_qrel.keys())\n",
        "    precision = num_relevant_retrieved_snippet / len(retrieved_response)\n",
        "    return (recall, precision)\n",
        "\n",
        "\n",
        "def calculate_document_level_metrics(queries, qrels, k_values, corpus_id):\n",
        "    \"\"\"Calculates the average recall, precision, and NDCG for a set of queries.\n",
        "\n",
        "    Args:\n",
        "      queries: A dictionary of queries with query IDs as keys and query text as values.\n",
        "      qrels: A dictionary of ground truth relevant documents for each query.\n",
        "      top_k: The number of top results to consider for evaluation.\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "\n",
        "    Prints:\n",
        "      A tuple containing the average recall, average precision, and average NDCG scores.\n",
        "    \"\"\"\n",
        "\n",
        "    for top_k in k_values:\n",
        "        start_time = time.time()\n",
        "        total_recall, total_precision, total_ndcg = 0, 0, 0\n",
        "        print(f\"Computing metrics for top_k value: {top_k}\")\n",
        "        print(f\"Total number of queries: {len(queries)}\")\n",
        "        for query_id, query in tqdm(\n",
        "            queries.items(),\n",
        "            total=len(queries),\n",
        "            desc=f\"Processing Queries (top_k={top_k})\",\n",
        "        ):\n",
        "            response = rag_api_retrieve(query, corpus_id, top_k)\n",
        "\n",
        "            recall, precision = calculate_document_level_recall_precision(\n",
        "                response, qrels[query_id]\n",
        "            )\n",
        "            ndcg = ndcg_at_k(response, qrels[query_id], top_k)\n",
        "\n",
        "            total_recall += recall\n",
        "            total_precision += precision\n",
        "            total_ndcg += ndcg\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        num_queries = len(queries)\n",
        "        average_recall, average_precision, average_ndcg = (\n",
        "            total_recall / num_queries,\n",
        "            total_precision / num_queries,\n",
        "            total_ndcg / num_queries,\n",
        "        )\n",
        "        print(f\"\\nAverage Recall@{top_k}: {average_recall:.4f}\")\n",
        "        print(f\"Average Precision@{top_k}: {average_precision:.4f}\")\n",
        "        print(f\"Average nDCG@{top_k}: {average_ndcg:.4f}\")\n",
        "        print(f\"Execution time: {execution_time} seconds.\")\n",
        "        print(\"=============================================\")\n",
        "\n",
        "\n",
        "def dcg_at_k_with_zero_padding_if_needed(r, k):\n",
        "    r = np.asarray(r)[:k]\n",
        "    if r.size:\n",
        "        # Pad with zeros if r is shorter than k\n",
        "        if r.size < k:\n",
        "            r = np.pad(r, (0, k - r.size))\n",
        "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, k + 2)))\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def ndcg_at_k(retriever_results, ground_truth_relevances, k):\n",
        "    if not retriever_results:\n",
        "        return 0\n",
        "\n",
        "    # Prepare retriever results\n",
        "    retrieved_relevances = []\n",
        "    for res in retriever_results[:k]:\n",
        "        doc_id, text, score = extract_retrieval_details(res)\n",
        "        if doc_id in ground_truth_relevances:\n",
        "            retrieved_relevances.append(ground_truth_relevances[doc_id])\n",
        "        else:\n",
        "            retrieved_relevances.append(0)  # Assume irrelevant if not in ground truth\n",
        "\n",
        "    # Calculate DCG\n",
        "    dcg = dcg_at_k_with_zero_padding_if_needed(retrieved_relevances, k)\n",
        "    # Calculate IDCG\n",
        "    ideal_relevances = sorted(ground_truth_relevances.values(), reverse=True)\n",
        "    idcg = dcg_at_k_with_zero_padding_if_needed(ideal_relevances, k)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyBPq_8fOPbL"
      },
      "source": [
        "### Run Retrieval Quality Evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7092Mp2syWPG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing metrics for top_k value: 5\n",
            "Total number of queries: 648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Queries (top_k=5): 100%|██████████| 648/648 [44:47<00:00,  4.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Recall@5: 0.5608\n",
            "Average Precision@5: 0.2713\n",
            "Average nDCG@5: 0.4450\n",
            "Execution time: 2687.608230829239 seconds.\n",
            "=============================================\n",
            "Computing metrics for top_k value: 10\n",
            "Total number of queries: 648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Queries (top_k=10): 100%|██████████| 648/648 [37:31<00:00,  3.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Recall@10: 0.6571\n",
            "Average Precision@10: 0.1679\n",
            "Average nDCG@10: 0.4039\n",
            "Execution time: 2251.886693954468 seconds.\n",
            "=============================================\n",
            "Computing metrics for top_k value: 100\n",
            "Total number of queries: 648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Queries (top_k=100): 100%|██████████| 648/648 [38:48<00:00,  3.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Recall@100: 0.8801\n",
            "Average Precision@100: 0.0253\n",
            "Average nDCG@100: 0.2592\n",
            "Execution time: 2328.4095141887665 seconds.\n",
            "=============================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "calculate_document_level_metrics(\n",
        "    queries, qrels, [5, 10, 100], corpus_id=extract_rag_id(rag_corpus.name)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfL6wSwyJMTe"
      },
      "source": [
        "# 3. Next steps\n",
        "* Once we're done with evaluation, we should carefully examine the metrics number are tune the hypeparameters. Below are some suggestions on how to optimize the hyperparameters to get the best retrieval quality.\n",
        "\n",
        "### How to optimize Recall:\n",
        "* If your recall metrics number is too low, consider the following steps:\n",
        "  * **Reducing chunk size:** Sometimes important information might be buried within large chunks, making it more difficult to retrieve relevant context. Try reducing the chunk size.\n",
        "  * **Increasing chunk overlap:** If the chunk overlap is too small, some relevant information at the edge might be lost. Consider increasing the chunk overlap (chunk overlap of 20% of chunk size is generally a good start.)\n",
        "  * **Increasing top-K:** If your top k is too small, the retriever might miss some relevant information due to a too restrictive context.\n",
        "\n",
        "### How to optimize Precision:\n",
        "* If your precision number is low, consider:\n",
        "  * **Reducing top-K:** Your top k might be too large, adding a lot of unwanted noise to the retrieved contexts.\n",
        "  * **Reducing chunk overlap:** Sometimes, too large of a chunk overlap could result in duplicate information.\n",
        "  * **Increasing chunk size:** If your chunk size is too small, it might lack sufficient context resulting in a low precision score.\n",
        "\n",
        "### How to optimize nDCG:\n",
        "* If your nDCG number is low, consider:\n",
        "  * **Changing your embedding model:** your embedding model might not capturing relevance well. Consider using a different embedding model (e.g. if your documents are multilingual, consider using a mulilingual embedding model). For more information on the currently supported embedding models, see documentation [here](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview#supported-embedding-models).\n",
        "\n",
        "### Evaluate Response Quality\n",
        "* If you want to evaluate response quality (generated answers) on top of retrieval quality, please refer to the [Gen AI Evaluation Service - RAG Evaluation Notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_rag_gen_ai_evaluation_service_sdk.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "# 4. Cleaning up (Delete `RagCorpus`)\n",
        "\n",
        "Once we are done with evaluation, we should clean up the `RagCorpus` to free up resources since we don't need it anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGIUz542z8To"
      },
      "outputs": [],
      "source": [
        "rag.delete_corpus(rag_corpus.name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "j0sHOGwdTDXZ",
        "m5sz02OdOPYk"
      ],
      "name": "rag_engine_evaluation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
