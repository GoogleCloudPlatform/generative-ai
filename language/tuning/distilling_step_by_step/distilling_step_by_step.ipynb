{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3710d312-cbbe-4fc1-9db6-712e8bccddd9",
      "metadata": {
        "id": "3710d312-cbbe-4fc1-9db6-712e8bccddd9"
      },
      "source": [
        "# Distillation step-by-step\n",
        "### A step-by-step guide\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763bc5ee",
      "metadata": {
        "id": "64d570be0973"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Anirudh Haritas Murali](https://github.com/anihm136) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7be8b8d-6e73-427f-a420-4e23a0001ad8",
      "metadata": {
        "id": "a7be8b8d-6e73-427f-a420-4e23a0001ad8"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e14832e-4aab-4051-b37f-1718db87c252",
      "metadata": {
        "id": "3e14832e-4aab-4051-b37f-1718db87c252"
      },
      "source": [
        "**Distillation** is a technique in machine learning that allows us to extract the learnings of a large model and represent it using a smaller model. This allows for improved scalability, as the smaller model requires less resources to run and less time to generate inferences while still achieving accuracy close to that of the larger model.\n",
        "\n",
        "Traditionally, distillation uses the internal parameters of the larger model (specifically, the logits) to train the smaller model. However, some of the best performing large language models today, including Google's [PaLM 2](https://ai.google/discover/palm2/) model, are exposed to consumers as an API, with no means to access the internal parameters. Until recently, this has prohibited the use of these models as teacher models for distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c72c65c-b6b5-47ac-915b-f1a19c5cf312",
      "metadata": {
        "id": "3c72c65c-b6b5-47ac-915b-f1a19c5cf312"
      },
      "source": [
        "## Objectives\n",
        "In this notebook, we will go over the technique described in the paper [Distilling step-by-step](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html), which describes a novel approach to distill the knowledge of a large LLM into a smaller LLM without requiring the internal parameters of the larger model. The original code from the research is available at [https://github.com/google-research/distilling-step-by-step](https://github.com/google-research/distilling-step-by-step).\n",
        "\n",
        "We will go through each step of training a small (student) model to mimic the reasoning ability of a larger (teacher) model. By training the student model to mimic the reasoning ability rather than the actual outputs, we can make the smaller model generalize better to other unseen inputs.\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Preparing a dataset for distillation\n",
        "- Setting up a distillation pipeline\n",
        "- Training a student model using PaLM as a teacher model\n",
        "- Evaluating the distilled model's performance\n",
        "- Deploying the distilled model to Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d4286f-c81f-4b5d-8bed-b2eb1daf550c",
      "metadata": {
        "id": "d7d4286f-c81f-4b5d-8bed-b2eb1daf550c"
      },
      "source": [
        "## Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "- Artifact Registry\n",
        "- Cloud Build\n",
        "    \n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing) and [Cloud Build pricing](https://cloud.google.com/build/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88b40ab-9b54-4435-aa50-5746381cd4e7",
      "metadata": {
        "id": "b88b40ab-9b54-4435-aa50-5746381cd4e7"
      },
      "source": [
        "# Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xW8juKok2cJF",
      "metadata": {
        "id": "xW8juKok2cJF"
      },
      "source": [
        "## (Only on Colab) Authenticate as user\n",
        "On Colab, we will authenticate as a user that has access to the Google Cloud resources mentioned above. This will be needed when we deploy the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HAeoGdSR2q5m",
      "metadata": {
        "id": "HAeoGdSR2q5m"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8xk5H9iF04xH",
      "metadata": {
        "id": "8xk5H9iF04xH"
      },
      "source": [
        "## Download supporting files\n",
        "To simplify the process of running this demo, some supporting files are provided (PaLM outputs for the dataset and code for building the model serving container)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4rgWlsRg05Nh",
      "metadata": {
        "id": "4rgWlsRg05Nh"
      },
      "outputs": [],
      "source": [
        "! gsutil -m cp -r gs://github-repo/distillation/* .\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/requirements.txt\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/Dockerfile -P prediction_container\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/main.py -P prediction_container/app\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/requirements.txt -P prediction_container/app\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/requirements-torch.txt -P prediction_container/app\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/prestart.sh -P prediction_container/app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ce302a-67c3-41b4-80ed-4d48d783170a",
      "metadata": {
        "id": "c8ce302a-67c3-41b4-80ed-4d48d783170a"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306ce182-e072-4576-9c6c-08652b3f1e16",
      "metadata": {
        "id": "306ce182-e072-4576-9c6c-08652b3f1e16"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32651b1a-d005-478b-8c8a-adc58b30c6e5",
      "metadata": {
        "id": "32651b1a-d005-478b-8c8a-adc58b30c6e5"
      },
      "source": [
        "## Enable required Google Cloud APIs\n",
        "For ease of cleaning up resources, you can create a new project and delete it at the end of this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ee8dab-5f96-4d46-a29c-1d8ac6ed825e",
      "metadata": {
        "id": "e5ee8dab-5f96-4d46-a29c-1d8ac6ed825e"
      },
      "outputs": [],
      "source": [
        "PROJECT = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62c1014-5024-4407-8c11-4d4aa278141b",
      "metadata": {
        "id": "b62c1014-5024-4407-8c11-4d4aa278141b"
      },
      "outputs": [],
      "source": [
        "!gcloud services enable aiplatform.googleapis.com --project {PROJECT}\n",
        "!gcloud services enable artifactregistry.googleapis.com --project {PROJECT}\n",
        "!gcloud services enable cloudbuild.googleapis.com --project {PROJECT}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f998a431-4219-4716-a61a-b7ebe48cace5",
      "metadata": {
        "id": "f998a431-4219-4716-a61a-b7ebe48cace5"
      },
      "source": [
        "# Step 1: Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff2eb53-9454-46ac-b3a4-5b01e93ba78e",
      "metadata": {
        "id": "aff2eb53-9454-46ac-b3a4-5b01e93ba78e"
      },
      "source": [
        "Our dataset will need three fields -\n",
        "1. An input prompt for the LLM\n",
        "2. A ground truth label, which is the expected output\n",
        "3. A 'rationale', which is the reasoning generated by the teacher model (using CoT prompting)\n",
        "\n",
        "Here, we will use the [Common Sense Explanations](https://huggingface.co/datasets/cos_e) dataset from Hugging Face to train our student model. This dataset contains around 10k training samples and 1.2k test samples. We will use pre-generated rationales from the PaLM model as a teacher, and we will preprocess the dataset to fit the above schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebefb0de-17b8-4f41-b56a-f3afdf21e8c3",
      "metadata": {
        "id": "ebefb0de-17b8-4f41-b56a-f3afdf21e8c3"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from datasets import DatasetDict, load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe43325-f2c7-4627-a561-1fe7aeb7828f",
      "metadata": {
        "id": "dfe43325-f2c7-4627-a561-1fe7aeb7828f"
      },
      "outputs": [],
      "source": [
        "SOURCE_DATASET = \"cos_e\"  # @param {type:\"string\"}\n",
        "SOURCE_DATASET_VERSION = \"v1.11\"  # @param {type:\"string\"}\n",
        "\n",
        "dataset = load_dataset(SOURCE_DATASET, SOURCE_DATASET_VERSION)\n",
        "dataset[\"test\"] = dataset[\"validation\"]\n",
        "del dataset[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a8dd69-70db-48bb-a421-71436ccd2ea9",
      "metadata": {
        "id": "52a8dd69-70db-48bb-a421-71436ccd2ea9"
      },
      "outputs": [],
      "source": [
        "def prepare_input(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    question = example[\"question\"]\n",
        "    c_0 = example[\"choices\"][0]\n",
        "    c_1 = example[\"choices\"][1]\n",
        "    c_2 = example[\"choices\"][2]\n",
        "    c_3 = example[\"choices\"][3]\n",
        "    c_4 = example[\"choices\"][4]\n",
        "\n",
        "    input = f\"{question}\\nAnswer Choices:\\n(a) {c_0}\\n(b) {c_1}\\n(c) {c_2}\\n(d) {c_3}\\n(e) {c_4}\"\n",
        "\n",
        "    example[\"input\"] = input\n",
        "    example[\"label\"] = example[\"answer\"]\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    prepare_input,\n",
        "    remove_columns=[\n",
        "        \"id\",\n",
        "        \"question\",\n",
        "        \"choices\",\n",
        "        \"answer\",\n",
        "        \"abstractive_explanation\",\n",
        "        \"extractive_explanation\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c110ef-384f-4b8e-a58a-8c81e3ad6d26",
      "metadata": {
        "id": "81c110ef-384f-4b8e-a58a-8c81e3ad6d26"
      },
      "outputs": [],
      "source": [
        "LLM_OUTPUTS_FILE_PREFIX = \"PaLM_CoT\"  # @param {type:\"string\"}\n",
        "LLM_OUTPUTS_FILE = LLM_OUTPUTS_FILE_PREFIX + \"_{split}.json\"\n",
        "\n",
        "\n",
        "def add_llm_outputs(dataset: DatasetDict, split: str) -> None:\n",
        "    llm_ds = load_dataset(\"json\", data_files=LLM_OUTPUTS_FILE.format(split=split))[\n",
        "        \"train\"\n",
        "    ]\n",
        "\n",
        "    def _add(example: dict[str, Any], idx: int) -> dict[str, Any]:\n",
        "        example[\"llm_rationale\"] = llm_ds[idx][\"rationale\"]\n",
        "        example[\"llm_label\"] = llm_ds[idx][\"label\"]\n",
        "        return example\n",
        "\n",
        "    dataset[split] = dataset[split].map(_add, with_indices=True)\n",
        "\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    add_llm_outputs(dataset, split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80e8263-1c01-4687-ade0-e85a194e17f6",
      "metadata": {
        "id": "e80e8263-1c01-4687-ade0-e85a194e17f6"
      },
      "source": [
        "# Step 2: Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9306cc-8db2-4715-9ced-8a72d5e47fad",
      "metadata": {
        "id": "cc9306cc-8db2-4715-9ced-8a72d5e47fad"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019a6709-25ba-4036-9735-47c8a105abf6",
      "metadata": {
        "id": "019a6709-25ba-4036-9735-47c8a105abf6"
      },
      "source": [
        "Here, we will use the T5 model as a pretrained base for distillation, and we will use the corresponding tokenizer. You can use a different pretrained model (and corresponding tokenizer) by changing the name of the model below to a different model on Hugging Face Hub, or use a custom model/train a tokenizer from scratch on your own dataset. Note that you will need significantly more data and compute to train a good model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da620fa-5e76-47b5-b29f-08eaa239ad21",
      "metadata": {
        "id": "5da620fa-5e76-47b5-b29f-08eaa239ad21"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_BASE_MODEL = \"google/flan-t5-base\"  # @param {type:\"string\"}\n",
        "MAX_INPUT_LENGTH = 1024  # @param {type:\"integer\"}\n",
        "MAX_OUTPUT_LENGTH = 256  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4880c1dc-c88a-43f7-a25c-a7fcb9c410ae",
      "metadata": {
        "id": "4880c1dc-c88a-43f7-a25c-a7fcb9c410ae"
      },
      "source": [
        "## a) Prepare the tokenizer and tokenize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7dceb4-582a-49b6-8fff-9ed709256863",
      "metadata": {
        "id": "af7dceb4-582a-49b6-8fff-9ed709256863"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
        "\n",
        "\n",
        "def tokenize_function(examples: dict[str, list[Any]]):\n",
        "    # Encode input to generate predictions and rationales\n",
        "    model_inputs = tokenizer(\n",
        "        [\"predict: \" + text for text in examples[\"input\"]],\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "    expl_model_inputs = tokenizer(\n",
        "        [\"explain: \" + text for text in examples[\"input\"]],\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "    model_inputs[\"expl_input_ids\"] = expl_model_inputs[\"input_ids\"]\n",
        "    model_inputs[\"expl_attention_mask\"] = expl_model_inputs[\"attention_mask\"]\n",
        "\n",
        "    # Encode target label and target rationale\n",
        "    label_output_encodings = tokenizer(\n",
        "        text_target=examples[\"label\"], max_length=MAX_OUTPUT_LENGTH, truncation=True\n",
        "    )\n",
        "    rationale_output_encodings = tokenizer(\n",
        "        text_target=examples[\"llm_rationale\"],\n",
        "        max_length=MAX_OUTPUT_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "    model_inputs[\"labels\"] = label_output_encodings[\"input_ids\"]\n",
        "    model_inputs[\"expl_labels\"] = rationale_output_encodings[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=[\"input\", \"llm_rationale\", \"label\", \"llm_label\"],\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4eb46c2-961b-4c30-a175-bdd36a325a88",
      "metadata": {
        "id": "c4eb46c2-961b-4c30-a175-bdd36a325a88"
      },
      "source": [
        "## b) Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3eab9b-ee7d-49da-8875-c14693358e4f",
      "metadata": {
        "id": "1c3eab9b-ee7d-49da-8875-c14693358e4f"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)\n",
        "# Uncomment if you have more than one GPU to enable parallelism\n",
        "# model.parallelize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9507d2e-6e78-4b38-ac22-208b9b81a791",
      "metadata": {
        "id": "b9507d2e-6e78-4b38-ac22-208b9b81a791"
      },
      "source": [
        "## c) Prepare data collator for multi-task training\n",
        "Since we need to generate predictions for both the answer as well as the rationale on each training and prediction step, we will use a custom DataCollator which will take each batch of features and return two sets of features and labels, one each for the answer and for the rationale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6695644-b788-4bab-ba72-cb8a2789ad3f",
      "metadata": {
        "id": "e6695644-b788-4bab-ba72-cb8a2789ad3f"
      },
      "outputs": [],
      "source": [
        "class TaskPrefixDataCollator(DataCollatorForSeq2Seq):\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        features_df = pd.DataFrame(features)\n",
        "\n",
        "        # Generate features for answers\n",
        "        ans_features = features_df.loc[\n",
        "            :, features_df.columns.isin([\"labels\", \"input_ids\", \"attention_mask\"])\n",
        "        ].to_dict(\"records\")\n",
        "        ans_features = super().__call__(ans_features, return_tensors)\n",
        "\n",
        "        # Generate features for explanations\n",
        "        expl_features = (\n",
        "            features_df.loc[\n",
        "                :,\n",
        "                features_df.columns.isin(\n",
        "                    [\"expl_labels\", \"expl_input_ids\", \"expl_attention_mask\"]\n",
        "                ),\n",
        "            ]\n",
        "            .rename(\n",
        "                columns={\n",
        "                    \"expl_labels\": \"labels\",\n",
        "                    \"expl_input_ids\": \"input_ids\",\n",
        "                    \"expl_attention_mask\": \"attention_mask\",\n",
        "                }\n",
        "            )\n",
        "            .to_dict(\"records\")\n",
        "        )\n",
        "        expl_features = super().__call__(expl_features, return_tensors)\n",
        "\n",
        "        return {\n",
        "            \"ans\": ans_features,\n",
        "            \"expl\": expl_features,\n",
        "        }\n",
        "\n",
        "\n",
        "data_collator = TaskPrefixDataCollator(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552e5c8c-0830-43ad-bcc8-ccdbfc8c3776",
      "metadata": {
        "id": "552e5c8c-0830-43ad-bcc8-ccdbfc8c3776"
      },
      "source": [
        "## d) Prepare trainer for multi-task training\n",
        "Similarly, we will use a custom Trainer for training the model, which takes into account both the losses for answer generation as well as rationale generation. We will use a hyperparameter `alpha` to control the relative contribution of the two losses to the overall model loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f0083b-b494-4dd8-ab58-29f850d2fa89",
      "metadata": {
        "id": "58f0083b-b494-4dd8-ab58-29f850d2fa89"
      },
      "outputs": [],
      "source": [
        "class TaskPrefixTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, alpha, output_rationale, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.output_rationale = output_rationale\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        ans_outputs = model(**inputs[\"ans\"])\n",
        "        expl_outputs = model(**inputs[\"expl\"])\n",
        "\n",
        "        loss = self.alpha * ans_outputs.loss + (1.0 - self.alpha) * expl_outputs.loss\n",
        "\n",
        "        return (\n",
        "            (loss, {\"ans\": ans_outputs, \"expl\": expl_outputs})\n",
        "            if return_outputs\n",
        "            else loss\n",
        "        )\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        ans_outputs = super().prediction_step(\n",
        "            model, inputs[\"ans\"], prediction_loss_only=False, ignore_keys=ignore_keys\n",
        "        )\n",
        "        if self.output_rationale:\n",
        "            expl_outputs = super().prediction_step(\n",
        "                model,\n",
        "                inputs[\"expl\"],\n",
        "                prediction_loss_only=False,\n",
        "                ignore_keys=ignore_keys,\n",
        "            )\n",
        "        else:\n",
        "            expl_outputs = ans_outputs  # placeholder only\n",
        "\n",
        "        loss = self.alpha * ans_outputs[0] + (1 - self.alpha) * expl_outputs[0]\n",
        "\n",
        "        return (\n",
        "            loss,\n",
        "            [ans_outputs[1], expl_outputs[1]],\n",
        "            [ans_outputs[2], expl_outputs[2]],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9019019-957d-4303-9c29-e07908ff6bbe",
      "metadata": {
        "id": "a9019019-957d-4303-9c29-e07908ff6bbe"
      },
      "source": [
        "# Step 3: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d06883-60c5-4916-b9d7-6ff82e2b40e3",
      "metadata": {
        "id": "82d06883-60c5-4916-b9d7-6ff82e2b40e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers.trainer_utils import set_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc86b1b2-1ff9-4249-b259-8d80a2c2c086",
      "metadata": {
        "id": "bc86b1b2-1ff9-4249-b259-8d80a2c2c086"
      },
      "outputs": [],
      "source": [
        "RUN_ID = 0  # @param {type:\"integer\"}\n",
        "CONFIG_DIR = \"distillation_outputs\"  # @param {type:\"string\"}\n",
        "CKPT_DIR = f\"{CONFIG_DIR}/ckpts/{RUN_ID}\"  # for model checkpoints\n",
        "LOG_DIR = f\"{CONFIG_DIR}/logs/{RUN_ID}\"  # for training logs\n",
        "\n",
        "EVAL_STEPS = 500  # @param {type:\"integer\"}\n",
        "SAVE_STEPS = 1000  # @param {type:\"integer\"}\n",
        "MAX_STEPS = 4000  # @param {type:\"integer\"}\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "ALPHA = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06445292-4027-444b-8a59-cb2ace9e808a",
      "metadata": {
        "id": "06445292-4027-444b-8a59-cb2ace9e808a"
      },
      "outputs": [],
      "source": [
        "set_seed(RUN_ID)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    CKPT_DIR,\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_dir=LOG_DIR,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=EVAL_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    predict_with_generate=True,\n",
        "    seed=RUN_ID,\n",
        "    local_rank=-1,\n",
        "    bf16=False,\n",
        "    generation_max_length=64,\n",
        "    prediction_loss_only=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e470d3a-8598-4899-8a45-b5ee92704198",
      "metadata": {
        "id": "2e470d3a-8598-4899-8a45-b5ee92704198"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Callable\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def compute_metrics_text(tokenizer: AutoTokenizer) -> Callable:\n",
        "    def compute_metrics(eval_pred: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
        "        predictions, labels = eval_pred\n",
        "        decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
        "\n",
        "        labels = np.where(labels[0] != -100, labels[0], tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        acc = np.mean(np.array(decoded_preds) == np.array(decoded_labels))\n",
        "\n",
        "        return {\"accuracy\": acc}\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "compute_metrics = compute_metrics_text(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ccb217-b010-4481-8607-e98363bb3776",
      "metadata": {
        "id": "b8ccb217-b010-4481-8607-e98363bb3776"
      },
      "outputs": [],
      "source": [
        "trainer_kwargs = {\n",
        "    \"alpha\": ALPHA,\n",
        "    \"output_rationale\": False,\n",
        "    \"model\": model,\n",
        "    \"args\": training_args,\n",
        "    \"train_dataset\": tokenized_dataset[\"train\"],\n",
        "    \"eval_dataset\": {\n",
        "        \"test\": tokenized_dataset[\"test\"],\n",
        "    },\n",
        "    \"data_collator\": data_collator,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"compute_metrics\": compute_metrics,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b687fe1b-40ad-4ae0-bdf2-cd4a50c2154c",
      "metadata": {
        "id": "b687fe1b-40ad-4ae0-bdf2-cd4a50c2154c"
      },
      "outputs": [],
      "source": [
        "trainer = TaskPrefixTrainer(**trainer_kwargs)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cde36bb-55c8-4e8b-88f0-a63493c22503",
      "metadata": {
        "id": "1cde36bb-55c8-4e8b-88f0-a63493c22503"
      },
      "source": [
        "# Step 4: Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394bb4ab-ce55-43f3-b8a2-ae01afd0f578",
      "metadata": {
        "id": "394bb4ab-ce55-43f3-b8a2-ae01afd0f578"
      },
      "source": [
        "Now let's compare the performance of our distilled student model against the PaLM model. We will also try to generate outputs from the base student model to compare the difference that the distilled training method has made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569ac9c6-dc0b-4bbc-9403-b9447055761b",
      "metadata": {
        "id": "569ac9c6-dc0b-4bbc-9403-b9447055761b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b54f85d-450a-474a-b005-15f44f594a3d",
      "metadata": {
        "id": "5b54f85d-450a-474a-b005-15f44f594a3d"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT = f\"{CKPT_DIR}/checkpoint-4000\"\n",
        "\n",
        "distilled_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "distilled_model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eafc3e5-6116-4370-be03-96a88c08fc0f",
      "metadata": {
        "id": "9eafc3e5-6116-4370-be03-96a88c08fc0f"
      },
      "outputs": [],
      "source": [
        "distill_generator = pipeline(\n",
        "    \"text2text-generation\", model=distilled_model, tokenizer=distilled_tokenizer\n",
        ")\n",
        "base_generator = pipeline(\n",
        "    \"text2text-generation\", model=base_model, tokenizer=base_tokenizer\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answers(sample: dict[str, Any]) -> dict[str, Any]:\n",
        "    sample[\"distill_label\"] = distill_generator([\"predict: \" + sample[\"input\"]])[0][\n",
        "        \"generated_text\"\n",
        "    ]\n",
        "    sample[\"base_label\"] = base_generator(sample[\"input\"])[0][\"generated_text\"]\n",
        "    return sample\n",
        "\n",
        "\n",
        "output_dataset = dataset[\"test\"].map(generate_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc3a499-0861-42b8-9d30-953d08b32543",
      "metadata": {
        "id": "afc3a499-0861-42b8-9d30-953d08b32543"
      },
      "outputs": [],
      "source": [
        "output_df = output_dataset.to_pandas().drop(\"llm_rationale\", axis=1)\n",
        "display_df = output_df.copy().rename(\n",
        "    columns={\n",
        "        \"input\": \"Question\",\n",
        "        \"label\": \"True answer\",\n",
        "        \"llm_label\": \"PaLM answer\",\n",
        "        \"base_label\": \"T5 answer\",\n",
        "        \"distill_label\": \"Distilled T5 answer\",\n",
        "    }\n",
        ")\n",
        "display_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e05aed0-57ab-4097-aad0-74bef2d7451f",
      "metadata": {
        "id": "8e05aed0-57ab-4097-aad0-74bef2d7451f"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"The accuracy of PaLM model is {:.2f}%\".format(\n",
        "        output_df[output_df[\"label\"] == output_df[\"llm_label\"]][\"label\"].count()\n",
        "        / len(output_df)\n",
        "        * 100\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"The accuracy of raw student model is {:.2f}%\".format(\n",
        "        output_df[output_df[\"label\"] == output_df[\"base_label\"]][\"label\"].count()\n",
        "        / len(output_df)\n",
        "        * 100\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"The accuracy of distilled student model is {:.2f}%\".format(\n",
        "        output_df[output_df[\"label\"] == output_df[\"distill_label\"]][\"label\"].count()\n",
        "        / len(output_df)\n",
        "        * 100\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec44724e-a62c-4ce1-af69-5bdf268e6aaf",
      "metadata": {
        "id": "ec44724e-a62c-4ce1-af69-5bdf268e6aaf"
      },
      "source": [
        "As we can see, the raw pretrained student model is unable to generate answers. However, with just a few training samples and epochs, we are able to approach the accuracy of the PaLM model using the much smaller T5 model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9498af79-57dd-47e0-9072-5507b5527da7",
      "metadata": {
        "id": "9498af79-57dd-47e0-9072-5507b5527da7"
      },
      "source": [
        "# Step 5: Deploy the model to Vertex AI\n",
        "*Note: The steps below will create a Cloud Storage bucket and an Artifact Registry Docker repository with the given names. If you would like to use an existing bucket or repository, provide their names below and comment out the steps to create the resources as indicated*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a31f785-399a-46c3-9fac-0d1dbb47706d",
      "metadata": {
        "id": "8a31f785-399a-46c3-9fac-0d1dbb47706d"
      },
      "outputs": [],
      "source": [
        "STAGING_BUCKET = \"\"  # @param {type:\"string\"}\n",
        "ARTIFACTS_DIR = f\"{STAGING_BUCKET}/distilled-t5\"\n",
        "CHECKPOINT_STEP = 4000  # @param {type:\"integer\"}\n",
        "CHECKPOINT = f\"{CKPT_DIR}/checkpoint-{CHECKPOINT_STEP}\"\n",
        "DOCKER_REPO_NAME = \"distill-step-by-step\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37e1e8f-f224-47b2-aaed-1ce2fa69e254",
      "metadata": {
        "id": "e37e1e8f-f224-47b2-aaed-1ce2fa69e254"
      },
      "source": [
        "## Upload artifacts to Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5656f337-5cb0-4b87-a350-ab0412ae8b91",
      "metadata": {
        "id": "5656f337-5cb0-4b87-a350-ab0412ae8b91"
      },
      "outputs": [],
      "source": [
        "! gsutil mb gs://{STAGING_BUCKET} # comment to use existing bucket\n",
        "! gsutil -m cp {CHECKPOINT}/* gs://{ARTIFACTS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09255866-e1a4-4dba-aa87-bb256f11caba",
      "metadata": {
        "id": "09255866-e1a4-4dba-aa87-bb256f11caba"
      },
      "source": [
        "## Create a model serving container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db4f85a-d178-42cb-b4c0-190892014960",
      "metadata": {
        "id": "9db4f85a-d178-42cb-b4c0-190892014960"
      },
      "outputs": [],
      "source": [
        "!gcloud artifacts repositories create {DOCKER_REPO_NAME} --location {REGION} --repository-format=docker  # comment to use existing bucket\n",
        "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet\n",
        "!gcloud builds submit --tag {REGION}-docker.pkg.dev/{PROJECT}/{DOCKER_REPO_NAME}/distilled-flan-t5:latest ./prediction_container"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d3fb839-068b-49fd-9b73-85010487358f",
      "metadata": {
        "id": "5d3fb839-068b-49fd-9b73-85010487358f"
      },
      "source": [
        "## Upload model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76afb3a9-b982-49ef-885c-8ced4cfd3eb8",
      "metadata": {
        "id": "76afb3a9-b982-49ef-885c-8ced4cfd3eb8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "DEPLOY_IMAGE = (\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT}/{DOCKER_REPO_NAME}/distilled-flan-t5:latest\"\n",
        ")\n",
        "HEALTH_ROUTE = \"/health\"\n",
        "PREDICT_ROUTE = \"/predict\"\n",
        "SERVING_CONTAINER_PORTS = [7080]\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=f\"distilled-flan-t5\",\n",
        "    description=f\"Distilled Flan T5 model using Step-By-Step Distillation\",\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    serving_container_predict_route=PREDICT_ROUTE,\n",
        "    serving_container_health_route=HEALTH_ROUTE,\n",
        "    serving_container_ports=SERVING_CONTAINER_PORTS,\n",
        "    artifact_uri=f\"gs://{ARTIFACTS_DIR}\",\n",
        ")\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16abe463-0673-4741-b73f-c7b02167d6ed",
      "metadata": {
        "id": "16abe463-0673-4741-b73f-c7b02167d6ed"
      },
      "source": [
        "## Deploy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e9e798-f35c-4b28-8e47-5e8ee84c68ec",
      "metadata": {
        "id": "e2e9e798-f35c-4b28-8e47-5e8ee84c68ec"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model(model.resource_name)\n",
        "\n",
        "endpoint = model.deploy(\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
        "    accelerator_count=1,\n",
        "    traffic_percentage=100,\n",
        "    deploy_request_timeout=1200,\n",
        "    sync=True,\n",
        ")\n",
        "endpoint.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AuBi541Ijkq2",
      "metadata": {
        "id": "AuBi541Ijkq2"
      },
      "source": [
        "# Wrap up\n",
        "In this notebook, we have learnt how we can use a large teacher LLM to teach a smaller student LLM to reason like it, which greatly improves the performance of smaller models over simple instruction tuning.\n",
        "\n",
        "If you are interested in running a similar distillation pipeline on LLMs available on Google Cloud, check out [Distilling text models on Google Cloud](https://cloud.google.com/vertex-ai/docs/generative-ai/models/distill-text-models/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "distilling_step_by_step.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
