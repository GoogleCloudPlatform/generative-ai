{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUZfht_yNsbz"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm9QleQMtnKh",
    "tags": []
   },
   "source": [
    "# DescriptionGen: SEO-optimized product decription generation for retail using LangChain ü¶úüîó\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/description-generation/product_description_generator_attributes_to_text.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/description-generation/product_description_generator_attributes_to_text.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/description-generation/product_description_generator_attributes_to_text.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrJ2HPRW1cpA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use [LangChain](https://python.langchain.com/docs/get_started/introduction.html) together with Vertex AI LLMs to solve a problem that many large retailers face: the automatic generation of informative, SEO-optimized, and potentially creative product descriptions and titles, based on product attributes or specifications (often provided by a supplier). The process of automated description generation can achieve significant cost and time savings.\n",
    "\n",
    "This tutorial shows how to start with raw product attributes and metadata to generate full, accurate, SEO-optimized, and safe descriptions using LLMs. You will also learn how to validate the descriptions using LLMs. Additionally, you how to use retrieval-augmented generation to make your prompts even better through k-NN (k-nearest neighbor) embedding search. Finally, it shows how to adapt the descriptions to your brand's writing style, even if the style differs per product.\n",
    "\n",
    "Notes on running this tutorial:\n",
    "\n",
    "The free public dataset sample used for this demo (authorized for personal or business use) can be downloaded [here](https://data.world/promptcloud/product-details-on-flipkart-com).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use LangChain with the PaLM API to generate your product descriptions from existing product attributes. You will work through the following examples:\n",
    "\n",
    "* Zero-shot prompting of foundational models to generate descriptions\n",
    "* Zero-shot prompting of parameter-efficient tuned foundational models on custom corpora\n",
    "* Few-shot untuned (and tuned) prompting using Vertex AI Embeddings to retrieve similar examples to incorporate into the prompt. This is done locally in-memory for the tutorial but can be extended to use [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) (a managed, scalable vector database)\n",
    "* Using prompt-engineered LLM zero-shot prompts to check the safety, veracity, and quality of generated product descriptions, and generate a reasoning for its evaluation\n",
    "* Evaluating the batch quality of generated prompts using n-gram overlap metrics like BLEU and ROUGE, as well as semantic similarity checks (using embeddings with the PaLM API) with both possible and negative examples\n",
    "* Using basic and advanced LangChain constructs such as prompt templates, LLM chains, sequential LLM chains (for sequential prompts with multiple inputs and outputs, with the output of one fed as input to another), k-NN retrievers, and custom LLM classes to use Vertex AI and LangChain.\n",
    "\n",
    "You will also see that using few-shot prompting computed using k-NN Vertex AI LLM embedding based  semantic similarity can boost the performance metrics across BLEU/ROUGE and semantic similarity. Additionally, you can add additional product properties by utilizing a Vertex Generative AI image captioning service, which can then also add to the richness of the product description.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdA2YcuDBxZA"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdPW7vEf0BRg"
   },
   "source": [
    "### Install Vertex AI SDK & Other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dea30abfe6e"
   },
   "outputs": [],
   "source": [
    "!pip3 install --user --upgrade  pydantic==1.10.9 \\\n",
    "                                keras-nlp==0.5.2 \\\n",
    "                                tensorflow==2.12.0 \\\n",
    "                                scikit-learn==1.2.2 \\\n",
    "                                lark==1.1.5 \\\n",
    "                                langchain==0.0.207 \\\n",
    "                                google-cloud-aiplatform==1.27.0 \\\n",
    "                                rouge-score==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvbNy16vvxLZ"
   },
   "source": [
    "Restart the kernel to re-load the packages you just installed. You may see a pop-up warning that you can simply close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8TaouAbvkAg"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "165a1dc0e619"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cells below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXUiRL2lvqE3"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agy6vPA3vrKl"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55VVrU1rMIGD"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import keras_nlp\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDqy9zKb-fxm"
   },
   "source": [
    "### Helper functions/classes for the rest of the code\n",
    "These functions & classes show you how you can create your own custom LLM models and embeddings (e.g. from tfhub) using base langchain constructs if needed to. Langchain also supports Vertex LLMs ( for generation and embedding) natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXc97lrn-YgM"
   },
   "outputs": [],
   "source": [
    "REQUESTS_PER_MINUTE = 16\n",
    "pp = pprint.PrettyPrinter(width=200)\n",
    "\n",
    "\n",
    "# for creating dynamic fewshot based on embedding based kNN approach\n",
    "def compute_fewshot(\n",
    "    query,\n",
    "    retriever,\n",
    "    ixed_df,\n",
    "    delimiter=\"\\n\",\n",
    "    input_label=\"input:\",\n",
    "    output_label=\"output:\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes in a query, a langchain retriever object and\n",
    "    a dataframe indexed on the product attributes, computes K nearest\n",
    "    neighbours based on embedding semantic similarity of product descriptions.\n",
    "    Then returns output as new-line delimited string of format key:value for\n",
    "    product attributes: product description of semantically similar products\n",
    "    to the original query.\n",
    "\n",
    "    E.g. Query = Color:White\n",
    "         Brand = Adidas\n",
    "\n",
    "       Output of function:\n",
    "       Input: Color:White \\n Brand= Nike\n",
    "       Output: These Nike sport shoes help you with your everyday run!\n",
    "\n",
    "       Input: Color:Grey \\n Brand= Adidas,\n",
    "       Output: Helps you protect your heels while running!\n",
    "    \"\"\"\n",
    "    results = list()\n",
    "    for spec in retriever.get_relevant_documents(query)[:3]:\n",
    "        results.append(\"{}{}{}\".format(input_label, delimiter, spec.page_content))\n",
    "        results.append(\n",
    "            \"%s%s%s\"\n",
    "            % (output_label, delimiter, ixed_df.loc[spec.page_content][\"description\"])\n",
    "        )\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "# to & extract parse the various fields in a clean and uniform key:value format\n",
    "def extract_tags(x, delimiter=\":\"):\n",
    "    \"\"\"\n",
    "    Takes in a row of a dataframe, & extracts/ parses the various fields to\n",
    "    create a newline delimited array of key:value pairs where key is the\n",
    "    name of the product attribute: and value is the value of the attribute itself\n",
    "\n",
    "    E.g. output\n",
    "       Color: White\n",
    "       Discounted_price: 200\n",
    "    \"\"\"\n",
    "    results = list()\n",
    "    name = x[\"product_name\"]\n",
    "    brand = x[\"brand\"]\n",
    "    price = x[\"discounted_price\"]\n",
    "    category = x[\"product_category_tree\"]\n",
    "    for sym in [\"[\", \"]\", '\"']:\n",
    "        category = category.replace(sym, \"\")\n",
    "\n",
    "    results.append(\"{}{}{}\".format(\"Product name\", delimiter, name))\n",
    "    results.append(\"{}{}{}\".format(\"brand\", delimiter, brand))\n",
    "    results.append(\"{}{}{}\".format(\"discounted price\", delimiter, price))\n",
    "    results.append(\"{}{}{}\".format(\"Product category\", delimiter, category))\n",
    "    x = x[\"product_specifications\"]\n",
    "\n",
    "    if \"nil\" in x:\n",
    "        return \"\"\n",
    "    x = json.loads(x.replace(\"=>\", \":\"))[\"product_specification\"]\n",
    "\n",
    "    if type(x) is not list:\n",
    "        results.append(\n",
    "            \"{}{}{}\".format(x.get(\"key\", \"other detail\"), delimiter, x.get(\"value\", \"\"))\n",
    "        )\n",
    "    else:\n",
    "        for attr in x:\n",
    "            results.append(\n",
    "                \"%s%s%s\"\n",
    "                % (attr.get(\"key\", \"other detail\"), delimiter, attr.get(\"value\", \"\"))\n",
    "            )\n",
    "\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "# to compute the quality metrics of the generated text w.r.t reference text using Bleu, Rouge\n",
    "# and semantic similarity scores\n",
    "def compute_quality_metrics_batch(\n",
    "    references, predictions, rouge_n_order=2, embedding=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a batch of generated text and corresponding reference texts,\n",
    "    computes and prints their corresponding\n",
    "    Bleu, Rouge and semantic similarity(using embeddings) scores.\n",
    "    \"\"\"\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=rouge_n_order)\n",
    "    bleu_n = keras_nlp.metrics.Bleu()\n",
    "    rouge_scr = rouge_n(references, predictions)[\"f1_score\"]\n",
    "    bleu_scr = bleu_n(references, predictions)\n",
    "    pp.pprint(\"Bleu:%s\" % (bleu_scr.numpy()))\n",
    "    pp.pprint(\"Rouge:%s\" % (rouge_scr.numpy()))\n",
    "    if embedding:\n",
    "        embed_predictions = embedding.embed_documents(predictions)\n",
    "        embed_references = embedding.embed_documents(references)\n",
    "        m = tf.keras.metrics.CosineSimilarity(axis=1)\n",
    "        m.update_state(embed_predictions, embed_references)\n",
    "        pp.pprint(\"Semantic Similarity:%s\" % (m.result().numpy()))\n",
    "\n",
    "\n",
    "class VertexLLM(LLM):\n",
    "    \"\"\"\n",
    "    Class to use Vertex AI LLMs to generate text throttled by specified\n",
    "    rate to avoid quota errors.\n",
    "    \"\"\"\n",
    "\n",
    "    model: TextGenerationModel\n",
    "    predict_kwargs: dict\n",
    "\n",
    "    def __init__(self, model, verbose, **predict_kwargs):\n",
    "        super().__init__(model=model, verbose=verbose, predict_kwargs=predict_kwargs)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"vertex\"\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        result = self.model.predict(prompt, **self.predict_kwargs)\n",
    "        return str(result)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {}\n",
    "\n",
    "\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            # print(f'Sleeping {sleep_time:.1f} seconds')\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class VertexEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    Class to use Vertex AI LLMs to generate embeddings by specified\n",
    "    rate to avoid quota errors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, *, requests_per_minute=20):\n",
    "        self.model = model\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches of 5 to stay below the quota limit\n",
    "            head, docs = docs[:5], docs[5:]\n",
    "            chunk = self.model.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        single_result = self.embed_documents([text])\n",
    "        return single_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sJCUeALAFh6"
   },
   "source": [
    "## Data preparation\n",
    "In this section, you will perform cleaning, parsing, preparing, and splitting of the full dataset. As part of the cleaning process, you will also ensure that the fields containing null or duplicate descriptions are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQNfBDFe_mKM"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"gs://github-repo/use-cases/product_description_generation_retail/dataset_sample.csv\"\n",
    ")\n",
    "df = (\n",
    "    df[~(df[\"product_specifications\"].str.contains(\"nil\", na=False))]\n",
    "    .dropna()\n",
    "    .drop_duplicates(subset=[\"description\"])\n",
    ")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dmi2y4-IAmKT"
   },
   "outputs": [],
   "source": [
    "df[\"parsed_product_specs\"] = df.apply(lambda x: extract_tags(x), axis=1)\n",
    "df_processed = df[[\"parsed_product_specs\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgxoBGMcAmTo"
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train, test = train_test_split(df_processed, test_size=0.05, random_state=42)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTrFHTqrAD-K"
   },
   "source": [
    "## Description generation using LLMs\n",
    "This section shows how you can use zero-shot, few-shot, and parameter-efficient fine-tuned approaches to continuously improve on BLEU, Rouge, and semantic similarity metrics. It also shows how you can use LLMs to tag text for toxicity and assess their results using BLEU, Rouge, and semantic similarity scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f406fd09480"
   },
   "source": [
    "### Import models\n",
    "\n",
    "You load the pre-trained text generation model called `text-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Si4OL7qjvufA"
   },
   "outputs": [],
   "source": [
    "# You need to provide a GCP project here\n",
    "embedding = VertexEmbeddings(\n",
    "    TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\"),\n",
    "    requests_per_minute=REQUESTS_PER_MINUTE,\n",
    ")\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "llm = VertexLLM(\n",
    "    model, max_output_tokens=512, temperature=0.1, top_p=0.8, top_k=40, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0Gxr8rdNkBO"
   },
   "source": [
    "Define re-usable prompt templates from LangChain and make a sequential chain which first generates the description using the first prompt and then evalutes it for safety using the second one. The input variables would be filled in at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvXKoqpy4gKy"
   },
   "outputs": [],
   "source": [
    "template_descgen = \"\"\"\n",
    "Imagine you are a copywriter for the online ecommerce platform {platform_name}. You have been given a series of information about a product and are asked to write a short and truthful product description based on that information, that can be put on the website.\n",
    "At the same time the description should be optimised for search engines, and look like a realistic description in freeform text for that product which could appear on the {platform_name}.com website. The information you have been given are the following:\n",
    "{product_attributes}\n",
    "\"\"\"\n",
    "template_evaluate = \"\"\"\n",
    "Does this text look like a real human written description for a product which could be found on\n",
    "{platform_name} and is also not toxic? Answer with one of the following options, along with a short explanation\n",
    "Options:\n",
    "- Yes\n",
    "- No\n",
    "\n",
    "text: {generated_description}\n",
    "Option:\n",
    "\"\"\"\n",
    "\n",
    "prompt_descgen = PromptTemplate(\n",
    "    input_variables=[\"platform_name\", \"product_attributes\"],\n",
    "    template=template_descgen,\n",
    ")\n",
    "prompt_eval = PromptTemplate(\n",
    "    input_variables=[\"platform_name\", \"generated_description\"],\n",
    "    template=template_evaluate,\n",
    ")\n",
    "eval_chain = LLMChain(llm=llm, prompt=prompt_eval, output_key=\"is_safe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgSmOWlWilji"
   },
   "source": [
    "### Method 1: Zero-shot description generation, validation & evaluation\n",
    "To create a product description, you first need to create a prompt with placeholder variables for the product attributes. These variables will be filled in with the actual product attributes at runtime. You then need to attach corresponding large language models (LLMs) to the description generation and evaluation models. Finally, you need to chain the models together so that the product description generated by the first model is sent as an input to the second model. The outputs of both models will then be available upon execution of the SequentialChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5AMPaHps7aw"
   },
   "outputs": [],
   "source": [
    "descgen_chain = LLMChain(\n",
    "    llm=llm, prompt=prompt_descgen, output_key=\"generated_description\"\n",
    ")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[descgen_chain, eval_chain],\n",
    "    input_variables=[\"platform_name\", \"product_attributes\"],\n",
    "    # Here you return multiple variables\n",
    "    output_variables=[\"generated_description\", \"is_safe\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYF6VPej8L98"
   },
   "outputs": [],
   "source": [
    "attrs = test[\"parsed_product_specs\"].iloc[4]\n",
    "orig_descr = test[\"description\"].iloc[4]\n",
    "pp.pprint(\"The original description:\\n\" + orig_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2UpEwRJ8S4s"
   },
   "outputs": [],
   "source": [
    "result_0shot_untuned = overall_chain(\n",
    "    {\"platform_name\": \"Flipkart\", \"product_attributes\": attrs}\n",
    ")\n",
    "pp.pprint(result_0shot_untuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKqe219mdqL"
   },
   "source": [
    "You see that the generated descriptions look SEO optimized and convincing, incorporating a lot of the product attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00857f4a8f49"
   },
   "source": [
    "Now you can evaluate the result of the above generated description against the original description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "031aee588484"
   },
   "outputs": [],
   "source": [
    "compute_quality_metrics_batch(\n",
    "    [result_0shot_untuned[\"generated_description\"]], [orig_descr], embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-lpxRIWFV35"
   },
   "source": [
    "Now you can perform the same evaluation for a randomly sampled batch of 10 product specs and evaluate their LLM-generated descriptions and against their original descriptions. The output will be the average of the evaluation metrics across the 10 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77676f6e9086"
   },
   "outputs": [],
   "source": [
    "sample_test = test[[\"parsed_product_specs\", \"description\"]].sample(10, random_state=42)\n",
    "sample_test[\"generated_description_0shot\"] = sample_test[\"parsed_product_specs\"].apply(\n",
    "    lambda x: overall_chain({\"platform_name\": \"Flipkart\", \"product_attributes\": x})[\n",
    "        \"generated_description\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_attrs = sample_test[\"parsed_product_specs\"].values.tolist()\n",
    "sample_descriptions = sample_test[\"description\"].values.tolist()\n",
    "sample_generated_descriptions_0shot = sample_test[\n",
    "    \"generated_description_0shot\"\n",
    "].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcBNP1DSFV35"
   },
   "source": [
    "Now check the averaged output of the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a93f469e7f1"
   },
   "outputs": [],
   "source": [
    "compute_quality_metrics_batch(\n",
    "    sample_generated_descriptions_0shot, sample_descriptions, embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JsYHvEJHADm"
   },
   "source": [
    "Now you can quickly skim through the original and generated descriptions and save the dataframe to disk in .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6EDUrC6YyuZ"
   },
   "outputs": [],
   "source": [
    "sample_test[[\"description\", \"generated_description_0shot\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJbwkmIjYli9"
   },
   "outputs": [],
   "source": [
    "sample_test.to_csv(\"./augmented_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXHR8mWhHehI"
   },
   "source": [
    "### Method 2: Few-shot description generation using dynamic k-nearest neighbours\n",
    "In this section, you will use few-shot prompting to try to improve the LLM-generated descriptions compared to the zero-shot prompting technique you used earlier.\n",
    "\n",
    "Here, instead of the few-shot examples being hardcoded or selected at random, the examples would be chosen by first embedding the query and document corpus, and then computing the k-nearest neighbors of the query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOSWKpzGHcuT"
   },
   "outputs": [],
   "source": [
    "template_descgen_fewshot = \"\"\"\n",
    "Imagine you are a copywriter for the online ecommerce platform {platform_name}. You have been given a series of information about a product as input and are asked to write a short and truthful product description based on that information as output, that can be put on the website.\n",
    "At the same time the description should also be optimised for search engines and look like a realistic description for that product which could appear on the {platform_name}.com website.\n",
    "{examples}\n",
    "input:\n",
    "{product_attributes}\n",
    "output:\n",
    "\"\"\"\n",
    "\n",
    "prompt_descgen_fewshot = PromptTemplate(\n",
    "    input_variables=[\"platform_name\", \"product_attributes\", \"examples\"],\n",
    "    template=template_descgen_fewshot,\n",
    ")\n",
    "\n",
    "descgen_chain_fewshot_untuned = LLMChain(\n",
    "    llm=llm, prompt=prompt_descgen_fewshot, output_key=\"generated_description\"\n",
    ")\n",
    "\n",
    "overall_chain_fewshot_untuned = SequentialChain(\n",
    "    chains=[descgen_chain_fewshot_untuned, eval_chain],\n",
    "    input_variables=[\"platform_name\", \"product_attributes\", \"examples\"],\n",
    "    # Here we return multiple variables\n",
    "    output_variables=[\"generated_description\", \"is_safe\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ojQA3MPVLN"
   },
   "source": [
    "To create input/output examples to guide the model, you can use Vertex LLM embedding-based k-nearest neighbour computation based on input product specs in the train set. You then do nearest neighbour computation based on the product specs/attributes\n",
    "in the train set and retrieve the corresponding description as well to guide the model through few-shot input/output examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkqPD9UQHc2u"
   },
   "outputs": [],
   "source": [
    "train_spec_ix = train.copy().set_index(\"parsed_product_specs\")\n",
    "retriever = KNNRetriever.from_texts(\n",
    "    train[\"parsed_product_specs\"].values.tolist()[:500], embedding\n",
    ")\n",
    "examples = compute_fewshot(attrs, retriever, train_spec_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy-byE3SHc__"
   },
   "outputs": [],
   "source": [
    "result_fewshot_untuned = overall_chain_fewshot_untuned(\n",
    "    {\"platform_name\": \"Flipkart\", \"product_attributes\": attrs, \"examples\": examples}\n",
    ")\n",
    "pp.pprint(result_fewshot_untuned[\"generated_description\"])\n",
    "pp.pprint(result_fewshot_untuned[\"is_safe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn3v1_JaHdI9"
   },
   "outputs": [],
   "source": [
    "sample_test[\"generated_description_fewshot\"] = sample_test[\n",
    "    \"parsed_product_specs\"\n",
    "].apply(\n",
    "    lambda x: overall_chain_fewshot_untuned(\n",
    "        {\n",
    "            \"platform_name\": \"Flipkart\",\n",
    "            \"product_attributes\": x,\n",
    "            \"examples\": compute_fewshot(x, retriever, train_spec_ix),\n",
    "        }\n",
    "    )[\"generated_description\"]\n",
    ")\n",
    "sample_generated_descriptions_fewshot = sample_test[\n",
    "    \"generated_description_fewshot\"\n",
    "].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR2cxH9hFV3-"
   },
   "source": [
    "You should now see that all of the metrics (Bleu, Rouge, and Semantic Similarity) should have all improved -- and in some cases improved remarkably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1lzZACsHdS9"
   },
   "outputs": [],
   "source": [
    "compute_quality_metrics_batch(\n",
    "    sample_generated_descriptions_fewshot, sample_descriptions, embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwqWdEZdZC3b"
   },
   "source": [
    "Now you can quickly skim through the original and generated descriptions and save the dataframe to disk in .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPAwQsHOaMtI"
   },
   "outputs": [],
   "source": [
    "sample_test[\n",
    "    [\"parsed_product_specs\", \"description\", \"generated_description_fewshot\"]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1M7zRR1aSh4"
   },
   "outputs": [],
   "source": [
    "sample_test.to_csv(\"./augmented_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct4xYbLW4Jvw"
   },
   "source": [
    "### Method 3: Fine-tuned zero-shot description generation validation & evaluation\n",
    "\n",
    "In this section you will perform parameter efficient fine tuning of the model on 500 randomly sampled (prompt, description) pairs from the training dataset, in order to tune the model to the description and writing style. Then you will generate descriptions for a batch of data and evaluate it against the original across the three metrics as demonstrated in previous sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY1gie8ZFV3-"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>‚ö†Ô∏è This section requires TPUs: Please note that fine tuning uses TPUs, hence you will need to ensure they are available to your project.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddd51ced-40ed-40fc-9b34-beaf314aa88d"
   },
   "outputs": [],
   "source": [
    "tuned_model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "train_tuning = train.copy()\n",
    "\n",
    "train_tuning[\"prompt_product_specs\"] = train_tuning[\"parsed_product_specs\"].apply(\n",
    "    lambda x: prompt_descgen.format(platform_name=\"Flipkart\", product_attributes=x)\n",
    ")\n",
    "\n",
    "train_tuning.rename(\n",
    "    columns={\"prompt_product_specs\": \"input_text\", \"description\": \"output_text\"},\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bMWIVc8wpqr"
   },
   "source": [
    "Note the code below will kickstart the tuning pipeline, which make take an hour or two to finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swIEdRtn4neB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_model.tune_model(\n",
    "    training_data=train_tuning.sample(10, random_state=42),\n",
    "    train_steps=1,\n",
    "    tuning_job_location=\"europe-west4\",\n",
    "    tuned_model_location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omVSG-X4BP6E"
   },
   "source": [
    "*Here* you load the most recently trained model and evaluate it on the same test sentences as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rw38HJHC5hJ"
   },
   "outputs": [],
   "source": [
    "model_id = tuned_model.list_tuned_model_names()[0]\n",
    "tuned_model = TextGenerationModel.get_tuned_model(tuned_model_name=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYB8si7PFV3_"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())  # started at 11:20am BST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbATrTBkiMbI"
   },
   "outputs": [],
   "source": [
    "llm_tuned = VertexLLM(\n",
    "    tuned_model,\n",
    "    max_output_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oyxQQLZhLFj"
   },
   "source": [
    "Create a new LLM chain with the same prompt template as before, just with the newly tuned model attached instead. Then include it in the sequential chain like you did with the zero-shot model, and then generate new descriptions for a batch of product attributes in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6iMPQxzbUGz"
   },
   "outputs": [],
   "source": [
    "descgen_chain_tuned = LLMChain(\n",
    "    llm=llm_tuned, prompt=prompt_descgen, output_key=\"generated_description\"\n",
    ")\n",
    "\n",
    "overall_chain_tuned = SequentialChain(\n",
    "    chains=[descgen_chain_tuned, eval_chain],\n",
    "    input_variables=[\"platform_name\", \"product_attributes\"],\n",
    "    # Here you return multiple variables\n",
    "    output_variables=[\"generated_description\", \"is_safe\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07e33ee8-d1db-4482-b32f-651dab737459"
   },
   "outputs": [],
   "source": [
    "result_0shot_tuned = overall_chain_tuned(\n",
    "    {\"platform_name\": \"Flipkart\", \"product_attributes\": attrs}\n",
    ")\n",
    "pp.pprint(result_0shot_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S18taNiPxDhL"
   },
   "outputs": [],
   "source": [
    "sample_test[\"generated_description_tuned_0shot\"] = sample_test[\n",
    "    \"parsed_product_specs\"\n",
    "].apply(\n",
    "    lambda x: overall_chain_tuned(\n",
    "        {\"platform_name\": \"Flipkart\", \"product_attributes\": x}\n",
    "    )[\"generated_description\"]\n",
    ")\n",
    "sample_generated_descriptions_tuned_0shot = sample_test[\n",
    "    \"generated_description_tuned_0shot\"\n",
    "].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaMUC1rphYzq"
   },
   "source": [
    "Compute batch Bleu, rouge and semantic similarity scores like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "660d0c4c81b3"
   },
   "outputs": [],
   "source": [
    "compute_quality_metrics_batch(\n",
    "    sample_generated_descriptions_tuned_0shot, sample_descriptions, embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6-A4bq7a_7D"
   },
   "source": [
    "Now you can quickly skim through the original and generated descriptions and save the dataframe to disk in .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WOlWldsa_aQ"
   },
   "outputs": [],
   "source": [
    "sample_test[\n",
    "    [\"parsed_product_specs\", \"description\", \"generated_description_tuned_0shot\"]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5KaCQnJbIKi"
   },
   "outputs": [],
   "source": [
    "sample_test.to_csv(\"./augmented_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1QWLiM6RLQg"
   },
   "source": [
    "### Method 4: Few-shot description generation validation & evaluation using fine-tuned models\n",
    "\n",
    "In this section, you perform parameter-efficient fine-tuning of the model on 500 randomly sampled (prompt, description) pairs from the training set in order to better match it to the description and writing style. Then, you generate descriptions for a batch and evaluate them against the original across the three metrics as done before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96d871810b88"
   },
   "source": [
    "Note: Since you did not train the tuned model in a few-shot fashion, sometimes it can be confused and generate unnatural text. In this case, you can rely on the validator model, which can catch this and be reused to reprompt until a valid reply is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29EXGCYKFV4A"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>‚ö†Ô∏è This section requires TPUs: Please note that fine tuning uses TPUs, hence you will need to ensure they are available to your project.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyJVc_ieVeCI"
   },
   "outputs": [],
   "source": [
    "descgen_chain_fewshot_tuned = LLMChain(\n",
    "    llm=llm_tuned, prompt=prompt_descgen_fewshot, output_key=\"generated_description\"\n",
    ")\n",
    "overall_chain_fewshot_tuned = SequentialChain(\n",
    "    chains=[descgen_chain_fewshot_tuned, eval_chain],\n",
    "    input_variables=[\"platform_name\", \"product_attributes\", \"examples\"],\n",
    "    # Here you return multiple variables\n",
    "    output_variables=[\"generated_description\", \"is_safe\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL5_cW_wwQFx"
   },
   "outputs": [],
   "source": [
    "result_fewshot_tuned = overall_chain_fewshot_tuned(\n",
    "    {\"platform_name\": \"Flipkart\", \"product_attributes\": attrs, \"examples\": examples}\n",
    ")\n",
    "pp.pprint(result_fewshot_tuned[\"generated_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWKSrJBTWHfq"
   },
   "source": [
    "It can happen sometimes that due to the reason mentioned above the fine tuned model generates incorrect responses: which is why the response of the validator model can be used to filter it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36e0c8413df6"
   },
   "outputs": [],
   "source": [
    "pp.pprint(result_fewshot_tuned[\"is_safe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQrziA-hwo3S"
   },
   "outputs": [],
   "source": [
    "sample_test[\"generated_description_tuned_fewshot\"] = sample_test[\n",
    "    \"parsed_product_specs\"\n",
    "].apply(\n",
    "    lambda x: overall_chain_fewshot_tuned(\n",
    "        {\n",
    "            \"platform_name\": \"Flipkart\",\n",
    "            \"product_attributes\": x,\n",
    "            \"examples\": compute_fewshot(x, retriever, train_spec_ix),\n",
    "        }\n",
    "    )[\"generated_description\"]\n",
    ")\n",
    "sample_generated_descriptions_tuned_fewshot = sample_test[\n",
    "    \"generated_description_tuned_fewshot\"\n",
    "].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fddc6bdf06c4"
   },
   "source": [
    "In this case since the fine tuned model decreased the quality especially for Bleu score which has length penalty. Hence its important to monitor all 3 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a70495bdd673"
   },
   "outputs": [],
   "source": [
    "compute_quality_metrics_batch(\n",
    "    sample_generated_descriptions_tuned_fewshot,\n",
    "    sample_descriptions,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxpRRhQmbeSi"
   },
   "source": [
    "Now you can quickly skim through the original and generated descriptions and save the dataframe to disk in .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQmoG-_lbcmu"
   },
   "outputs": [],
   "source": [
    "sample_test[\n",
    "    [\"parsed_product_specs\", \"description\", \"generated_description_tuned_fewshot\"]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LPRzlT1bkKT"
   },
   "outputs": [],
   "source": [
    "sample_test.to_csv(\"./augmented_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk_k-2AEVxZe"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how SEO-optimized, truthful, and creative product descriptions can be created using Vertex AI Generative AI models and Langchain.\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "* Leverage few-shot examples to ground the LLM and avoid hallucination, as well as tailor the generated descriptions closer to existing ones for similar products.\n",
    "* Use the Vertex AI textembeddings model to assess semantic similarity\n",
    "* Create LangChain Prompts, Retrievers, Chains, and Sequential Chains to generate more creative and engaging product descriptions.\n",
    "* Batch evaluate the quality of generated text against original text using BLEU, ROUGE, and semantic similarity (based on cosine distance) scores.\n",
    "* Guardrail the agent using a validator LLM model to ensure that the generated text is accurate, truthful, and creative.\n",
    "\n",
    "### Possible next steps you can take:\n",
    "\n",
    "* You can add additional product properties by utilizing the,[Vertex AI Image Captioning service](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning) which can then also add to the richness of the product description.\n",
    "* You can try using RLHF (reinforcement learning with human feedback) to perform preference optimization between multiple generated descriptions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
