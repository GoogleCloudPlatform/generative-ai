{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxCkB_DXTHzf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Editing with Imagen 2 and MaskMode on Vertex AI\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing_maskmode.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fgetting-started%2Fimage_editing_maskmode.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/image_editing_maskmode.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing_maskmode.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Jorj Ismailyan](https://github.com/jismailyan-google), Shuai Tang |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Imagen 2 on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview) brings Google's state of the art generative AI capabilities to application developers. With Imagen 2 on Vertex AI, application developers can build next-generation AI products that edit images.\n",
    "\n",
    "With Imagen 2, you can not only generate an image, but edit an image by setting a MaskMode in the request.\n",
    "\n",
    "This notebook focuses on **image editing with MaskMode** only. Learn more about [editing with the Imagen](https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-images).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsvgIuwTPZw"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, you will be exploring the image editing features of Imagen using the Vertex AI Python SDK. You will\n",
    "\n",
    "- Edit an image with a text prompt and mask mode.\n",
    "- Insert an object into an image.\n",
    "- Remove an object from an image.\n",
    "- Pad and outpaint an image.\n",
    "- Generate new backgrounds for images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Costs\n",
    "\n",
    "- This notebook uses billable components of Google Cloud:\n",
    "  - Vertex AI (Imagen)\n",
    "\n",
    "- Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwFMpIMrTV_4"
   },
   "source": [
    "### Install Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYUu8VMdJs3V"
   },
   "outputs": [],
   "source": [
    "! pip install --quiet --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime (Jupyter only)\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
    "\n",
    "The restart process might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIvVfyyhTPKi"
   },
   "source": [
    "After the restart is complete, continue to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opUxT_k5TdgP"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbNgv4q1T2Mi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybBXSukZkgjg"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.\n",
    "\n",
    "Learn more about setting up a project and a development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gUjJ42Nh5kf"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ju_PctW22NUl"
   },
   "outputs": [],
   "source": [
    "# @title Import libraries\n",
    "# @markdown Run the cell below before proceeding to import libraries and define utility functions. \\\n",
    "# @markdown You will also load the imagegeneration@006 model from the Vertex AI SDK.\n",
    "\n",
    "import io\n",
    "import math\n",
    "from typing import Any, List\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "from vertexai.preview.vision_models import Image as Vertex_Image\n",
    "from vertexai.preview.vision_models import ImageGenerationModel, ImageGenerationResponse\n",
    "\n",
    "\n",
    "# Gets the image bytes from a PIL Image object.\n",
    "def get_bytes_from_pil(image: Image) -> bytes:\n",
    "    byte_io_png = io.BytesIO()\n",
    "    image.save(byte_io_png, \"PNG\")\n",
    "    return byte_io_png.getvalue()\n",
    "\n",
    "\n",
    "# Corrects the orientation of an image if needed.\n",
    "def maybe_rotate(img_pil):\n",
    "    exif = img_pil.getexif()\n",
    "    rotation = exif.get(274)\n",
    "\n",
    "    if rotation == 3:\n",
    "        img_pil = img_pil.rotate(180, expand=True)\n",
    "    elif rotation == 6:\n",
    "        img_pil = img_pil.rotate(270, expand=True)\n",
    "    elif rotation == 8:\n",
    "        img_pil = img_pil.rotate(90, expand=True)\n",
    "    return img_pil\n",
    "\n",
    "\n",
    "# Extract bounding boxes from a mask.\n",
    "def get_bbox_from_mask(mask_image: np.ndarray, box_area_thres: int = 50) -> np.ndarray:\n",
    "    \"\"\"Finds the contours from a mask image.\"\"\"\n",
    "    contours, _ = cv.findContours(mask_image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    bboxes = np.zeros((len(contours), 4))\n",
    "    for i, contour in enumerate(contours):\n",
    "        x, y, w, h = cv.boundingRect(contour)\n",
    "        bboxes[i] = (x, y, x + w, y + h)\n",
    "    bboxes = filter(lambda x: (x[2] - x[0]) * (x[3] - x[1]) > box_area_thres, bboxes)\n",
    "    bboxes = sorted(bboxes, key=lambda x: (x[2] - x[0]) * (x[3] - x[1]), reverse=True)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# Edits specific areas and pastes them back into the original image.\n",
    "def crop_insert_paste(\n",
    "    generation_model: ImageGenerationModel,\n",
    "    image: Image,\n",
    "    mask: Image,\n",
    "    boxes: np.array,\n",
    "    pad_ratio: int,\n",
    "    prompt: str,\n",
    "    neg_prompt: str,\n",
    "    seed: int = 0,\n",
    "    mask_dilation: float = 0.01,\n",
    "    guidance_scale: int = 60,\n",
    "    samples: int = 4,\n",
    "):\n",
    "    generated_imgs = [image.copy() for _ in range(samples)]\n",
    "    for box in boxes:\n",
    "        # Calculate cropping area with padding.\n",
    "        box_w_pad = pad_ratio * (box[2] - box[0])\n",
    "        box_h_pad = pad_ratio * (box[3] - box[1])\n",
    "        x1 = np.round(np.clip(box[0] - box_w_pad, 0, image.width)).astype(\"int\")\n",
    "        x2 = np.round(np.clip(box[2] + box_w_pad, 0, image.width)).astype(\"int\")\n",
    "        y1 = np.round(np.clip(box[1] - box_h_pad, 0, image.height)).astype(\"int\")\n",
    "        y2 = np.round(np.clip(box[3] + box_h_pad, 0, image.height)).astype(\"int\")\n",
    "\n",
    "        im_crop = image.crop([x1, y1, x2, y2])\n",
    "        mask_crop = mask.crop([x1, y1, x2, y2])\n",
    "        image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(im_crop))\n",
    "        mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_crop))\n",
    "\n",
    "        # Edit the cropped area of the image.\n",
    "        generated_crops = generation_model.edit_image(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=neg_prompt,\n",
    "            base_image=image_vertex,\n",
    "            mask=mask_vertex,\n",
    "            number_of_images=samples,\n",
    "            edit_mode=\"inpainting-insert\",\n",
    "            seed=seed,\n",
    "            guidance_scale=guidance_scale,\n",
    "            mask_dilation=mask_dilation,\n",
    "        )\n",
    "\n",
    "        # Paste the generated edits of the cropped area into the corresponding\n",
    "        # positions in the base image.\n",
    "        for i, crop in enumerate(generated_crops.images):\n",
    "            generated_imgs[i].paste(crop._pil_image, (x1, y1))\n",
    "    return generated_imgs\n",
    "\n",
    "\n",
    "# Pads an image for outpainting. Provides options to control the positioning of\n",
    "# the original image.\n",
    "def pad_to_target_size(\n",
    "    source_image,\n",
    "    target_size=(1536, 1536),\n",
    "    mode=\"RGB\",\n",
    "    vertical_offset_ratio=0,\n",
    "    horizontal_offset_ratio=0,\n",
    "    fill_val=255,\n",
    "):\n",
    "    orig_image_size_w, orig_image_size_h = source_image.size\n",
    "    target_size_w, target_size_h = target_size\n",
    "\n",
    "    insert_pt_x = (target_size_w - orig_image_size_w) // 2 + int(\n",
    "        horizontal_offset_ratio * target_size_w\n",
    "    )\n",
    "    insert_pt_y = (target_size_h - orig_image_size_h) // 2 + int(\n",
    "        vertical_offset_ratio * target_size_h\n",
    "    )\n",
    "    insert_pt_x = min(insert_pt_x, target_size_w - orig_image_size_w)\n",
    "    insert_pt_y = min(insert_pt_y, target_size_h - orig_image_size_h)\n",
    "\n",
    "    if mode == \"RGB\":\n",
    "        source_image_padded = Image.new(\n",
    "            mode, target_size, color=(fill_val, fill_val, fill_val)\n",
    "        )\n",
    "    elif mode == \"L\":\n",
    "        source_image_padded = Image.new(mode, target_size, color=(fill_val))\n",
    "    else:\n",
    "        raise ValueError(\"source image mode must be RGB or L.\")\n",
    "\n",
    "    source_image_padded.paste(source_image, (insert_pt_x, insert_pt_y))\n",
    "    return source_image_padded\n",
    "\n",
    "\n",
    "# Pads and resizes image and mask to the same target size.\n",
    "def pad_image_and_mask(\n",
    "    image_vertex: Vertex_Image,\n",
    "    mask_vertex: Vertex_Image,\n",
    "    target_size,\n",
    "    vertical_offset_ratio,\n",
    "    horizontal_offset_ratio,\n",
    "    viz=True,\n",
    "):\n",
    "    image_vertex.thumbnail(target_size)\n",
    "    mask_vertex.thumbnail(target_size)\n",
    "\n",
    "    image_vertex = pad_to_target_size(\n",
    "        image_vertex,\n",
    "        target_size=target_size,\n",
    "        mode=\"RGB\",\n",
    "        vertical_offset_ratio=vertical_offset_ratio,\n",
    "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
    "        fill_val=0,\n",
    "    )\n",
    "    mask_vertex = pad_to_target_size(\n",
    "        mask_vertex,\n",
    "        target_size=target_size,\n",
    "        mode=\"L\",\n",
    "        vertical_offset_ratio=vertical_offset_ratio,\n",
    "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
    "        fill_val=255,\n",
    "    )\n",
    "    if viz:\n",
    "        print(\n",
    "            f\"image size(with x height): {image_vertex.size[0]} x {image_vertex.size[1]}\"\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image_vertex)\n",
    "        plt.imshow(mask_vertex, alpha=0.3)\n",
    "        plt.title(\"Padded image and mask overlay\")\n",
    "    return image_vertex, mask_vertex\n",
    "\n",
    "\n",
    "# Displays images in a grid below the cell\n",
    "def display_images_in_grid(images: List[Any]) -> None:\n",
    "    \"\"\"Displays the provided images in a grid format. 4 images per row.\n",
    "\n",
    "    Args:\n",
    "        images: A list of images to display.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the number of rows and columns for the grid layout.\n",
    "    nrows: int = math.ceil(len(images) / 4)  # Display at most 4 images per row\n",
    "    ncols: int = min(len(images) + 1, 4)  # Adjust columns based on the number of images\n",
    "\n",
    "    # Create a figure and axes for the grid layout.\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            # Display the image in the current axis.\n",
    "            if hasattr(images[i], \"_pil_image\"):\n",
    "                image = images[i]._pil_image\n",
    "            else:\n",
    "                image = images[i]\n",
    "            ax.imshow(image)\n",
    "\n",
    "            # Adjust the axis aspect ratio to maintain image proportions.\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "            # Disable axis ticks for a cleaner appearance.\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            # Hide empty subplots to avoid displaying blank axes.\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    # Adjust the layout to minimize whitespace between subplots.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the figure with the arranged images.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "IMAGE_GENERATION_MODEL = \"imagegeneration@006\"\n",
    "generation_model = ImageGenerationModel.from_pretrained(IMAGE_GENERATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R45VRKWInfQQ"
   },
   "source": [
    "## Select an image to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I9caIrZ7Dek1"
   },
   "outputs": [],
   "source": [
    "# @title Upload an image from local\n",
    "# @markdown Run this cell to enable and select the `Choose files` button. \\\n",
    "# @markdown You can then select an image file from your local device to upload. \\\n",
    "# @markdown Your uploaded image will be resized to 1024x1024 pixels for faster processing.\n",
    "\n",
    "images = files.upload()\n",
    "image_bytes = list(images.values())[0]\n",
    "image_pil = maybe_rotate(Image.open(io.BytesIO(image_bytes))).convert(\"RGB\")\n",
    "image_pil.thumbnail((1024, 1024))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image_pil)\n",
    "print(f\"image size(with x height): {image_pil.size[0]} x {image_pil.size[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqUv45zZ8UiZ"
   },
   "source": [
    "### Generate with Imagen\n",
    "Use the `generate_images` function with Imagen. All you need is a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GYBwQuciCco"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"a deer in a field looking at the camera\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.generate_images(\n",
    "    prompt=PROMPT,\n",
    ")\n",
    "\n",
    "INPUT_IMAGE = response.images[0]\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fU32286ooc8Q"
   },
   "source": [
    "## Edit images using MaskMode\n",
    "\n",
    "Now you can edit images without providing your own mask. Configure the `mask_mode` field `edit_image` request to automatically generate a mask on the input image.\n",
    "\n",
    "MaskMode provides the following modes:\n",
    "* **Background**: Edit the background of an image\n",
    "* **Foreground**: Edit the foreground of an image\n",
    "* **Semantic**: Edit specified objects in an image. You can edit 1 to 5 objects in an image using semantic segmentation classes.\n",
    "\n",
    "The `semantic` maskMode option requires you to set **Segmentation classes**. You must set 1 to 5 classes using the desired class ID. The full table of available classes is listed in the `Appendix` section at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBLJtICO8iMQ"
   },
   "source": [
    "### Explore different MaskMode options\n",
    "\n",
    "This section will explores how to edit images using different `edit_mode` and `mask_mode` parameter options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRXg5vk-p3GN"
   },
   "outputs": [],
   "source": [
    "# Set the edit mode. Choose from [\"inpainting-insert\", \"inpainting-remove\", \"outpainting\"].\n",
    "EDIT_MODE = \"inpainting-insert\"\n",
    "# Set how the mask should be generated. Choose from [\"background\", \"foreground\", \"semantic\"].\n",
    "MASK_MODE = \"foreground\"\n",
    "# Specify an object to edit using a segmentation class. Only valid for `semantic` maskMode.\n",
    "SEGMENTATION_CLASS = 16\n",
    "\n",
    "# Set a text prompt to influence how the masked part of the image will be edited.\n",
    "PROMPT = \"a cow looking at the camera\"\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "\n",
    "classes = None\n",
    "if MASK_MODE == \"semantic\":\n",
    "    classes = [SEGMENTATION_CLASS]\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=INPUT_IMAGE,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    number_of_images=4,\n",
    "    edit_mode=EDIT_MODE,\n",
    "    mask_mode=MASK_MODE,\n",
    "    segmentation_classes=classes,\n",
    ")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--7rofOb95hT"
   },
   "source": [
    "### Inpainting-insert with Background maskMode\n",
    "\n",
    "Edit the background of an image using a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBY_5JWlxojZ"
   },
   "outputs": [],
   "source": [
    "EDIT_MODE = \"inpainting-insert\"\n",
    "MASK_MODE = \"background\"\n",
    "# The background will be edited to adhere to the text prompt below.\n",
    "PROMPT = \"sandy desert oasis\"\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=INPUT_IMAGE,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    edit_mode=EDIT_MODE,\n",
    "    mask_mode=MASK_MODE,\n",
    "    segmentation_classes=classes,\n",
    ")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4YrxGDE-OY9"
   },
   "source": [
    "### Inpainting-insert with Foreground maskMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atoD2gTKycAp"
   },
   "outputs": [],
   "source": [
    "EDIT_MODE = \"inpainting-insert\"\n",
    "MASK_MODE = \"foreground\"\n",
    "\n",
    "# The foreground of the object will be edited according to the text prompt below.\n",
    "PROMPT = \"a bear looking at the camera\"\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=INPUT_IMAGE,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    edit_mode=EDIT_MODE,\n",
    "    mask_mode=MASK_MODE,\n",
    "    segmentation_classes=classes,\n",
    ")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XXCDNGz-ew2"
   },
   "source": [
    "### Inpainting-insert with Semantic maskMode\n",
    "\n",
    "Edit a specified object or multiple objects in an image using Semantic maskMode.\n",
    "You must set between 1 and 5 IDs in the `segmentation_classes` field. The full\n",
    "list of available segmentation classes is listed in the Appendix section at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFL0Hh_9ycXj"
   },
   "outputs": [],
   "source": [
    "EDIT_MODE = \"inpainting-insert\"\n",
    "MASK_MODE = \"semantic\"\n",
    "\n",
    "# Set the specified object(s) to edit in an image using a segmentation class.\n",
    "SEGMENTATION_CLASS = 16\n",
    "\n",
    "PROMPT = \"A cow looking at the camera\"\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=INPUT_IMAGE,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    edit_mode=EDIT_MODE,\n",
    "    mask_mode=MASK_MODE,\n",
    "    segmentation_classes=[SEGMENTATION_CLASS],\n",
    ")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbbpshyY_Hey"
   },
   "source": [
    "### Inpainting-remove with foreground maskMode\n",
    "\n",
    "Remove the foreground object of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_I2QlwQz0I2"
   },
   "outputs": [],
   "source": [
    "EDIT_MODE = \"inpainting-remove\"\n",
    "MASK_MODE = \"foreground\"\n",
    "PROMPT = \"Background, landscape photo\"\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=INPUT_IMAGE,\n",
    "    edit_mode=EDIT_MODE,\n",
    "    mask_mode=MASK_MODE,\n",
    "    segmentation_classes=classes,\n",
    ")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PixKFo0h_SZj"
   },
   "source": [
    "### Inpainting-remove with Semantic maskMode\n",
    "\n",
    "Remove the specified object(s) in an image using a segmentation class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9ZV33281192"
   },
   "outputs": [],
   "source": [
    "EDIT_MODE = \"inpainting-remove\"\n",
    "MASK_MODE = \"semantic\"\n",
    "# Set the object that will be removed according to its segmentation class ID.\n",
    "SEGMENTATION_CLASS = 125\n",
    "\n",
    "# Set a text prompt to guide the edited image.\n",
    "PROMPT = \"Background, landscape photo\"\n",
    "# Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=INPUT_IMAGE,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    edit_mode=EDIT_MODE,\n",
    "    mask_mode=MASK_MODE,\n",
    "    segmentation_classes=[SEGMENTATION_CLASS],\n",
    ")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk0eXjQ1nR4F"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You have explored the Imagen's image editing features through the Vertex AI Python SDK, including the additional parameters that influence image generation.\n",
    "\n",
    "Check out the Vertex AI reference to learn more about how to [Edit image prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide#edit-prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuaLTarf-hvO"
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZzRAwQ0dIjT"
   },
   "source": [
    "### Semantic segmentation classes\n",
    "\n",
    "| Class ID | Instance Type | Class ID | Instance Type | Class ID | Instance Type | Class ID | Instance Type |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | backpack | 50 | carrot | 100 | sidewalk_pavement | 150 | skis |\n",
    "| 1 | umbrella | 51 | hot_dog | 101 | runway | 151 | snowboard |\n",
    "| 2 | bag | 52 | pizza | 102 | terrain | 152 | sports_ball |\n",
    "| 3 | tie | 53 | donut | 103 | book | 153 | kite |\n",
    "| 4 | suitcase | 54 | cake | 104 | box | 154 | baseball_bat |\n",
    "| 5 | case | 55 | fruit_other | 105 | clock | 155 | baseball_glove |\n",
    "| 6 | bird | 56 | food_other | 106 | vase | 156 | skateboard |\n",
    "| 7 | cat | 57 | chair_other | 107 | scissors | 157 | surfboard |\n",
    "| 8 | dog | 58 | armchair | 108 | plaything_other | 158 | tennis_racket |\n",
    "| 9 | horse | 59 | swivel_chair | 109 | teddy_bear | 159 | net |\n",
    "| 10 | sheep | 60 | stool | 110 | hair_dryer | 160 | base |\n",
    "| 11 | cow | 61 | seat | 111 | toothbrush | 161 | sculpture |\n",
    "| 12 | elephant | 62 | couch | 112 | painting | 162 | column |\n",
    "| 13 | bear | 63 | trash_can | 113 | poster | 163 | fountain |\n",
    "| 14 | zebra | 64 | potted_plant | 114 | bulletin_board | 164 | awning |\n",
    "| 15 | giraffe | 65 | nightstand | 115 | bottle | 165 | apparel |\n",
    "| 16 | animal_other | 66 | bed | 116 | cup | 166 | banner |\n",
    "| 17 | microwave | 67 | table | 117 | wine_glass | 167 | flag |\n",
    "| 18 | radiator | 68 | pool_table | 118 | knife | 168 | blanket |\n",
    "| 19 | oven | 69 | barrel | 119 | fork | 169 | curtain_other |\n",
    "| 20 | toaster | 70 | desk | 120 | spoon | 170 | shower_curtain |\n",
    "| 21 | storage_tank | 71 | ottoman | 121 | bowl | 171 | pillow |\n",
    "| 22 | conveyor_belt | 72 | wardrobe | 122 | tray | 172 | towel |\n",
    "| 23 | sink | 73 | crib | 123 | range_hood | 173 | rug_floormat |\n",
    "| 24 | refrigerator | 74 | basket | 124 | plate | 174 | vegetation |\n",
    "| 25 | washer_dryer | 75 | chest_of_drawers | 125 | person | 175 | bicycle |\n",
    "| 26 | fan | 76 | bookshelf | 126 | rider_other | 176 | car |\n",
    "| 27 | dishwasher | 77 | counter_other | 127 | bicyclist | 177 | autorickshaw |\n",
    "| 28 | toilet | 78 | bathroom_counter | 128 | motorcyclist | 178 | motorcycle |\n",
    "| 29 | bathtub | 79 | kitchen_island | 129 | paper | 179 | airplane |\n",
    "| 30 | shower | 80 | door | 130 | streetlight | 180 | bus |\n",
    "| 31 | tunnel | 81 | light_other | 131 | road_barrier | 181 | train |\n",
    "| 32 | bridge | 82 | lamp | 132 | mailbox | 182 | truck |\n",
    "| 33 | pier_wharf | 83 | sconce | 133 | cctv_camera | 183 | trailer |\n",
    "| 34 | tent | 84 | chandelier | 134 | junction_box | 184 | boat_ship |\n",
    "| 35 | building | 85 | mirror | 135 | traffic_sign | 185 | slow_wheeled_object |\n",
    "| 36 | ceiling | 86 | whiteboard | 136 | traffic_light | 186 | river_lake |\n",
    "| 37 | laptop | 87 | shelf | 137 | fire_hydrant | 187 | sea |\n",
    "| 38 | keyboard | 88 | stairs | 138 | parking_meter | 188 | water_other |\n",
    "| 39 | mouse | 89 | escalator | 139 | bench | 189 | swimming_pool |\n",
    "| 40 | remote | 90 | cabinet | 140 | bike_rack | 190 | waterfall |\n",
    "| 41 | cell phone | 91 | fireplace | 141 | billboard | 191 | wall |\n",
    "| 42 | television | 92 | stove | 142 | sky | 192 | window |\n",
    "| 43 | floor | 93 | arcade_machine | 143 | pole | 193 | window_blind |\n",
    "| 44 | stage | 94 | gravel | 144 | fence | | |\n",
    "| 45 | banana | 95 | platform | 145 | railing_banister | | |\n",
    "| 46 | apple | 96 | playingfield | 146 | guard_rail | | |\n",
    "| 47 | sandwich | 97 | railroad | 147 | mountain_hill | | |\n",
    "| 48 | orange | 98 | road | 148 | rock | | |\n",
    "| 49 | broccoli | 99 | snow | 149 | frisbee | | |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
