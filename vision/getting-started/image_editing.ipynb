{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxCkB_DXTHzf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Editing with Imagen 2 on Vertex AI\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fgetting-started%2Fimage_editing.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/image_editing.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Jorj Ismailyan](https://github.com/jismailyan-google), Shuai Tang |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Imagen 2 on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview) brings Google's state of the art generative AI capabilities to application developers. With Imagen 2 on Vertex AI, application developers can build next-generation AI products for image editing.\n",
    "\n",
    "This notebook focuses on **image editing** only. Learn more about [editing with Imagen](https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-images).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsvgIuwTPZw"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, you will be exploring the image editing features of Imagen using the Vertex AI Python SDK. You will\n",
    "\n",
    "- Edit an image with a text prompt and a mask.\n",
    "- Insert an object into an image.\n",
    "- Remove an object from an image.\n",
    "- Pad and outpaint an image.\n",
    "- Generate new backgrounds for product images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Costs\n",
    "\n",
    "- This notebook uses billable components of Google Cloud:\n",
    "  - Vertex AI (Imagen)\n",
    "\n",
    "- Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwFMpIMrTV_4"
   },
   "source": [
    "### Install Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYUu8VMdJs3V"
   },
   "outputs": [],
   "source": [
    "! pip install --quiet --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime (Jupyter only)\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
    "\n",
    "The restart process might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIvVfyyhTPKi"
   },
   "source": [
    "After the restart is complete, continue to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opUxT_k5TdgP"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbNgv4q1T2Mi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybBXSukZkgjg"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.\n",
    "\n",
    "Learn more about setting up a project and a development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gUjJ42Nh5kf"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ju_PctW22NUl"
   },
   "outputs": [],
   "source": [
    "# @title Import libraries\n",
    "# @markdown Run the cell below before proceeding to import libraries and define utility functions. \\\n",
    "# @markdown You will also load the imagegeneration@006 model from the Vertex AI SDK.\n",
    "\n",
    "import io\n",
    "import math\n",
    "from typing import Any, List\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "from vertexai.preview.vision_models import Image as Vertex_Image\n",
    "from vertexai.preview.vision_models import ImageGenerationModel, ImageGenerationResponse\n",
    "\n",
    "\n",
    "# Gets the image bytes from a PIL Image object.\n",
    "def get_bytes_from_pil(image: Image) -> bytes:\n",
    "    byte_io_png = io.BytesIO()\n",
    "    image.save(byte_io_png, \"PNG\")\n",
    "    return byte_io_png.getvalue()\n",
    "\n",
    "\n",
    "# Corrects the orientation of an image if needed. Uses the EXIF tag 274\n",
    "# to check if an image has been rotated and if so, reverts it.\n",
    "def maybe_rotate(img_pil: Image):\n",
    "    exif = img_pil.getexif()\n",
    "    rotation = exif.get(274)\n",
    "\n",
    "    if rotation == 3:\n",
    "        img_pil = img_pil.rotate(180, expand=True)\n",
    "    elif rotation == 6:\n",
    "        img_pil = img_pil.rotate(270, expand=True)\n",
    "    elif rotation == 8:\n",
    "        img_pil = img_pil.rotate(90, expand=True)\n",
    "    return img_pil\n",
    "\n",
    "\n",
    "# Extract bounding boxes from a mask.\n",
    "def get_bbox_from_mask(mask_image: np.ndarray, box_area_thres: int = 50) -> np.ndarray:\n",
    "    contours, _ = cv.findContours(mask_image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    bboxes = np.zeros((len(contours), 4))\n",
    "    for i, contour in enumerate(contours):\n",
    "        x, y, w, h = cv.boundingRect(contour)\n",
    "        bboxes[i] = (x, y, x + w, y + h)\n",
    "    bboxes = filter(lambda x: (x[2] - x[0]) * (x[3] - x[1]) > box_area_thres, bboxes)\n",
    "    bboxes = sorted(bboxes, key=lambda x: (x[2] - x[0]) * (x[3] - x[1]), reverse=True)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# Edits specific areas and pastes them back into the original image.\n",
    "def crop_insert_paste(\n",
    "    generation_model: ImageGenerationModel,\n",
    "    image: Image,\n",
    "    mask: Image,\n",
    "    boxes: np.array,\n",
    "    pad_ratio: int,\n",
    "    prompt: str,\n",
    "    neg_prompt: str,\n",
    "    seed: int = 0,\n",
    "    mask_dilation: float = 0.01,\n",
    "    guidance_scale: int = 60,\n",
    "    samples: int = 4,\n",
    "):\n",
    "    generated_imgs = [image.copy() for _ in range(samples)]\n",
    "    for box in boxes:\n",
    "        # Calculate cropping area with padding.\n",
    "        box_w_pad = pad_ratio * (box[2] - box[0])\n",
    "        box_h_pad = pad_ratio * (box[3] - box[1])\n",
    "        x1 = np.round(np.clip(box[0] - box_w_pad, 0, image.width)).astype(\"int\")\n",
    "        x2 = np.round(np.clip(box[2] + box_w_pad, 0, image.width)).astype(\"int\")\n",
    "        y1 = np.round(np.clip(box[1] - box_h_pad, 0, image.height)).astype(\"int\")\n",
    "        y2 = np.round(np.clip(box[3] + box_h_pad, 0, image.height)).astype(\"int\")\n",
    "\n",
    "        im_crop = image.crop([x1, y1, x2, y2])\n",
    "        mask_crop = mask.crop([x1, y1, x2, y2])\n",
    "        image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(im_crop))\n",
    "        mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_crop))\n",
    "\n",
    "        # Edit the cropped area of the image.\n",
    "        generated_crops = generation_model.edit_image(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=neg_prompt,\n",
    "            base_image=image_vertex,\n",
    "            mask=mask_vertex,\n",
    "            number_of_images=samples,\n",
    "            edit_mode=\"inpainting-insert\",\n",
    "            seed=seed,\n",
    "            guidance_scale=guidance_scale,\n",
    "            mask_dilation=mask_dilation,\n",
    "        )\n",
    "\n",
    "        # Paste the generated edits of the cropped area into the corresponding\n",
    "        # positions in the base image.\n",
    "        for i, crop in enumerate(generated_crops.images):\n",
    "            generated_imgs[i].paste(crop._pil_image, (x1, y1))\n",
    "    return generated_imgs\n",
    "\n",
    "\n",
    "# Pads an image for outpainting. Provides options to control the positioning of\n",
    "# the original image.\n",
    "def pad_to_target_size(\n",
    "    source_image,\n",
    "    target_size=(1536, 1536),\n",
    "    mode=\"RGB\",\n",
    "    vertical_offset_ratio=0,\n",
    "    horizontal_offset_ratio=0,\n",
    "    fill_val=255,\n",
    "):\n",
    "    orig_image_size_w, orig_image_size_h = source_image.size\n",
    "    target_size_w, target_size_h = target_size\n",
    "\n",
    "    insert_pt_x = (target_size_w - orig_image_size_w) // 2 + int(\n",
    "        horizontal_offset_ratio * target_size_w\n",
    "    )\n",
    "    insert_pt_y = (target_size_h - orig_image_size_h) // 2 + int(\n",
    "        vertical_offset_ratio * target_size_h\n",
    "    )\n",
    "    insert_pt_x = min(insert_pt_x, target_size_w - orig_image_size_w)\n",
    "    insert_pt_y = min(insert_pt_y, target_size_h - orig_image_size_h)\n",
    "\n",
    "    if mode == \"RGB\":\n",
    "        source_image_padded = Image.new(\n",
    "            mode, target_size, color=(fill_val, fill_val, fill_val)\n",
    "        )\n",
    "    elif mode == \"L\":\n",
    "        source_image_padded = Image.new(mode, target_size, color=(fill_val))\n",
    "    else:\n",
    "        raise ValueError(\"source image mode must be RGB or L.\")\n",
    "\n",
    "    source_image_padded.paste(source_image, (insert_pt_x, insert_pt_y))\n",
    "    return source_image_padded\n",
    "\n",
    "\n",
    "# Pads and resizes image and mask to the same target size.\n",
    "def pad_image_and_mask(\n",
    "    image_vertex: Vertex_Image,\n",
    "    mask_vertex: Vertex_Image,\n",
    "    target_size,\n",
    "    vertical_offset_ratio,\n",
    "    horizontal_offset_ratio,\n",
    "    viz=True,\n",
    "):\n",
    "    image_vertex.thumbnail(target_size)\n",
    "    mask_vertex.thumbnail(target_size)\n",
    "\n",
    "    image_vertex = pad_to_target_size(\n",
    "        image_vertex,\n",
    "        target_size=target_size,\n",
    "        mode=\"RGB\",\n",
    "        vertical_offset_ratio=vertical_offset_ratio,\n",
    "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
    "        fill_val=0,\n",
    "    )\n",
    "    mask_vertex = pad_to_target_size(\n",
    "        mask_vertex,\n",
    "        target_size=target_size,\n",
    "        mode=\"L\",\n",
    "        vertical_offset_ratio=vertical_offset_ratio,\n",
    "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
    "        fill_val=255,\n",
    "    )\n",
    "    if viz:\n",
    "        print(\n",
    "            f\"image size(with x height): {image_vertex.size[0]} x {image_vertex.size[1]}\"\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image_vertex)\n",
    "        plt.imshow(mask_vertex, alpha=0.3)\n",
    "        plt.title(\"Padded image and mask overlay\")\n",
    "    return image_vertex, mask_vertex\n",
    "\n",
    "\n",
    "# Displays images in a grid below the cell\n",
    "def display_images_in_grid(images: List[Any]) -> None:\n",
    "    \"\"\"Displays the provided images in a grid format. 4 images per row.\n",
    "\n",
    "    Args:\n",
    "        images: A list of images to display.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the number of rows and columns for the grid layout.\n",
    "    nrows: int = math.ceil(len(images) / 4)  # Display at most 4 images per row\n",
    "    ncols: int = min(len(images) + 1, 4)  # Adjust columns based on the number of images\n",
    "\n",
    "    # Create a figure and axes for the grid layout.\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            # Display the image in the current axis.\n",
    "            if hasattr(images[i], \"_pil_image\"):\n",
    "                image = images[i]._pil_image\n",
    "            else:\n",
    "                image = images[i]\n",
    "            ax.imshow(image)\n",
    "\n",
    "            # Adjust the axis aspect ratio to maintain image proportions.\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "            # Disable axis ticks for a cleaner appearance.\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            # Hide empty subplots to avoid displaying blank axes.\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    # Adjust the layout to minimize whitespace between subplots.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the figure with the arranged images.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "IMAGE_GENERATION_MODEL = \"imagegeneration@006\"\n",
    "generation_model = ImageGenerationModel.from_pretrained(IMAGE_GENERATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R45VRKWInfQQ"
   },
   "source": [
    "## Set a base image to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I9caIrZ7Dek1"
   },
   "outputs": [],
   "source": [
    "# @title Upload an image from local\n",
    "# @markdown Run this cell to enable and select the `Choose files` button. \\\n",
    "# @markdown You can then select an image file from your local device to upload. \\\n",
    "# @markdown Your uploaded image will be resized for faster processing.\n",
    "\n",
    "images = files.upload()\n",
    "image_bytes = list(images.values())[0]\n",
    "image_pil = maybe_rotate(Image.open(io.BytesIO(image_bytes))).convert(\"RGB\")\n",
    "image_pil.thumbnail((1024, 1024))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image_pil)\n",
    "print(f\"image size(with x height): {image_pil.size[0]} x {image_pil.size[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqUv45zZ8UiZ"
   },
   "source": [
    "### Generate with Imagen\n",
    "Use the `generate_images` function with Imagen. All you need is a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GYBwQuciCco"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"a deer on a grassy barn field looking at the camera\"\n",
    "\n",
    "response: ImageGenerationResponse = generation_model.generate_images(\n",
    "    prompt=PROMPT,\n",
    ")\n",
    "\n",
    "local_file_path = \"tmp.png\"\n",
    "response.images[0].save(local_file_path)\n",
    "image_pil = Image.open(local_file_path)\n",
    "\n",
    "# Resize to a maximum dimension of 1024\n",
    "image_pil.thumbnail((1024, 1024))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image_pil)\n",
    "print(f\"mask size(with x height): {image_pil.size[0]} x {image_pil.size[1]}\")\n",
    "\n",
    "display_images_in_grid(response.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjAK-W4pJRnj"
   },
   "source": [
    "## Set a mask for the base image\n",
    "\n",
    "The mask is an image file that specifies which part of the image you want to edit.\n",
    "\n",
    "*To edit an image without specifying a mask, try out the notebook tutorial for [Imagen 2 Editing with MaskMode](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing_maskmode.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jTo3dpGRFQp2"
   },
   "outputs": [],
   "source": [
    "# @title Upload a mask from local device.\n",
    "# @markdown Run this cell to enable and select the `Choose files` button. \\\n",
    "# @markdown You can then select a mask image file from your local device to upload. \\\n",
    "# @markdown Your uploaded mask will be resized for faster processing.\n",
    "\n",
    "masks = files.upload()\n",
    "mask_bytes = list(masks.values())[0]\n",
    "mask_pil = Image.open(io.BytesIO(mask_bytes)).convert(\"L\")\n",
    "mask_pil.thumbnail((1024, 1024))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mask_pil)\n",
    "print(f\"mask size(with x height): {mask_pil.size[0]} x {mask_pil.size[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhFBZBkMHYbr"
   },
   "source": [
    "## Edit images\n",
    "Use the `edit_image` function to edit a base image. You can set the following parameters:\n",
    "\n",
    "- `prompt` - A text prompt describing the object to insert.\n",
    "- `negative_prompt` - [Optional] - Define what you DON'T want to see.\n",
    "- `samples` - The number of edited images to generate. An integer between 1 to 4.\n",
    "- `mask_dilation` - The dilation percentage of the mask. Dilation sets the mask sensitivity around edges. This is helpful for thin objects (e.g. spokes on a bicycle wheel). A decimal value between 0 and 1 and default at 0.01 for `inpainting-insert`. 0 means no dilation and 1 means the entire image will be covered by the mask.\n",
    "\n",
    "- `guidance_scale` - Controls how much the model adheres to the text prompt. Large values increase output and prompt alignment, but may compromise image quality. A value between 0 to 500, default at 60.\n",
    "- `seed` - Pseudo random seed for reproducible generated outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN_TNaLWKvA5"
   },
   "source": [
    "### Insert objects into an image\n",
    "\n",
    "With the `inpainting-insert` edit mode, you add a new object to an image.\n",
    "\n",
    "Best practices:\n",
    " - If providing a mask, use medium to large masks.\n",
    " - The edits should be reasonable in regards to lighting, placement, and creating a realistic scene.\n",
    " - Use related context (e.g., natural object interactions, object size reference, composition) in the text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNCgeYZNJEwz"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"<add your prompt here>\"\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "SAMPLES = 4\n",
    "MASK_DILATION = 0.01\n",
    "GUIDANCE_SCALE = 60\n",
    "SEED = 0\n",
    "\n",
    "image_pil_insert = image_pil\n",
    "mask_pil_insert = mask_pil\n",
    "\n",
    "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil_insert))\n",
    "mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_pil_insert))\n",
    "\n",
    "generated_images = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    base_image=image_vertex,\n",
    "    mask=mask_vertex,\n",
    "    number_of_images=SAMPLES,\n",
    "    edit_mode=\"inpainting-insert\",\n",
    "    seed=SEED,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    mask_dilation=MASK_DILATION,\n",
    ")\n",
    "\n",
    "display_images_in_grid(generated_images.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCDEDPYwMyr4"
   },
   "source": [
    "#### Tip: inpainting-insert for small objects\n",
    "\n",
    "For some small objects, you can get better results with this method.\n",
    "\n",
    "1. crop RGB image and mask, set a prompt that describes the area to be edited.\n",
    "2. use the cropped image and mask to edit the image.\n",
    "3. paste the generated edited images back into the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KPGbxZGMvuY"
   },
   "outputs": [],
   "source": [
    "image_pil_insert = image_pil\n",
    "mask_pil_insert = mask_pil\n",
    "\n",
    "bboxes = get_bbox_from_mask(np.array(mask_pil_insert))\n",
    "generated_images = crop_insert_paste(\n",
    "    generation_model,\n",
    "    image_pil_insert,\n",
    "    mask_pil_insert,\n",
    "    bboxes,\n",
    "    pad_ratio=0.5,\n",
    "    prompt=PROMPT,\n",
    "    neg_prompt=NEGATIVE_PROMPT,\n",
    "    samples=SAMPLES,\n",
    "    mask_dilation=MASK_DILATION,\n",
    "    seed=SEED,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    ")\n",
    "\n",
    "display_images_in_grid(generated_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh3ZTaT2N7ku"
   },
   "source": [
    "### Remove objects from an image\n",
    "\n",
    "Set the `edit_mode` to `inpainting-remove` to remove objects from an image.\n",
    "\n",
    "Best practices:\n",
    " - Uses small / medium size masks. Using large size masks is similar to generating new objects\n",
    " - When the target objects throw a shadow, including the shadow in the mask significantly improves result quality. Omitting shadows from input masks provides a cue for the model to generate a corresponding object to the shadow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxaNm59-O7bN"
   },
   "outputs": [],
   "source": [
    "# A text prompt and negative prompt for `inpainting-remove` are not required.\n",
    "# You can set the prompt argument to an empty string.\n",
    "PROMPT = \"\"\n",
    "SAMPLES = 4\n",
    "MASK_DILATION = 0.03\n",
    "GUIDANCE_SCALE = 60\n",
    "SEED = 0\n",
    "\n",
    "image_pil_remove = image_pil\n",
    "mask_pil_remove = mask_pil\n",
    "\n",
    "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil_remove))\n",
    "mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_pil_remove))\n",
    "generated_images = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=image_vertex,\n",
    "    mask=mask_vertex,\n",
    "    number_of_images=SAMPLES,\n",
    "    edit_mode=\"inpainting-remove\",\n",
    "    seed=SEED,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    mask_dilation=MASK_DILATION,\n",
    ")\n",
    "\n",
    "display_images_in_grid(generated_images.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un1dJrLEQWtn"
   },
   "source": [
    "### Expand an image outwards\n",
    "\n",
    "Use the `outpainting` edit mode to expand an image content beyond its boundaries.\n",
    "\n",
    "Best practices:\n",
    "- For optimal results, start with smaller expansions.\n",
    "- This feature works best with images that have softer, more blended textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiZSgH_bVp5M"
   },
   "source": [
    "#### Prepare outpainting image data\n",
    "\n",
    "To use the outpainting feature, you must prepare the base image by padding some empty space around it. This empty space will be filled in by Imagen. You can run the cell below to create some padding around your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCAuoo8iUtGF"
   },
   "outputs": [],
   "source": [
    "# Set rotation degree. Positive degrees to rotate the image anti-clockwise, negative degrees for clockwise.\n",
    "rotation_deg = 0\n",
    "# Pad input image and mask to new size with *target_size_h* as height.\n",
    "target_size_h = 2500\n",
    "# Set the width-to-height aspect ratio for the outpainted images.\n",
    "width_to_height_aspect_ratio = \"3/4\"\n",
    "# Move input image *target_size* x *vertical_offset_ratio* pixels vertically, positive value means downward \\\n",
    "vertical_offset_ratio = 0\n",
    "# Move input image *target_size* x *horizontal_offset_ratio* pixels horizontally, positive value moves image to the right.\n",
    "horizontal_offset_ratio = 0\n",
    "# Visualize padded image and mask.\n",
    "viz = True\n",
    "\n",
    "# Prepare the outpainting input image and mask\n",
    "rot_img = image_pil.rotate(rotation_deg, expand=1, resample=Image.Resampling.BICUBIC)\n",
    "rot_mask = mask_pil.rotate(rotation_deg, expand=1, fillcolor=255)\n",
    "target_size_w = int(target_size_h * eval(width_to_height_aspect_ratio))\n",
    "target_size = (target_size_w, target_size_h)\n",
    "\n",
    "image_pil_outpaint, mask_pil_outpaint = pad_image_and_mask(\n",
    "    rot_img,\n",
    "    rot_mask,\n",
    "    target_size,\n",
    "    vertical_offset_ratio,\n",
    "    horizontal_offset_ratio,\n",
    "    viz=viz,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVKnj_yvVxSD"
   },
   "source": [
    "#### Outpaint images\n",
    "\n",
    "Once you have padded the image, you can use the `outpainting` edit mode to fill the empty space as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaVCqqMbVwzX"
   },
   "outputs": [],
   "source": [
    "# [Optional]Set a positive prompt to define what you want to see in outpainted region,\n",
    "# If left empty, the model takes the context cue from the base image.\n",
    "PROMPT = \"<add your prompt here>\"\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "NEGATIVE_PROMPT = \"\"\n",
    "SAMPLES = 4\n",
    "# Default at 0.03 for `outpainting`.\n",
    "MASK_DILATION = 0.03\n",
    "GUIDANCE_SCALE = 60\n",
    "SEED = 0\n",
    "\n",
    "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil_outpaint))\n",
    "mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_pil_outpaint))\n",
    "generated_images = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    base_image=image_vertex,\n",
    "    mask=mask_vertex,\n",
    "    number_of_images=SAMPLES,\n",
    "    edit_mode=\"outpainting\",\n",
    "    seed=SEED,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    mask_dilation=MASK_DILATION,\n",
    ")\n",
    "\n",
    "display_images_in_grid(generated_images.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_zmAGkoWf2y"
   },
   "source": [
    "### Generate a new background for product images\n",
    "\n",
    "Use Imagen's `product-image` edit mode to set a new background for product images. Imagen excels at generating high image quality background content that fits the lighting and shadows of the product - great for advertisements and online shopping. The `product-image` mode always returns four 1024x1024 images.\n",
    "\n",
    "Best practices:\n",
    "\n",
    "- Imagen generates the foreground product mask for you. Avoid using products or objects that have thin, isolated parts, e.g. long handbag straps, a single golf club.\n",
    "- Put the desired objects closer to the bottom of a square input image. This is achieved by pre-padding objects in a square input image and setting product_position to `fixed` in the API.\n",
    "\n",
    "You can use the `product_position` parameter to set how the primary product is placed in the generated images.\n",
    "\n",
    "*   `reposition`- Re-centers the primary product in the generated image.\n",
    "*   `fixed`- For square input images, this will keep the primary product in the same relative location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmwqAj1YWXZI"
   },
   "outputs": [],
   "source": [
    "# Set a text prompt that describes the new image you want to see with the product.\n",
    "PROMPT = \"<add your prompt here>\"\n",
    "# Recenter the primary product in the generated image\n",
    "PRODUCT_POSITION = \"reposition\"\n",
    "\n",
    "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil))\n",
    "generated_images = generation_model.edit_image(\n",
    "    prompt=PROMPT,\n",
    "    base_image=image_vertex,\n",
    "    edit_mode=\"product-image\",\n",
    "    product_position=PRODUCT_POSITION,\n",
    ")\n",
    "\n",
    "display_images_in_grid(generated_images.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk0eXjQ1nR4F"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You have explored the Imagen's image editing features through the Vertex AI Python SDK, including the additional parameters that influence image generation.\n",
    "\n",
    "\n",
    "Check out the Vertex AI reference to learn more about how to [Edit image prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide#edit-prompts)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_editing.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
