{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Using samplers with Gemma\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/open-models/gemma/using_samplers_with_gemma.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fopen-models%2Fgemma%2Fusing_samplers_with_gemma.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/open-models/gemma/using_samplers_with_gemma.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/gemma/using_samplers_with_gemma.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author |  [Laurie White](https://github.com/Annie29/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows you how to use sampling, sometimes called decoding, to change the behavior of the Gemma model.  Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.  This tutorial uses KerasNLP, a collection of natural language processing (NLP) models implemented in [Keras](https://keras.io/) and runnable on JAX, PyTorch, and TensorFlow.\n",
        "\n",
        "In this tutorial, you'll use Gemma to generate text responses using several samplers. You'll see how changes to the sampler can change the usefulness of the responses Gemma gives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Gemma setup\n",
        "\n",
        "To complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma at Kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run\n",
        "  the Gemma 2B model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gN-IVRC3dQe"
      },
      "source": [
        "### Accept the Gemma Terms of Use\n",
        "\n",
        "While you have accepted the Gemma Terms of Use in a previous step, each time you use a non-local version of Gemma you'll need to link to your acceptance.  You can either do this by setting and accessing secrets in Colab or by entering your username and key at a prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2XT80G7SZIg"
      },
      "source": [
        "Use this version with Colab secret values for KAGGLE_USERNAME and KAGGLE_KEY.  The `userdata.get` code will need to be rewritten for your system if you are not using Colab or you can connect to Kaggle directly in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrBoa_Urw9Vx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5XttTtKSsGC"
      },
      "source": [
        "The following version will connect you to Kaggle where you'll be prompted for your username and key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-oGdJxmVpMc"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub\n",
        "import os\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oy3QUmXtSd"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "Install Keras and KerasNLP.  (You may see a warning about pip's dependency resolver.  You can ignore it; it should not cause trouble later on.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcGLzDeQ8NwN"
      },
      "outputs": [],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5cVOFt5YvZ"
      },
      "source": [
        "### Select a backend\n",
        "\n",
        "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. [Keras 3](https://keras.io/keras_3) lets you choose the backend: TensorFlow, JAX, or PyTorch. All three will work for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rS7ryTs5wjf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599765c72722"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "Import KerasNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2fa267d75bc"
      },
      "outputs": [],
      "source": [
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsxDCbLN555T"
      },
      "source": [
        "## Create a model\n",
        "\n",
        "KerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/). In this tutorial, you'll create a model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n",
        "\n",
        "Create the model using the `from_preset` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yygIK9DEIldp"
      },
      "outputs": [],
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrAWvsU6pI0E"
      },
      "source": [
        "`from_preset` instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma_2b_en\"` specifies the preset architecture: a Gemma model with 2 billion parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOBW7piN5-sl"
      },
      "source": [
        "## Samplers\n",
        "\n",
        "An LLM has a number of choices when creating its responses.  You can affect the way Gemma makes these choices by using a _sampler_.  \n",
        "\n",
        "To change a sampler in Gemma, you will recompile the model with the sampler you want to use.  \n",
        "The easiest way to do this is to just send the name of the sampler as the `sampler` parameter when compiling the model, as shown below.\n",
        "\n",
        "```\n",
        "gemma_lm.compile(sampler=\"greedy\")\n",
        "```\n",
        "\n",
        "If you want to send parameters to a sampler, you may find it easier to first create the sampler with the parameters and then send the new sampler as the `sampler` parameter.\n",
        "\n",
        "```\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoBtaHvwjSsh"
      },
      "source": [
        "Let's take a look at some samplers that can work with Gemma in Keras.  You can read more about them in the [Keras documentation](https://keras.io/api/keras_nlp/samplers/). If none of the built-in samplers fit your needs, you can create [custom samplers](https://keras.io/api/keras_nlp/samplers/samplers/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0g614Vbww6"
      },
      "source": [
        "### Greedy sampler\n",
        "\n",
        "The default sampler is the [`Greedy` sampler](https://keras.io/api/keras_nlp/samplers/greedy_sampler/).  It will pick the token with the largest probability as the next token, thus having no variation in its output if all tokens have unique probabilities.\n",
        "\n",
        "Consider the case below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBIJa5MclEMO"
      },
      "outputs": [],
      "source": [
        "gemma_lm.compile(sampler=\"greedy\")\n",
        "print(gemma_lm.generate(\"Are cats or dogs better?\", max_length=32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Um_aEGG7kSZ"
      },
      "source": [
        "If you run it multiple times with the same prompt, you should notice the output does not change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUk-YD58AX6s"
      },
      "source": [
        "### TopK sampler\n",
        "\n",
        "The [`Top K` sampler](https://keras.io/api/keras_nlp/samplers/top_k_sampler/).  allows for some variabilty in output.\n",
        "\n",
        "It will first restrict the options to the _k_ possible tokens with the highest probability.\n",
        " It will then select from those _k_ elements with the chance of selection determined by the probability.\n",
        "\n",
        "Consider the case below which will select from the top 5 next tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aae5GHrdpj2_"
      },
      "outputs": [],
      "source": [
        "sampler = keras_nlp.samplers.TopKSampler(k=5)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(\"What should I do on a trip to Europe?\", max_length=64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TVzvZDFMqVB"
      },
      "source": [
        "If you run this code more than once, you should notice different answers.\n",
        "\n",
        "If you're debugging or doing a demo or something similar and want to ensure you get the same \"random\" values each time, you can add a seed parameter to the line which creates the sampler to use the same random number sequence:\n",
        "\n",
        "```\n",
        "sampler = keras.nlp.samplers.TopKSampler(k=5, seed = 2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH0eFH_DvYwM"
      },
      "source": [
        "### TopP sampler\n",
        "\n",
        "Top P sampling, also known as nucleus sampling, will first order the options in descending order of probability.    \n",
        "\n",
        "It will then select tokens starting with the one with the highest probability and keeping adding tokens to the set to be considered until the sum of their probabilities is â‰¥ *p*.\n",
        "\n",
        "Consider the case below which will select from the smallest set of tokens with a total probability that's greater than or equal to 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-eXdb7tMn0C"
      },
      "outputs": [],
      "source": [
        "sampler = keras_nlp.samplers.TopPSampler(p=0.9)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(\"What should I do on a trip to Europe?\", max_length=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJaYKk-Ttq3p"
      },
      "source": [
        "Since ordering the tokens by frequency can be an expensive operation, Top K sampling is often used before Top P sampling.  In Keras, this can be done by sending a k value when creating the sampler, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnktWdMCugPO"
      },
      "outputs": [],
      "source": [
        "sampler = keras_nlp.samplers.TopPSampler(p=0.9, k=200)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(\"What should I do on a trip to Europe?\", max_length=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b66_XclUbtai"
      },
      "source": [
        "### Random models and temperature\n",
        "\n",
        "Random sampling is similar to Top K sampling, but it will consider all possible tokens as the next token, with selection chance determined by the probability of each token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rfzcjascc6s"
      },
      "outputs": [],
      "source": [
        "gemma_lm.compile(sampler=\"random\")\n",
        "print(gemma_lm.generate(\"What should I do on a trip to Europe?\", max_length=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlgYzE1L54q2"
      },
      "source": [
        "By default, the _temperature_ of a sampler is 1.0.  By adjusting the temperature to a value between 0.0 and 2.0, you can adjust how much difference is between the likelihoods.\n",
        "\n",
        "Temperature values greater than 1.0 will reduce the difference between likelihood values, thus making the LLM seem more creative.  Temperatures less than 1.0 will increase the difference between likelihood, making the more likely values even likelier to happen, thus making the LLM seem less creative.\n",
        "\n",
        "Try changing the temperature parameter of the random sampler below to see how it changes the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5aks4tl6TcY"
      },
      "outputs": [],
      "source": [
        "sampler = keras_nlp.samplers.RandomSampler(temperature=0.7)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(\"What should I do on a trip to Europe?\", max_length=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBrbTYasoo-J"
      },
      "source": [
        "## What's next\n",
        "\n",
        "In this tutorial, you learned how to modify the output of Gemma by using different sampling techniques. Here are a few suggestions for what to learn next:\n",
        "\n",
        "* Learn how to [finetune a Gemma model](https://ai.google.dev/gemma/docs/lora_tuning).\n",
        "* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/distributed_tuning).\n",
        "* Learn about [Gemma integration with Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
        "* Learn how to [use Gemma models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "using_samplers_with_gemma.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
