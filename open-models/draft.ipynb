{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485a7260",
   "metadata": {},
   "source": [
    "# üìì model_garden_litellm_inference.ipynb  \n",
    "**Deploy an Open-Source Model on Vertex AI and Serve Inference via LiteLLM with OpenAI-Compatible APIs (Chat & Function Calling)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2833c52",
   "metadata": {},
   "source": [
    "## üß≠ 1. Overview\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Deploy an open-source LLM (e.g. DeepSeek, LLaMA, Gemma) via [Vertex AI Model Garden](https://cloud.google.com/vertex-ai/docs/generative-ai/model-garden).\n",
    "- Serve the model with a public Vertex AI endpoint.\n",
    "- Connect it to [LiteLLM](https://docs.litellm.ai/) using an OpenAI-compatible schema for chat completion and function calling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b025d",
   "metadata": {},
   "source": [
    "## üîß 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81747952",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-aiplatform litellm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240eb3cd",
   "metadata": {},
   "source": [
    "## üöÄ 3. Deploy OSS Model from Model Garden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87c4c6",
   "metadata": {},
   "source": [
    "### 3.1 Choose Model\n",
    "\n",
    "You can browse available models in the [Model Garden UI](https://console.cloud.google.com/vertex-ai/generative/models).\n",
    "\n",
    "Example: DeepSeek Coder 6.7B or LLaMA3-8B-Instruct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601985c",
   "metadata": {},
   "source": [
    "### 3.2 Deploy to Endpoint (Auto or Manual)\n",
    "\n",
    "You can use the console to deploy a prebuilt model, or automate via SDK.\n",
    "\n",
    "```python\n",
    "DEPLOYED_MODEL_NAME = \"deepseek-7b-instruct\"\n",
    "ENDPOINT_NAME = f\"{DEPLOYED_MODEL_NAME}-endpoint\"\n",
    "```\n",
    "\n",
    "Insert deployment logic as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c604f",
   "metadata": {},
   "source": [
    "## üîó 4. Set Up LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LITELLM_MODEL\"] = f\"vertex_ai/openai/{ENDPOINT_NAME}\"\n",
    "os.environ[\"GOOGLE_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_REGION\"] = REGION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb131bab",
   "metadata": {},
   "source": [
    "Optional: Save a `litellm_config.yaml` file for easier CLI access.\n",
    "\n",
    "```yaml\n",
    "model_list:\n",
    "  - model_name: openai-compatible-model\n",
    "    litellm_provider: vertex_ai\n",
    "    model_info:\n",
    "      endpoint_id: <your-endpoint-id>\n",
    "      project: <your-project-id>\n",
    "      region: <your-region>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494e8e1",
   "metadata": {},
   "source": [
    "## üí¨ 5. Chat Completion via LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d79a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=f\"vertex_ai/openai/{ENDPOINT_NAME}\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of Japan?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5abf3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 6. Function Calling via LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\n",
    "    model=f\"vertex_ai/openai/{ENDPOINT_NAME}\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What‚Äôs the weather in New York today?\"}\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get weather information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a00e5",
   "metadata": {},
   "source": [
    "## ‚úÖ 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88627b",
   "metadata": {},
   "source": [
    "In this notebook, you:\n",
    "- Deployed an open-source model using Vertex AI Model Garden\n",
    "- Exposed it via a Vertex AI endpoint\n",
    "- Used LiteLLM to route OpenAI-style chat and function calling traffic to your model\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
