{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Build and deploy a Hugging Face smolagent using DeepSeek on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/tree/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fopen-models%2Fuse-cases%2Fvertex_ai_deepseek_smolagents.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/tree/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Ivan Nardini](https://github.com/inardini) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "> [DeepSeek-R1 from DeepSeek](https://huggingface.co/deepseek-ai/DeepSeek-R1) is a powerful language model developed with a focus on enhancing reasoning capabilities. DeepSeek-R1-Zero, DeepSeek-R1, and a collection of six distilled, dense models derived from DeepSeek-R1. These distilled models, based on the popular Llama and Qwen architectures, offer a range of sizes and capabilities to suit diverse research needs.\n",
        "\n",
        "> [HuggingFace's smol-agents](https://huggingface.co/docs/smolagents/en/index) library provides a lightweight and flexible framework for building and experimenting with language agents.\n",
        "\n",
        "> [Vertex AI](https://cloud.google.com/vertex-ai/docs) provides a comprehensive platform for the entire machine learning lifecycle.  It empowers you to build, train, and deploy ML models and AI applications, including customizing powerful large language models (LLMs).\n",
        "\n",
        "This notebook showcases how to deploy DeepSeek R1 Distill Qwen 32B from the Hugging Face Hub on Vertex AI using Vertex AI Model Garden. It also shows how to prototype and deploy a simple agent using HuggingFace's smol-agents library on Vertex AI Reasoning Engine.\n",
        "\n",
        "\n",
        "By the end of this notebook, you will learn how to:\n",
        "\n",
        "- Register and deploy an LLM from the Hugging Face Hub on Vertex AI\n",
        "- Prototype and deploy an agent on Vertex AI Reasoning Engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet \"google-cloud-aiplatform[reasoningengine]\" \"openai\" \"smolagents\" \\\n",
        "    \"cloudpickle==3.0.0\" \\\n",
        "    \"pydantic>=2.10\" \\\n",
        "    \"requests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. In Colab or Colab Enterprise, you might see an error message that says \"Your session crashed for an unknown reason.\" This is expected. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZeNZkeq3wS6"
      },
      "source": [
        "### Authenticate your Hugging Face account\n",
        "\n",
        "Then you can install the `huggingface_hub` that comes with a CLI that will be used for the authentication with the token generated in advance. So that then the token can be safely retrieved via `huggingface_hub.get_token`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB-DuCZj30hb"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import interpreter_login\n",
        "\n",
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTYJgjRN34RH"
      },
      "source": [
        "Read more about [Hugging Face Security](https://huggingface.co/docs/hub/en/security), specifically about [Hugging Face User Access Tokens](https://huggingface.co/docs/hub/en/security-tokens).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type: \"string\", placeholder: \"[your-bucket-name]\", isTemplate: true}\n",
        "\n",
        "if not BUCKET_NAME or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = f\"{PROJECT_ID}-bucket\"\n",
        "\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "! gsutil mb -p $PROJECT_ID -l $LOCATION $BUCKET_URI\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "import google.auth\n",
        "from google.auth import default\n",
        "import google.auth.transport.requests\n",
        "from google.cloud import aiplatform\n",
        "from huggingface_hub import get_token\n",
        "import openai\n",
        "from smolagents import ChatMessage, CodeAgent, Model\n",
        "from smolagents.tools import Tool\n",
        "from vertexai.preview import reasoning_engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "## Set model\n",
        "\n",
        "Set the model ID from Hugging Face Hub. In this case, you use DeepSeek-R1-Distill-Qwen-32B, a dense model distilled from DeepSeek-R1 good at math."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"  # @param {type:\"string\", isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Ze75ouQrdZ"
      },
      "source": [
        "## Register and Deploy DeepSeek model on Vertex AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJsGNCv4T6R-"
      },
      "source": [
        "### Register a DeepSeek model on Vertex AI Model Registry\n",
        "\n",
        "Deploying a DeepSeek model on Vertex AI begins with importing the model into the [Vertex AI Model Registry](https://www.google.com/search?q=model+registry+vertex+ai&oq=model+registry+vertex+ai&gs_lcrp=EgZjaHJvbWUqBwgAEAAYgAQyBwgAEAAYgAQyCggBEAAYgAQYogQyBggCEEUYPDIGCAMQRRg8MgYIBBBFGDwyBggFEEUYQDIGCAYQRRhAMgYIBxBFGEDSAQg2MzMxajBqN6gCALACAA&sourceid=chrome&ie=UTF-8), a central hub for managing your ML model lifecycle.  This registry stores model configurations, enabling streamlined organization, tracking, and versioning.  \n",
        "\n",
        "The `aiplatform.Model.upload` method specifies the display name, the serving container image URI (pointing to the vLLM inference container on Vertex AI Model Garden), and arguments for the vLLM API server. Key arguments include the model name, tensor parallelism size, maximum model length, and enforcement of eager execution.\n",
        "\n",
        "It also defines the serving container port, predict route, health route, and crucial environment variables, notably the Hugging Face token for downloading the model from the Hugging Face Hub.\n",
        "\n",
        "See the [vLLM documentation](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#quickstart-online) and [aiplatform.Model.upload](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload) Python reference for a complete list of arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc-Owc_7WiZb"
      },
      "outputs": [],
      "source": [
        "deepseek_model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_ID.replace(\"/\", \"--\").lower(),\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference.cu121.0-6.ubuntu2204.py310\",\n",
        "    serving_container_args=[\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={MODEL_ID}\",\n",
        "        # Hugging Face configuration\n",
        "        \"--tensor-parallel-size=2\",\n",
        "        \"--max-model-len=32768\",\n",
        "        \"--enforce-eager\",\n",
        "    ],\n",
        "    serving_container_ports=[8080],\n",
        "    serving_container_predict_route=\"/generate\",\n",
        "    serving_container_health_route=\"/ping\",\n",
        "    serving_container_environment_variables={\n",
        "        \"HF_TOKEN\": get_token(),\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    },\n",
        ")\n",
        "deepseek_model.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c427c0a87016"
      },
      "source": [
        "### Deploy DeepSeek model on Vertex AI Prediction\n",
        "\n",
        "After the model is registered on Vertex AI, you can deploy the model to an endpoint.\n",
        "\n",
        "First create the endpoint with aiplatform.Endpoint.create method. Then you deploys the model to this endpoint, specifying the machine type (`g2-standard-24`), accelerator type (`NVIDIA_L4`), and the number of accelerators (`2`). \n",
        "\n",
        "> This deployment configuration is based on [Hugging Face's documentation](https://huggingface.co/deepseek-ai/DeepSeek-R1#6-how-to-run-locally). \n",
        "\n",
        "For more information on the supported `aiplatform.Model.deploy` arguments, you can check its [Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxI3SsHxVYn4"
      },
      "outputs": [],
      "source": [
        "deepseek_endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=MODEL_ID.replace(\"/\", \"--\").lower() + \"-endpoint\"\n",
        ")\n",
        "\n",
        "deployed_deepseek_model = deepseek_model.deploy(\n",
        "    endpoint=deepseek_endpoint,\n",
        "    machine_type=\"g2-standard-24\",\n",
        "    accelerator_type=\"NVIDIA_L4\",\n",
        "    accelerator_count=2,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEY206Oj0U2G"
      },
      "source": [
        "> Note that the model deployment on Vertex AI can take around 20 minutes to get deployed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lIr8uko04T_"
      },
      "source": [
        "### Generate predictions with Vertex AI API\n",
        "\n",
        "After deploying the model, you can use the `aiplatform.Endpoint.predict` method to generate online predictions. This sends requests to the deployed endpoint, utilizing the `/predict` route defined within the container and adhering to Vertex AI's input/output payload formatting requirements.\n",
        "\n",
        "> Note the instance request format is aligned the [vLLM OpenAI Completions API interface](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#openai-completions-api-with-vllm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lu1C-nXSvH-"
      },
      "outputs": [],
      "source": [
        "prediction_request = {\n",
        "    \"instances\": [\n",
        "        {\n",
        "            \"@requestFormat\": \"chatCompletions\",\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Count the number of 'r' in the word Strawberry\",\n",
        "                }\n",
        "            ],\n",
        "            \"max_tokens\": 2048,\n",
        "            \"temperature\": 0.7,\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "output = deployed_deepseek_model.predict(instances=prediction_request[\"instances\"])\n",
        "for prediction in output.predictions[0]:\n",
        "    print(\"------- DeepSeek prediction -------\")\n",
        "    print(prediction[\"message\"][\"content\"])\n",
        "    print(\"---------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iljyx7y5Nbl1"
      },
      "source": [
        "## Build a simple math agent with Hugging Face's smolagents\n",
        "\n",
        "With your DeepSeek model now deployed on Vertex AI, let's leverage its mathematical capabilities. The `deepseek-ai/DeepSeek-R1-Distill-Qwen-32B` excels at mathematical reasoning, making it an ideal tool for an agent designed to verify math results.  \n",
        "\n",
        "Let's create a simple agent that combines the strengths of Gemini's function calling for orchestration and answer generation with DeepSeek's verification abilities on Vertex AI. This agent will use Hugging Face's smol-agents library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ5l0fTr3wAu"
      },
      "source": [
        "### Create a VertexAIServerModel class\n",
        "\n",
        "To integrate Gemini with Vertex AI for agent development, a custom [Model](https://huggingface.co/docs/smolagents/v1.5.0/en/reference/agents#models) class is required. This class will represent the Gemini text generation model, serving as the engine for your agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_yzMnYZ4tz8"
      },
      "source": [
        "> Note the code is based on the official [Model](https://github.com/huggingface/smolagents/blob/main/src/smolagents/models.py) implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvClfddqLxcR"
      },
      "outputs": [],
      "source": [
        "class VertexAIServerModel(Model):\n",
        "    \"\"\"This model connects to a Vertex AI-compatible API server.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, model_id: str, project_id: str, location: str, endpoint_id: str, **kwargs\n",
        "    ):\n",
        "        #  Try to import dependencies\n",
        "        try:\n",
        "            from google.auth import default\n",
        "        except ModuleNotFoundError:\n",
        "            raise ModuleNotFoundError(\n",
        "                \"Please install 'openai, google-auth and requests' extra to use VertexAIGeminiModel as described in the official documentation: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library\"\n",
        "            ) from None\n",
        "\n",
        "        # Initialize parent class with any additional keyword arguments\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_id = model_id\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.endpoint_id = endpoint_id\n",
        "        self.kwargs = kwargs\n",
        "        self._refresh_task = None\n",
        "\n",
        "        # Initialize credentials and set up Google Cloud authentication with required permissions\n",
        "        self.credentials, _ = default(\n",
        "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "        )\n",
        "        self._refresh_token()\n",
        "        self._setup_client()\n",
        "        self._start_refresh_loop()\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        messages: list[dict[str, str]],\n",
        "        **kwargs,\n",
        "    ) -> ChatMessage:\n",
        "\n",
        "        # Prepare the API call parameters\n",
        "        completion_kwargs = self._prepare_completion_kwargs(\n",
        "            messages=messages,\n",
        "            model=self.model_id,\n",
        "            **self.kwargs,\n",
        "        )\n",
        "\n",
        "        # Make the API call to Vertex AI\n",
        "        response = self.client.chat.completions.create(**completion_kwargs)\n",
        "        self.last_input_token_count = response.usage.prompt_tokens\n",
        "        self.last_output_token_count = response.usage.completion_tokens\n",
        "\n",
        "        # Convert API response to ChatMessage format\n",
        "        message = ChatMessage.from_dict(\n",
        "            response.choices[0].message.model_dump(\n",
        "                include={\"role\", \"content\", \"tool_calls\"}\n",
        "            )\n",
        "        )\n",
        "        return message\n",
        "\n",
        "    def _refresh_token(self):\n",
        "        \"\"\"Refresh the Google Cloud token\"\"\"\n",
        "        try:\n",
        "            self.credentials.refresh(google.auth.transport.requests.Request())\n",
        "            self._setup_client()\n",
        "        except Exception as e:\n",
        "            print(f\"Token refresh failed: {e}\")\n",
        "\n",
        "    def _setup_client(self):\n",
        "        \"\"\"Setup OpenAI client with current credentials\"\"\"\n",
        "        self.client = openai.OpenAI(\n",
        "            base_url=f\"https://{self.location}-aiplatform.googleapis.com/v1beta1/projects/{self.project_id}/locations/{self.location}/endpoints/{self.endpoint_id}\",\n",
        "            api_key=self.credentials.token,\n",
        "        )\n",
        "\n",
        "    def _start_refresh_loop(self):\n",
        "        \"\"\"Start the token refresh loop\"\"\"\n",
        "\n",
        "        def refresh_loop():\n",
        "            while True:\n",
        "                time.sleep(3600)\n",
        "                self._refresh_token()\n",
        "\n",
        "        self._refresh_thread = threading.Thread(target=refresh_loop, daemon=True)\n",
        "        self._refresh_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HX1BrGz30_i"
      },
      "source": [
        "### Create a math tool using a DeepSeek model\n",
        "\n",
        "In the context of language agents, a tool is a self-contained function the agent can utilize.  For a language model to effectively use a tool, the tool must have a well-defined API, including a name, a concise description, specifications for input types and their descriptions, and a defined output type.  \n",
        "\n",
        "To integrate our deployed DeepSeek model on Vertex AI as a tool within a smol-agents framework, a custom [Tool](https://huggingface.co/docs/smolagents/en/guided_tour#tools) class is required. This class will represent the DeepSeek model, serving as the mean to take action for your agent. In this case, the tool would verify math results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6wciAad4MAB"
      },
      "outputs": [],
      "source": [
        "class DeepSeekMathVerifierTool(Tool):\n",
        "    \"\"\"A tool that verifies math responses\"\"\"\n",
        "\n",
        "    name = \"math_verifier\"\n",
        "    description = \"\"\"This is a tool that verifies math responses\"\"\"\n",
        "    inputs = {\n",
        "        \"content\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"a text containing math\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, project_id: str, location: str, endpoint_id: str, **kwargs):\n",
        "        try:\n",
        "            from google.cloud import aiplatform\n",
        "            import vertexai\n",
        "        except ModuleNotFoundError:\n",
        "            raise ModuleNotFoundError(\n",
        "                \"Please install 'vertexai' and 'google-cloud-aiplatform' extra to use DeepSeekMathVerifierTool\"\n",
        "            ) from None\n",
        "\n",
        "        super().__init__()\n",
        "        self.endpoint_id = endpoint_id\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.kwargs = kwargs\n",
        "        self._refresh_task = None\n",
        "\n",
        "        # Initialize credentials and set up Google Cloud authentication with required permissions\n",
        "        self.credentials, _ = default(\n",
        "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "        )\n",
        "        self._refresh_token()\n",
        "        self._start_refresh_loop()\n",
        "\n",
        "        # Initialize Vertex ai session and the endpoint\n",
        "        vertexai.init(\n",
        "            project=self.project_id,\n",
        "            location=self.location,\n",
        "            credentials=self.credentials,\n",
        "            **self.kwargs,\n",
        "        )\n",
        "        self.endpoint = aiplatform.Endpoint(\n",
        "            endpoint_name=f\"projects/{self.project_id}/locations/{self.location}/endpoints/{self.endpoint_id}\"\n",
        "        )\n",
        "\n",
        "    def forward(self, content: str):\n",
        "        \"\"\"Submit the prediction request\"\"\"\n",
        "        content = str(content)\n",
        "        prediction_request = {\n",
        "            \"instances\": [\n",
        "                {\n",
        "                    \"@requestFormat\": \"chatCompletions\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            output = self.endpoint.predict(instances=prediction_request[\"instances\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction failed: {e}\")\n",
        "            return None\n",
        "        prediction = output.predictions[0][0][\"message\"][\"content\"]\n",
        "        return prediction\n",
        "\n",
        "    def _refresh_token(self):\n",
        "        \"\"\"Refresh the Google Cloud token\"\"\"\n",
        "        try:\n",
        "            self.credentials.refresh(google.auth.transport.requests.Request())\n",
        "        except Exception as e:\n",
        "            print(f\"Token refresh failed: {e}\")\n",
        "\n",
        "    def _start_refresh_loop(self):\n",
        "        \"\"\"Start the token refresh loop\"\"\"\n",
        "\n",
        "        def refresh_loop():\n",
        "            while True:\n",
        "                time.sleep(3600)\n",
        "                self._refresh_token()\n",
        "\n",
        "        self._refresh_thread = threading.Thread(target=refresh_loop, daemon=True)\n",
        "        self._refresh_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDoVJbEo920U"
      },
      "source": [
        "### Assemble the agent\n",
        "\n",
        "Having defined both the model and the tool, we can now assemble a basic agent.  \n",
        "\n",
        "`smolagents` provides a default implementation called `CodeAgent`, which is designed to write and execute Python code at each step of its process.  \n",
        "\n",
        "For more detailed information on agent construction and capabilities, refer to the `smolagents` [Agent](https://huggingface.co/docs/smolagents/en/guided_tour#codeagent-and-toolcallingagent) documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqZRJTL96OIa"
      },
      "outputs": [],
      "source": [
        "endpoint_id = next(\n",
        "    (\n",
        "        endpoint.name\n",
        "        for endpoint in aiplatform.Endpoint.list()\n",
        "        if endpoint.display_name == MODEL_ID.replace(\"/\", \"--\").lower() + \"-endpoint\"\n",
        "    ),\n",
        "    None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO4-Smd3C63D"
      },
      "outputs": [],
      "source": [
        "model = VertexAIServerModel(\n",
        "    model_id=\"google/gemini-1.5-flash\",\n",
        "    endpoint_id=\"openapi\",\n",
        "    project_id=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    DeepSeekMathVerifierTool(\n",
        "        endpoint_id=endpoint_id, project_id=PROJECT_ID, location=LOCATION\n",
        "    )\n",
        "]\n",
        "\n",
        "agent = CodeAgent(model=model, tools=tools, add_base_tools=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKBraQd5GefX"
      },
      "source": [
        "### Test the agent\n",
        "\n",
        "After you assemble the agent, you are now able to test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cITl7egiTWl-"
      },
      "outputs": [],
      "source": [
        "response = agent.run(\"Hello! How are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtNKWuWwDE4Q"
      },
      "outputs": [],
      "source": [
        "response = agent.run(\n",
        "    \"Count the number of 'r' in the word Strawberry. Verify the answer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RK-hjsL97hH"
      },
      "source": [
        "## Deploy the agent on Vertex AI Reasoning Engine\n",
        "\n",
        "Your agent prototype is running smoothly in Colab, but it's time to scale it for wider accessibility.\n",
        "\n",
        "[Reasoning Engine on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/reasoning-engine) provides a managed platform for creating and deploying advanced agent reasoning frameworks.  \n",
        "\n",
        "This notebook's approach utilizes a custom application template within Reasoning Engine, which can be further extended with frameworks like smolagents.\n",
        "\n",
        "Let's explore how to deploy our smol-agents agent using Reasoning Engine on Vertex AI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c1IqyNOGbfz"
      },
      "source": [
        "### Assemble the agent\n",
        "\n",
        "Define a SmolAgent class designed to interact with a DeepSeek model deployed on Vertex AI.\n",
        "\n",
        "The two main components for building a custom agent are `set_up` and query methods:\n",
        "\n",
        "- The `set_up` method instantiates the agent's core components: a VertexAIServerModel to connect to the deployed DeepSeek model, a DeepSeekMathVerifierTool for mathematical verification tasks, and a CodeAgent to orchestrate the model and tools.\n",
        "\n",
        "- The `query` method provides a simple interface for sending input to the agent and receiving its response, effectively triggering the agent's execution.\n",
        "\n",
        "To know more about custom agent, check out how to [customize an application template ](https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/customize).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaTZPdSB1Nbl"
      },
      "outputs": [],
      "source": [
        "class SmolAgent:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str,\n",
        "        endpoint_id: str,\n",
        "        tool_endpoint_id: str,\n",
        "        project_id: str,\n",
        "        location: str,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.model_id = model_id\n",
        "        self.endpoint_id = endpoint_id\n",
        "        self.tool_endpoint_id = tool_endpoint_id\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.add_base_tools = False\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def set_up(self) -> None:\n",
        "        \"\"\"Set up the agent.\"\"\"\n",
        "\n",
        "        self.model = VertexAIServerModel(\n",
        "            model_id=self.model_id,\n",
        "            endpoint_id=self.endpoint_id,\n",
        "            project_id=self.project_id,\n",
        "            location=self.location,\n",
        "            **self.kwargs,\n",
        "        )\n",
        "        self.tools = [\n",
        "            DeepSeekMathVerifierTool(\n",
        "                project_id=self.project_id,\n",
        "                location=self.location,\n",
        "                endpoint_id=self.tool_endpoint_id,\n",
        "                **self.kwargs,\n",
        "            )\n",
        "        ]\n",
        "        self.app = CodeAgent(\n",
        "            model=self.model,\n",
        "            tools=self.tools,\n",
        "            add_base_tools=self.add_base_tools,\n",
        "            **self.kwargs,\n",
        "        )\n",
        "\n",
        "    def query(self, input: str):\n",
        "        \"\"\"Query the application.\"\"\"\n",
        "        return self.app.run(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSQunNwr95Du"
      },
      "source": [
        "### Test the agent\n",
        "\n",
        "After you get the agent assembled, you can now test it locally to confirm its expected behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iatBy35eBmBc"
      },
      "outputs": [],
      "source": [
        "local_custom_agent = SmolAgent(\n",
        "    model_id=\"google/gemini-1.5-flash\",\n",
        "    endpoint_id=\"openapi\",\n",
        "    tool_endpoint_id=endpoint_id,\n",
        "    project_id=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        ")\n",
        "local_custom_agent.set_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHoBatTRUA_4"
      },
      "outputs": [],
      "source": [
        "response = local_custom_agent.query(input=\"Hello! How are you?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zLz5qWqfpjE"
      },
      "outputs": [],
      "source": [
        "response = local_custom_agent.query(\n",
        "    input=\"Count the number of 'r' in the word Strawberry. Verify the answer\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvHDdFB6UKBS"
      },
      "source": [
        "### Deploy the SmolAgent\n",
        "\n",
        "Your `smol-agent` application is running smoothly locally—excellent!  \n",
        "\n",
        "Let's now deploy it to Reasoning Engine on Vertex AI. This deployment will make your application accessible remotely, opening up possibilities for integration with broader systems and use as a standalone service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJclYoSDUPo3"
      },
      "outputs": [],
      "source": [
        "local_custom_agent = SmolAgent(\n",
        "    model_id=\"google/gemini-1.5-flash\",\n",
        "    endpoint_id=\"openapi\",\n",
        "    tool_endpoint_id=endpoint_id,\n",
        "    project_id=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        ")\n",
        "\n",
        "remote_custom_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    local_custom_agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[reasoningengine]\",\n",
        "        \"openai\",\n",
        "        \"smolagents\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic>=2.10\",\n",
        "        \"requests\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NL5UL6jVT66"
      },
      "source": [
        "### Call the agent\n",
        "\n",
        "Now that the agent is deployed, let's call the agent to answer our math questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HbvguziVVtw"
      },
      "outputs": [],
      "source": [
        "response = remote_custom_agent.query(\n",
        "    input=\"Count the number of 'r' in the word Strawberry. Verify the answer\"\n",
        ")\n",
        "print(\"Agent response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYDB5a_MDWiO"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "delete_endpoint = False\n",
        "delete_model = False\n",
        "delete_remote_agent = False\n",
        "\n",
        "if delete_bucket:\n",
        "    ! gsutil rm -r $BUCKET_URI\n",
        "if delete_endpoint:\n",
        "    deepseek_endpoint.delete(force=True)\n",
        "if delete_model:\n",
        "    deepseek_model.delete()\n",
        "if delete_remote_agent:\n",
        "    remote_custom_agent.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertex_ai_deepseek_smolagents.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
